{
    "docs": [
        {
            "location": "/", 
            "text": "Portability Across DOE Office of Science HPC Facilities\n\n\nAs the HPC community prepares for exascale and the semiconductor industry approaches the end of Moore's Law in terms of transistor size, we have entered a \nperiod of time of increased diversity in computer architecture for HPC with relatively new designs joining mature x86, DDR standard processor and memory \ntechnologies. These technologies include GPUs, Many Core Processors, ARM, FPGA and ASICs as well as new memory technology in the form of High Bandwidth \nMemory (HBM) often incorporated on the processor die as well as Non-Volatile memory (NVRAM) and Solid-State Disk (SSD) technology for accelerated IO. \n\n\nThe DOE Office of Science operates three world leading HPC facilities located at the Argonne Leadership Computing Facility (ALCF), National Energy Research \nScienctifc Computing Center (NERSC) at Lawrence Berkeley Lab and the Oak Ridge Leadership Computing Center (OLCF). These facilities field three of the most \npowerful supercomputers in world used by scientists throughout the DOE Office of Science and the world solving a \nnumber of important science problem in domains from materials science and chemistry to nuclear, particle and astrophysics. \n\n\nThese facilities have begun the transition for DOE users to energy-efficient like architectures. The facilities are currently fielding \nsystems with two-distinct \"pre-exascale\" like architectures that we discuss in detail on the subsequent pages: \n\n\n\n\n\n\n\n\nSystem\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nOLCF\n\n\n\n\n\n\nArchitecture\n\n\nCPU + NVIDIA GPU\n\n\n\n\n\n\nScale\n\n\n18,688 Nodes\n\n\n\n\n\n\nPicture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem\n\n\nCori\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nNERSC\n\n\n\n\n\n\nArchitecture\n\n\nXeon-Phi\n\n\n\n\n\n\nScale\n\n\n9688 Nodes\n\n\n\n\n\n\nNotes\n\n\nSSD Burst-Buffer IO layer\n\n\n\n\n\n\nPicture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSystem\n\n\nTheta\n\n\n\n\n\n\n\n\n\n\nLocation\n\n\nALCF\n\n\n\n\n\n\nArchitecture\n\n\nXeon-Phi\n\n\n\n\n\n\nScale\n\n\n3624 Nodes\n\n\n\n\n\n\nPicture\n\n\n\n\n\n\n\n\n\n\nThe two processor architectures deployed on these systems are the CPU+NVidia GPU hybrid architecture on Titan and the \"self-hosted\" Xeon-Phi processors \n(code named \"Knights Landing\"). These two architectures, while seemingly quite different at first appearance, have a number of similarities that we believe \nrepresent general trends in exascale like architectures:\n\n\n\n\nIncrease parallelism (Cores, Threads, Warps/Blocks)\n\n\nVectorization (AVX512, 32 Wide Warps)\n\n\nSmall Amount High-bandwidth Coupled with Large Amounts of Traditional DDR\n\n\n\n\nWhile the details of the architectures are distinct and vendor specific programming libraries/languages (CUDA, AVX512 Intrinsics etc.) exist to address \nspecific architecture features; the commonalities are significant that a number of portable programming approaches exist for writing code that supports both \narchitectures. \n\n\nThis living website is intended to be a guide for applications teams targetting systems at multiple DOE office of science facilities. In the below pages, we \ndiscuss in detail the differences between the systems, the software environment and job-submission process. We discuss how to define and measure performacne \nportability and we provide recommendations and case studies for the most promising performance-portability pogramming approaches.\n\n\nAs an additional valuable resource, the Centers of Excellence from DOE facilities in both the Office of Science and the National Nuclear Security Agency \n(NNSA) have coordinated an annual meeting with a detailed report of findings available \nhere\n).", 
            "title": "Introduction"
        }, 
        {
            "location": "/#portability-across-doe-office-of-science-hpc-facilities", 
            "text": "As the HPC community prepares for exascale and the semiconductor industry approaches the end of Moore's Law in terms of transistor size, we have entered a \nperiod of time of increased diversity in computer architecture for HPC with relatively new designs joining mature x86, DDR standard processor and memory \ntechnologies. These technologies include GPUs, Many Core Processors, ARM, FPGA and ASICs as well as new memory technology in the form of High Bandwidth \nMemory (HBM) often incorporated on the processor die as well as Non-Volatile memory (NVRAM) and Solid-State Disk (SSD) technology for accelerated IO.   The DOE Office of Science operates three world leading HPC facilities located at the Argonne Leadership Computing Facility (ALCF), National Energy Research \nScienctifc Computing Center (NERSC) at Lawrence Berkeley Lab and the Oak Ridge Leadership Computing Center (OLCF). These facilities field three of the most \npowerful supercomputers in world used by scientists throughout the DOE Office of Science and the world solving a \nnumber of important science problem in domains from materials science and chemistry to nuclear, particle and astrophysics.   These facilities have begun the transition for DOE users to energy-efficient like architectures. The facilities are currently fielding \nsystems with two-distinct \"pre-exascale\" like architectures that we discuss in detail on the subsequent pages:      System  Titan      Location  OLCF    Architecture  CPU + NVIDIA GPU    Scale  18,688 Nodes    Picture         System  Cori      Location  NERSC    Architecture  Xeon-Phi    Scale  9688 Nodes    Notes  SSD Burst-Buffer IO layer    Picture         System  Theta      Location  ALCF    Architecture  Xeon-Phi    Scale  3624 Nodes    Picture      The two processor architectures deployed on these systems are the CPU+NVidia GPU hybrid architecture on Titan and the \"self-hosted\" Xeon-Phi processors \n(code named \"Knights Landing\"). These two architectures, while seemingly quite different at first appearance, have a number of similarities that we believe \nrepresent general trends in exascale like architectures:   Increase parallelism (Cores, Threads, Warps/Blocks)  Vectorization (AVX512, 32 Wide Warps)  Small Amount High-bandwidth Coupled with Large Amounts of Traditional DDR   While the details of the architectures are distinct and vendor specific programming libraries/languages (CUDA, AVX512 Intrinsics etc.) exist to address \nspecific architecture features; the commonalities are significant that a number of portable programming approaches exist for writing code that supports both \narchitectures.   This living website is intended to be a guide for applications teams targetting systems at multiple DOE office of science facilities. In the below pages, we \ndiscuss in detail the differences between the systems, the software environment and job-submission process. We discuss how to define and measure performacne \nportability and we provide recommendations and case studies for the most promising performance-portability pogramming approaches.  As an additional valuable resource, the Centers of Excellence from DOE facilities in both the Office of Science and the National Nuclear Security Agency \n(NNSA) have coordinated an annual meeting with a detailed report of findings available  here ).", 
            "title": "Portability Across DOE Office of Science HPC Facilities"
        }, 
        {
            "location": "/facilities/overview/", 
            "text": "This should be high level overview of differences at centers - we can then cover more in comparison\n\n\nThe \nAdvanced Scientific Computing Research\n\nprogram in DOE Office of Science sponsors three computing facilities - \nthe\nArgonne Leadership Computing Facility\n (ALCF), the\n\nOak Ridge Leadership Computing Facility\n (OLCF),\nand the \nNational Energy Research Scientific Computing\nCenter\n (NERSC). Below we summarize the technical\nspecifications of the current or upcoming computing systems deployed at each\nfacility.\n\n\n\n\n\n\n\n\nSystem\n\n\nFacility\n\n\nModel\n\n\nProcessor\n\n\nAccelerator\n\n\nNodes\n\n\nPerf. Per Node\n\n\nPeak Perf.\n\n\n\n\n\n\n\n\n\n\nAurora\n\n\nALCF\n\n\nIntel\n\n\nIntel Xeon Phi (3\nrd\n gen)\n\n\n(none)\n\n\n50 000\n\n\n?\n\n\n?\n\n\n\n\n\n\nCori\n\n\nNERSC\n\n\nCray XC40\n\n\nIntel Xeon Phi (2\nnd\n gen)\n\n\n(none)\n\n\n9 688\n\n\n2.6 TF\n\n\n30 PF\n\n\n\n\n\n\nSummit\n\n\nOLCF\n\n\nIBM\n\n\nIBM POWER9\n\n\nNVIDIA Tesla (\"Volta\")\n\n\n~4 600\n\n\n 40 TF\n\n\n?\n\n\n\n\n\n\nTheta\n\n\nALCF\n\n\nCray XC40\n\n\nIntel Xeon Phi (2\nnd\n gen)\n\n\n(none)\n\n\n3 624\n\n\n2.6 TF\n\n\n10 PF\n\n\n\n\n\n\nTitan\n\n\nOLCF\n\n\nCray XK7\n\n\nAMD Opteron (\"Interlagos\")\n\n\nNVIDIA Tesla (\"Kepler\")\n\n\n18 688\n\n\n1.4 TF\n\n\n27 PF", 
            "title": "Overview"
        }, 
        {
            "location": "/facilities/overview/#this-should-be-high-level-overview-of-differences-at-centers-we-can-then-cover-more-in-comparison", 
            "text": "The  Advanced Scientific Computing Research \nprogram in DOE Office of Science sponsors three computing facilities -  the\nArgonne Leadership Computing Facility  (ALCF), the Oak Ridge Leadership Computing Facility  (OLCF),\nand the  National Energy Research Scientific Computing\nCenter  (NERSC). Below we summarize the technical\nspecifications of the current or upcoming computing systems deployed at each\nfacility.     System  Facility  Model  Processor  Accelerator  Nodes  Perf. Per Node  Peak Perf.      Aurora  ALCF  Intel  Intel Xeon Phi (3 rd  gen)  (none)  50 000  ?  ?    Cori  NERSC  Cray XC40  Intel Xeon Phi (2 nd  gen)  (none)  9 688  2.6 TF  30 PF    Summit  OLCF  IBM  IBM POWER9  NVIDIA Tesla (\"Volta\")  ~4 600   40 TF  ?    Theta  ALCF  Cray XC40  Intel Xeon Phi (2 nd  gen)  (none)  3 624  2.6 TF  10 PF    Titan  OLCF  Cray XK7  AMD Opteron (\"Interlagos\")  NVIDIA Tesla (\"Kepler\")  18 688  1.4 TF  27 PF", 
            "title": "This should be high level overview of differences at centers - we can then cover more in comparison"
        }, 
        {
            "location": "/facilities/tools/", 
            "text": "Performance Analysis Tools\n\n\n!Add HPC Toolkit and link to documentation on using these at centers\n\n\nEvaluating application performance portability across diverse computing\narchitectures often requires the aid of performance analysis tools. Such tools\nprovide detailed information and statistics characterizing an application's\nusage of the architecture, and can guide the developer as she optimizes\nbottlenecks to achieve higher performance.\n\n\nEach ASCR facility is equipped with a wide range of tools for measuring\napplication performance. The applications running at the three facilities\nexhibit a broad range of demands from computer architectures - some are limited\nby memory bandwidth, others by latency, and others still by the CPU itself. The\nperformance measurement tools available at the ASCR facilities can measure in\ndetail how an application uses each of these resources. They include, but are\nnot limited to, the list provided below. The description of each tool is copied\nfrom its official documentation.\n\n\n\n\nAllinea MAP\n:\n  Allinea MAP is the profiler for parallel, multithreaded or single threaded C,\n  C++, Fortran and F90 codes. It provides in depth analysis and bottleneck\n  pinpointing to the source line.\n\n\nCray Performance Measurement and Analysis\n  Tools\n:\n  The Cray Performance Measurement and Analysis Tools (or CrayPat) are a suite\n  of utilities that enable the user to capture and analyze performance data\n  generated during the execution of a program on a Cray system. The information\n  collected and analysis produced by use of these tools can help the user to\n  find answers to two fundamental programming questions: \nHow fast is my\n  program running?\n and \nHow can I make it run faster?\n\n\nIntel Advisor\n:\n  Intel Advisor is used early in the process of adding vectorization into your\n  code, or while converting parts of a serial program to a parallel\n  (multithreaded) program. It helps you explore and locate areas in which the\n  optimizations might provide significant benefit. It also helps you predict the\n  costs and benefits of adding vectorization or parallelism to those parts of\n  your program, allowing you to experiment.\n\n\nIntel VTune Amplifier\n:\n  Intel VTune Amplifier is a performance analysis tool targeted for users\n  developing serial and multithreaded applications.\n\n\nnvprof\n:\n  nvprof enables the collection of a timeline of CUDA-related activities on both\n  CPU and GPU, including kernel execution, memory transfers, memory set and CUDA\n  API calls and events or metrics for CUDA kernels.\n\n\nTuning and Analysis Utilities (TAU)\n:\n  TAU Performance System is a portable profiling and tracing toolkit for\n  performance analysis of parallel programs written in Fortran, C, C++, UPC,\n  Java, Python.", 
            "title": "Tools"
        }, 
        {
            "location": "/facilities/tools/#performance-analysis-tools", 
            "text": "!Add HPC Toolkit and link to documentation on using these at centers  Evaluating application performance portability across diverse computing\narchitectures often requires the aid of performance analysis tools. Such tools\nprovide detailed information and statistics characterizing an application's\nusage of the architecture, and can guide the developer as she optimizes\nbottlenecks to achieve higher performance.  Each ASCR facility is equipped with a wide range of tools for measuring\napplication performance. The applications running at the three facilities\nexhibit a broad range of demands from computer architectures - some are limited\nby memory bandwidth, others by latency, and others still by the CPU itself. The\nperformance measurement tools available at the ASCR facilities can measure in\ndetail how an application uses each of these resources. They include, but are\nnot limited to, the list provided below. The description of each tool is copied\nfrom its official documentation.   Allinea MAP :\n  Allinea MAP is the profiler for parallel, multithreaded or single threaded C,\n  C++, Fortran and F90 codes. It provides in depth analysis and bottleneck\n  pinpointing to the source line.  Cray Performance Measurement and Analysis\n  Tools :\n  The Cray Performance Measurement and Analysis Tools (or CrayPat) are a suite\n  of utilities that enable the user to capture and analyze performance data\n  generated during the execution of a program on a Cray system. The information\n  collected and analysis produced by use of these tools can help the user to\n  find answers to two fundamental programming questions:  How fast is my\n  program running?  and  How can I make it run faster?  Intel Advisor :\n  Intel Advisor is used early in the process of adding vectorization into your\n  code, or while converting parts of a serial program to a parallel\n  (multithreaded) program. It helps you explore and locate areas in which the\n  optimizations might provide significant benefit. It also helps you predict the\n  costs and benefits of adding vectorization or parallelism to those parts of\n  your program, allowing you to experiment.  Intel VTune Amplifier :\n  Intel VTune Amplifier is a performance analysis tool targeted for users\n  developing serial and multithreaded applications.  nvprof :\n  nvprof enables the collection of a timeline of CUDA-related activities on both\n  CPU and GPU, including kernel execution, memory transfers, memory set and CUDA\n  API calls and events or metrics for CUDA kernels.  Tuning and Analysis Utilities (TAU) :\n  TAU Performance System is a portable profiling and tracing toolkit for\n  performance analysis of parallel programs written in Fortran, C, C++, UPC,\n  Java, Python.", 
            "title": "Performance Analysis Tools"
        }, 
        {
            "location": "/facilities/comparison/", 
            "text": "Tim-Scott / Bronson / Brian\n\n\nHardware In-Depth\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nFacility\n\n\nNERSC\n\n\nALCF\n\n\nOLCF\n\n\n\n\n\n\nModel\n\n\nCray XC40\n\n\nCray XC40\n\n\nCray XK7\n\n\n\n\n\n\nProcessor\n\n\nIntel Xeon Phi (2\nnd\n gen)\n\n\nIntel Xeon Phi (2\nnd\n gen)\n\n\nAMD Opteron (\"Interlagos\")\n\n\n\n\n\n\nSpecific Processor\n\n\n?\n\n\nIntel SKU 7230\n\n\n?\n\n\n\n\n\n\nProcessor Cores\n\n\n68\n\n\n64\n\n\n?\n\n\n\n\n\n\nProcessor Base Frequency\n\n\n?\n\n\n1.3 GHz\n\n\n?\n\n\n\n\n\n\nProcessor Max Frequency\n\n\n?\n\n\n1.5 GHz\n\n\n?\n\n\n\n\n\n\nProcessor On-Package Memory\n\n\n16 GB MCDRAM\n\n\n16 GB MCDRAM\n\n\nn/a\n\n\n\n\n\n\nProcessor DRAM\n\n\n?\n\n\n192 GB DDR4\n\n\n?\n\n\n\n\n\n\nAccelerator\n\n\n(none)\n\n\n(none)\n\n\nNVIDIA Tesla (\"Kepler\")\n\n\n\n\n\n\nNodes\n\n\n9 688\n\n\n3 624\n\n\n18 688\n\n\n\n\n\n\nPerf. Per Node\n\n\n2.6 TF\n\n\n2.6 TF\n\n\n1.4 TF\n\n\n\n\n\n\nNode local storage\n\n\n?\n\n\n128 GB SSD\n\n\n?\n\n\n\n\n\n\nExternal Burst Buffer\n\n\n?\n\n\nn/a\n\n\n?\n\n\n\n\n\n\nParallel File System\n\n\n?\n\n\n10 PB Lustre\n\n\n?\n\n\n\n\n\n\nInterconnect\n\n\n?\n\n\nCray Aries\n\n\n?\n\n\n\n\n\n\nTopology\n\n\n?\n\n\nDragonfly\n\n\n?\n\n\n\n\n\n\nPeak Perf\n\n\n30 PF\n\n\n10 PF\n\n\n27 PFF\n\n\n\n\n\n\n\n\nSoftware Environment\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nSoftware environment management\n\n\nmodules\n\n\nmodules\n\n\nmodules\n\n\n\n\n\n\nBatch Job Scheduler\n\n\nSlurm\n\n\nCobalt\n\n\nPBS\n\n\n\n\n\n\nCompilers\n\n\n\n\n\n\n\n\n\n\n\n\nIntel\n\n\n?\n\n\n(\ndefault\n) \nmodule load PrgEnv-intel\n\n\n?\n\n\n\n\n\n\nCray\n\n\n?\n\n\nmodule load PrgEnv-cray\n\n\n?\n\n\n\n\n\n\nGNU\n\n\n?\n\n\nmodule load PrgEnv-gnu\n\n\n?\n\n\n\n\n\n\nPGI\n\n\n?\n\n\nn/a\n\n\n?\n\n\n\n\n\n\nCLANG\n\n\n?\n\n\nmodule load PrgEnv-llvm\n\n\n?\n\n\n\n\n\n\nInterpreters\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\n?\n\n\nmodule load cray-R\n\n\n?\n\n\n\n\n\n\nPython 2\n\n\n?\n\n\nCray: \nmodule load cray-python\n Intel: \nmodule load intelpython26\n\n\n?\n\n\n\n\n\n\nPython 3\n\n\n?\n\n\nIntel: \nmodule load intelpython35\n\n\n?\n\n\n\n\n\n\nLibraries\n\n\n\n\n\n\n\n\n\n\n\n\nFFT\n\n\n?\n\n\nFFTW: \nmodule load fftw\n \n Cray FFTW: \nmodule load cray-fftw\n \n Intel MKL: \nautomatic with Intel compilers\n\n\n?\n\n\n\n\n\n\nCray LibSci\n\n\n?\n\n\nmodule load cray-libsci\n\n\n?\n\n\n\n\n\n\nIntel MKL\n\n\n?\n\n\nautomatic with Intel compilers\n\n\n?\n\n\n\n\n\n\nTrilinos\n\n\n?\n\n\nmodule load cray-trilinos\n\n\n?\n\n\n\n\n\n\nPETSc\n\n\n?\n\n\nmodule load cray-petsc\n\n\n?\n\n\n\n\n\n\nSHMEM\n\n\n?\n\n\nmodule load cray-shmem\n\n\n?\n\n\n\n\n\n\nmemkind\n\n\n?\n\n\nmodule load cray-memkind\n\n\n?\n\n\n\n\n\n\nI/O Libraries\n\n\n\n\n\n\n\n\n\n\n\n\nHDF5\n\n\n?\n\n\nmodule load cray-hdf5\n\n\n?\n\n\n\n\n\n\nNetCDF\n\n\n?\n\n\nmodule load cray-netcdf\n\n\n?\n\n\n\n\n\n\nParallel NetCDF\n\n\n?\n\n\nmodule load cray-parallel-netcdf\n\n\n?\n\n\n\n\n\n\nPerformance Tools and APIs\n\n\n\n\n\n\n\n\n\n\n\n\nIntel VTune Amplifier\n\n\n?\n\n\nsource /opt/intel/vtune_amplifier_xe/amplxe-vars.sh\n\n\nn/a?\n\n\n\n\n\n\nCrayPAT\n\n\n?\n\n\nmodule load perftools\n\n\n?\n\n\n\n\n\n\nPAPI\n\n\n?\n\n\nmodule load papi\n\n\n?\n\n\n\n\n\n\nDarshan\n\n\nn/a?\n\n\nmodule load cray-memkind\n\n\nn/a?\n\n\n\n\n\n\nOther Packages and Frameworks\n\n\n\n\n\n\n\n\n\n\n\n\nShifter\n\n\n?\n\n\nmodule load shifter\n\n\nn/a?\n\n\n\n\n\n\n\n\nCompiler Wrappers\n\n\nUse these wrappers to properly cross-compile your source code for the compute\nnodes of the systems, and bring in appropriate headers for MPI, etc.\n\n\n\n\n\n\n\n\nSystem-\n\n\nCori\n\n\nTheta\n\n\nTitan\n\n\n\n\n\n\n\n\n\n\nC++\n\n\n?\n\n\nCC\n\n\n?\n\n\n\n\n\n\nC\n\n\n?\n\n\ncc\n\n\n?\n\n\n\n\n\n\nFortran\n\n\n?\n\n\nftn\n\n\n?\n\n\n\n\n\n\n\n\nJob Submission\n\n\nCori\n\n\nJob Script\n\n\nJob Submit Command\n\n\nTheta\n\n\nJob Script\n\n\n#!/bin/bash\n\n\n#COBALT -t 30\n\n\n#COBALT --attrs mcdram=cache:numa=quad\n\n\n#COBALT -A \nyourALCFProjectName\n\n\necho\n \nStarting Cobalt job script\n\n\nexport\n \nn_nodes\n=\n$COBALT_JOBSIZE\n\n\nexport\n \nn_mpi_ranks_per_node\n=\n32\n\n\nexport\n \nn_mpi_ranks\n=\n$((\n$n_nodes\n \n*\n \n$n_mpi_ranks_per_node\n))\n\n\nexport\n \nn_openmp_threads_per_rank\n=\n4\n\n\nexport\n \nn_hyperthreads_per_core\n=\n2\n\n\nexport\n \nn_hyperthreads_skipped_between_ranks\n=\n4\n\naprun -n \n$n_mpi_ranks\n -N \n$n_mpi_ranks_per_node\n \n\\\n\n  --env \nOMP_NUM_THREADS\n=\n$n_openmp_threads_per_rank\n -cc depth \n\\\n\n  -d \n$n_hyperthreads_skipped_between_ranks\n \n\\\n\n  -j \n$n_hyperthreads_per_core\n \n\\\n\n  \nexecutable\n \nexecutable args\n\n\n\n\n\nThe \n#COBALT -t 30\n line indicates 30 minutes runtime. Generally, \n#COBALT\n\nlines are equivalent to specifying \nqsub\n command-line arguments.\n\n\nJob Submit Command\n\n\nqsub -n 512 ./theta_script.sh\n\n\nThe \n-n 512\n argument requests 512 nodes.\n\n\nTitan\n\n\nJob Script\n\n\nJob Submit Command", 
            "title": "Comparison"
        }, 
        {
            "location": "/facilities/comparison/#hardware-in-depth", 
            "text": "System-  Cori  Theta  Titan      Facility  NERSC  ALCF  OLCF    Model  Cray XC40  Cray XC40  Cray XK7    Processor  Intel Xeon Phi (2 nd  gen)  Intel Xeon Phi (2 nd  gen)  AMD Opteron (\"Interlagos\")    Specific Processor  ?  Intel SKU 7230  ?    Processor Cores  68  64  ?    Processor Base Frequency  ?  1.3 GHz  ?    Processor Max Frequency  ?  1.5 GHz  ?    Processor On-Package Memory  16 GB MCDRAM  16 GB MCDRAM  n/a    Processor DRAM  ?  192 GB DDR4  ?    Accelerator  (none)  (none)  NVIDIA Tesla (\"Kepler\")    Nodes  9 688  3 624  18 688    Perf. Per Node  2.6 TF  2.6 TF  1.4 TF    Node local storage  ?  128 GB SSD  ?    External Burst Buffer  ?  n/a  ?    Parallel File System  ?  10 PB Lustre  ?    Interconnect  ?  Cray Aries  ?    Topology  ?  Dragonfly  ?    Peak Perf  30 PF  10 PF  27 PFF", 
            "title": "Hardware In-Depth"
        }, 
        {
            "location": "/facilities/comparison/#software-environment", 
            "text": "System-  Cori  Theta  Titan      Software environment management  modules  modules  modules    Batch Job Scheduler  Slurm  Cobalt  PBS    Compilers       Intel  ?  ( default )  module load PrgEnv-intel  ?    Cray  ?  module load PrgEnv-cray  ?    GNU  ?  module load PrgEnv-gnu  ?    PGI  ?  n/a  ?    CLANG  ?  module load PrgEnv-llvm  ?    Interpreters       R  ?  module load cray-R  ?    Python 2  ?  Cray:  module load cray-python  Intel:  module load intelpython26  ?    Python 3  ?  Intel:  module load intelpython35  ?    Libraries       FFT  ?  FFTW:  module load fftw    Cray FFTW:  module load cray-fftw    Intel MKL:  automatic with Intel compilers  ?    Cray LibSci  ?  module load cray-libsci  ?    Intel MKL  ?  automatic with Intel compilers  ?    Trilinos  ?  module load cray-trilinos  ?    PETSc  ?  module load cray-petsc  ?    SHMEM  ?  module load cray-shmem  ?    memkind  ?  module load cray-memkind  ?    I/O Libraries       HDF5  ?  module load cray-hdf5  ?    NetCDF  ?  module load cray-netcdf  ?    Parallel NetCDF  ?  module load cray-parallel-netcdf  ?    Performance Tools and APIs       Intel VTune Amplifier  ?  source /opt/intel/vtune_amplifier_xe/amplxe-vars.sh  n/a?    CrayPAT  ?  module load perftools  ?    PAPI  ?  module load papi  ?    Darshan  n/a?  module load cray-memkind  n/a?    Other Packages and Frameworks       Shifter  ?  module load shifter  n/a?", 
            "title": "Software Environment"
        }, 
        {
            "location": "/facilities/comparison/#compiler-wrappers", 
            "text": "Use these wrappers to properly cross-compile your source code for the compute\nnodes of the systems, and bring in appropriate headers for MPI, etc.     System-  Cori  Theta  Titan      C++  ?  CC  ?    C  ?  cc  ?    Fortran  ?  ftn  ?", 
            "title": "Compiler Wrappers"
        }, 
        {
            "location": "/facilities/comparison/#job-submission", 
            "text": "", 
            "title": "Job Submission"
        }, 
        {
            "location": "/facilities/comparison/#cori", 
            "text": "", 
            "title": "Cori"
        }, 
        {
            "location": "/facilities/comparison/#job-script", 
            "text": "", 
            "title": "Job Script"
        }, 
        {
            "location": "/facilities/comparison/#job-submit-command", 
            "text": "", 
            "title": "Job Submit Command"
        }, 
        {
            "location": "/facilities/comparison/#theta", 
            "text": "", 
            "title": "Theta"
        }, 
        {
            "location": "/facilities/comparison/#job-script_1", 
            "text": "#!/bin/bash  #COBALT -t 30  #COBALT --attrs mcdram=cache:numa=quad  #COBALT -A  yourALCFProjectName  echo   Starting Cobalt job script  export   n_nodes = $COBALT_JOBSIZE  export   n_mpi_ranks_per_node = 32  export   n_mpi_ranks = $(( $n_nodes   *   $n_mpi_ranks_per_node ))  export   n_openmp_threads_per_rank = 4  export   n_hyperthreads_per_core = 2  export   n_hyperthreads_skipped_between_ranks = 4 \naprun -n  $n_mpi_ranks  -N  $n_mpi_ranks_per_node   \\ \n  --env  OMP_NUM_THREADS = $n_openmp_threads_per_rank  -cc depth  \\ \n  -d  $n_hyperthreads_skipped_between_ranks   \\ \n  -j  $n_hyperthreads_per_core   \\ \n   executable   executable args   The  #COBALT -t 30  line indicates 30 minutes runtime. Generally,  #COBALT \nlines are equivalent to specifying  qsub  command-line arguments.", 
            "title": "Job Script"
        }, 
        {
            "location": "/facilities/comparison/#job-submit-command_1", 
            "text": "qsub -n 512 ./theta_script.sh \nThe  -n 512  argument requests 512 nodes.", 
            "title": "Job Submit Command"
        }, 
        {
            "location": "/facilities/comparison/#titan", 
            "text": "", 
            "title": "Titan"
        }, 
        {
            "location": "/facilities/comparison/#job-script_2", 
            "text": "", 
            "title": "Job Script"
        }, 
        {
            "location": "/facilities/comparison/#job-submit-command_2", 
            "text": "", 
            "title": "Job Submit Command"
        }, 
        {
            "location": "/perfport/definition/", 
            "text": "The 2016 DOE Center of Excellence (COE) meeting in Phoenix brought together engineers from the DOE's Office of Science and National Nuclear Security Agency \nas well as vendor staff (from Intel, NVIDIA, IBM, Cray and others) to share portability lessons and best practicies from their respect app-readiness \nprograms. One of the high-level take-away messages from the meeting is that \"there is not yet a universally accepted defition of 'performance portability'\". \nThere is generally agreement on what performance-portability \"basically means\" but details differ in everyone's idea for the term. A number of attendees \ngave the following definitions:\n\n\n\n\n\n\n\"For the purposes of this meeting, it is the ability to run an application with acceptable performance across KNL and GPU-based systems with a single \nversion of source code.\" (Rob Neely)\n\n\n\n\n\n\n\"An application is performance portable if it achieves a consistent level of performance (e.g. defined by execution time or \n\nother figure of merit (not percentage of peak flops across platforms)) relative to the best known implementation on each platform.\" (John Pennycook, Intel)\n\n\n\n\n\n\n\"Hard portability = no code changes and no tuning. Software portability = simple code mods with no algorithmic changes. Non-portable = algorithmic changes\" (Adrian Pope, Vitali Morozov)\n\n\n\n\n\n\n(Performance portability means) the same source code  will run productively on a variety of different architectures\" (Larkin)\n\n\n\n\n\n\n\"Code is performance portable when the application team says its performance  portable!\" (Richards)\n\n\n\n\n\n\nFor our purposes, we combine a few the ideas above into the following working definition:\n\n\n\n\nAn application is performance portable if it achieves a consistent ratio of the actual time to solution to either the best-known or the theoretical best time to \nsolution on each platform with minimal specialized source-code required for each.\n\n\n\n\nWe discuss the details on how to begin to quantify the level to which a code meets this definition on the \n\nMeasurement Techniques\n page.", 
            "title": "Definition"
        }, 
        {
            "location": "/perfport/measurements/", 
            "text": "Measuring Performance Portability\n\n\nAs discussed in the previous section, performance portability can be an elusive topic to quantify \nand different engineers often provide different definitions or measurement techniques.\n\n\nMeasuring Portability\n\n\nMeasuring 'portability' itself is somewhat more well defined. One can, in principle, measure the \ntotal lines of code used in common across different architectures vs. the amount of code intended \nfor a single architecture via \nIFDEF\n pre-processing statements, separate routines and the like. A code with 0% \narchitecture specfic code being completely portable and a code with a 100% architecture specific \ncode being essentially made up of multiple applications for each architecture. \n\n\nOne subtlety that this approach hides is that it is possible that shared source-code requires more lines than source-code intended for a single architecture \nor, in some cases, even two sets of separate source-code intended for multiple architectures. We ignore this case for now, assuming that using a portable \napproach to express an algorithm doesn't signficant change the amount of code required. \n\n\nMeasuring Performance\n\n\n'Performance', even on a single architecture, is a bit less simple to define and measure. In \npractice, scientists generally care about the quality and quantity of scientific output they \nproduce. This typically maps for them to relative performance concepts, such as how much faster \ncan a particular run or set of runs run today than yesterday or on this machine than that. The \ndrawback of trying to measure performance in this way is that the baseline is arbitrary - i.e. you \ndon't know how well your code is performing on any architecture compared to how it 'should' be \nperforming if it were well optimized.\n\n\nOne may in principle define absolute performance as a measure of the actual floating point operations (or, for example, integer operations) per second \n(FLOPS) of an \napplication during execution compared to the theoretical peak performance of the system or fraction of the system in use, as say reported on the Top 500 \nlist - \nTop500.org\n - or as reported in the system specs on NERSC, ALCF and OLCF websites.\n\n\nHowever, this is a poor measure of application performance (and a particularly poor measure to use when trying to quantify performance portability) for a \nnumber of reasons:\n\n\n\n\n\n\nThe application or algorithm may be fundamentally limited by an aspect of the HPC system other than the compute capability (number of cores/theads, \nclock-speed and vector/instruction-sets)\n\n\n\n\n\n\nThe application or algorithm may be fundamentally limited by \ndifferent\n aspects of the system on different HPC system. \n\n\n\n\n\n\nAs an example, an implemenation of an algorithm that is limited by memory bandwidth may be achieving the best performance it theoretially can multiple \narchitectures but could be achieving widely varying percentage of peaks FLOPS on the different systems. \n\n\nInstead we advocate for one of two approaches for defining performance against expected or optimal performance on the system for algorithm:\n\n\n1. Compare against a known, well-recognized (potentially non-portable), implementation.\n\n\nSome applications, algorithms or methods have well-recognized optimal (often hand-tuned) implementations on different architectures. These can be used as a \nbaseline for defining relative performance of portable versions. Our Chroma application case-study shows this approach. \nSee \nhere\n \n\n\nMany performance tools exist at ALCF, NERSC and OLCF for the purposes profiling applications, regions of applications and determining performance limiters \nwhen comparing different implementation of an algorithm or method. See the comprehensive list \nhere\n with links to detailed \ninstructions and example use-cases at each site. \n\n\n2. Use the roofline approach to compare actual to expected performance\n\n\nAs discussed above, the major limitation of defining performance relative to the peak FLOPS capability of the system is that applications in practice are \nlimited by many different aspects of an HPC system. \n\n\nThe roofline performance model and extensions to the roofline model attempt to take these into account. In the roofline approach, one defines various \ntheoretial performance ceilings for an algorithm or implementation with various properties. In the simplest model, one may classify an algorithm based on \nits DRAM arithmetic-intensity - that is the ratio of the FLOPs performed vs the data moved from main-memory (DRAM) to the processor over the course of \nexecution, which can be measured for a given application as described on the subpages. Below, we show the performance ceilings provided by the roofline \nmodel on KNL for applications as a function of the DRAM arithmetic-intensity:\n\n\n\n\nHere the blue line represents the optimal performance on the system that can be achieved for an application running out of the KNL High-Bandwidth Memory \n(HBM) with a given\nDRAM-AI (the x-axis value). For low\nvalues of DRAM-AI, the performance is limited by the diagonal ceiling, meaning that memory-bandwidth is the limiting factor. The location of the diagonal\nline are typically computed empiracally from the available bandwidth reported by stream triad. CITE stream.\n\n\nFor high values of DRAM-AI,\nmemory bandwidth no longer limits performance and one can, in principle, achieve the max compute performance on the system. However, for such cases we draw \nother ceilings that represent common limitations in algorithms or implementations of algorithms. The dashed-dotted green line labeled \"-ILP\" is the\nperformance ceiling for applications that: 1. do not provide a balance of multiply and add instructions, or simply don't use the Fused Multiply Add (FMA)\ninstructions on the processor and 2. don't have enough instruction level parallelism to keep both VPUs on the KNL busy. The dashed purple line labeled \n\"-Vectorization\" is performance ceiling of an algorithm or implementation that, in addition to the above two deficiencies, lacks vectorization (a \ncombined factor of 32 reduction in the ceiling).\n\n\nFor applications that limited by other system properties, it is possible to extend the roofline model to include related ceilings. For example, we commonly  \n\nextend the roofline approach to use arithmetic-intensities based on data movement from different levels of cache (e.g. L1, L2 on the KNL), in order to \ndiscover the relevant limiting cache level. The figure below shows an example of such a plot of an application limited by the L2 cache level.\n\n\n\n\nIn addition, for applications with non-stream like memory access patterns, lower memory-ceilings may be computed \nfrom benchmark values. For example, many codes use strided or indirect-addressed (scatter/gather) patterns. In some cases memory-latency is the limiting resources. \nFor convenience we include a table of empirical/effective bandwidths on the KNL nodes (Cori/Theta) and Titan GPUs for \ndifferent patterns. \n\n\nINSERT Table from Sam?\n\n\nFinally, one may additional define an AI value and roofline-ceiling for data coming from off-node due to internode \ncommunication. The relevant bandwidth here is the injection bandwidth of the node:\n\n\nINSERT Table from Sam?\n\n\nThe value of the roofline approach is that relative performance of an application kernel to relevant ceilings (those related to fundamental limitations in an algorithm \nthat cannot be overcome via optimization) allow us to define an absolute performance ratio for each architecture to quantity absolute performance and performance \nportability.", 
            "title": "Measurement Techniques"
        }, 
        {
            "location": "/perfport/measurements/#measuring-performance-portability", 
            "text": "As discussed in the previous section, performance portability can be an elusive topic to quantify \nand different engineers often provide different definitions or measurement techniques.", 
            "title": "Measuring Performance Portability"
        }, 
        {
            "location": "/perfport/measurements/#measuring-portability", 
            "text": "Measuring 'portability' itself is somewhat more well defined. One can, in principle, measure the \ntotal lines of code used in common across different architectures vs. the amount of code intended \nfor a single architecture via  IFDEF  pre-processing statements, separate routines and the like. A code with 0% \narchitecture specfic code being completely portable and a code with a 100% architecture specific \ncode being essentially made up of multiple applications for each architecture.   One subtlety that this approach hides is that it is possible that shared source-code requires more lines than source-code intended for a single architecture \nor, in some cases, even two sets of separate source-code intended for multiple architectures. We ignore this case for now, assuming that using a portable \napproach to express an algorithm doesn't signficant change the amount of code required.", 
            "title": "Measuring Portability"
        }, 
        {
            "location": "/perfport/measurements/#measuring-performance", 
            "text": "'Performance', even on a single architecture, is a bit less simple to define and measure. In \npractice, scientists generally care about the quality and quantity of scientific output they \nproduce. This typically maps for them to relative performance concepts, such as how much faster \ncan a particular run or set of runs run today than yesterday or on this machine than that. The \ndrawback of trying to measure performance in this way is that the baseline is arbitrary - i.e. you \ndon't know how well your code is performing on any architecture compared to how it 'should' be \nperforming if it were well optimized.  One may in principle define absolute performance as a measure of the actual floating point operations (or, for example, integer operations) per second \n(FLOPS) of an \napplication during execution compared to the theoretical peak performance of the system or fraction of the system in use, as say reported on the Top 500 \nlist -  Top500.org  - or as reported in the system specs on NERSC, ALCF and OLCF websites.  However, this is a poor measure of application performance (and a particularly poor measure to use when trying to quantify performance portability) for a \nnumber of reasons:    The application or algorithm may be fundamentally limited by an aspect of the HPC system other than the compute capability (number of cores/theads, \nclock-speed and vector/instruction-sets)    The application or algorithm may be fundamentally limited by  different  aspects of the system on different HPC system.     As an example, an implemenation of an algorithm that is limited by memory bandwidth may be achieving the best performance it theoretially can multiple \narchitectures but could be achieving widely varying percentage of peaks FLOPS on the different systems.   Instead we advocate for one of two approaches for defining performance against expected or optimal performance on the system for algorithm:", 
            "title": "Measuring Performance"
        }, 
        {
            "location": "/perfport/measurements/#1-compare-against-a-known-well-recognized-potentially-non-portable-implementation", 
            "text": "Some applications, algorithms or methods have well-recognized optimal (often hand-tuned) implementations on different architectures. These can be used as a \nbaseline for defining relative performance of portable versions. Our Chroma application case-study shows this approach.  See \nhere    Many performance tools exist at ALCF, NERSC and OLCF for the purposes profiling applications, regions of applications and determining performance limiters \nwhen comparing different implementation of an algorithm or method. See the comprehensive list  here  with links to detailed \ninstructions and example use-cases at each site.", 
            "title": "1. Compare against a known, well-recognized (potentially non-portable), implementation."
        }, 
        {
            "location": "/perfport/measurements/#2-use-the-roofline-approach-to-compare-actual-to-expected-performance", 
            "text": "As discussed above, the major limitation of defining performance relative to the peak FLOPS capability of the system is that applications in practice are \nlimited by many different aspects of an HPC system.   The roofline performance model and extensions to the roofline model attempt to take these into account. In the roofline approach, one defines various \ntheoretial performance ceilings for an algorithm or implementation with various properties. In the simplest model, one may classify an algorithm based on \nits DRAM arithmetic-intensity - that is the ratio of the FLOPs performed vs the data moved from main-memory (DRAM) to the processor over the course of \nexecution, which can be measured for a given application as described on the subpages. Below, we show the performance ceilings provided by the roofline \nmodel on KNL for applications as a function of the DRAM arithmetic-intensity:   Here the blue line represents the optimal performance on the system that can be achieved for an application running out of the KNL High-Bandwidth Memory \n(HBM) with a given\nDRAM-AI (the x-axis value). For low\nvalues of DRAM-AI, the performance is limited by the diagonal ceiling, meaning that memory-bandwidth is the limiting factor. The location of the diagonal\nline are typically computed empiracally from the available bandwidth reported by stream triad. CITE stream.  For high values of DRAM-AI,\nmemory bandwidth no longer limits performance and one can, in principle, achieve the max compute performance on the system. However, for such cases we draw \nother ceilings that represent common limitations in algorithms or implementations of algorithms. The dashed-dotted green line labeled \"-ILP\" is the\nperformance ceiling for applications that: 1. do not provide a balance of multiply and add instructions, or simply don't use the Fused Multiply Add (FMA)\ninstructions on the processor and 2. don't have enough instruction level parallelism to keep both VPUs on the KNL busy. The dashed purple line labeled \n\"-Vectorization\" is performance ceiling of an algorithm or implementation that, in addition to the above two deficiencies, lacks vectorization (a \ncombined factor of 32 reduction in the ceiling).  For applications that limited by other system properties, it is possible to extend the roofline model to include related ceilings. For example, we commonly   \nextend the roofline approach to use arithmetic-intensities based on data movement from different levels of cache (e.g. L1, L2 on the KNL), in order to \ndiscover the relevant limiting cache level. The figure below shows an example of such a plot of an application limited by the L2 cache level.   In addition, for applications with non-stream like memory access patterns, lower memory-ceilings may be computed \nfrom benchmark values. For example, many codes use strided or indirect-addressed (scatter/gather) patterns. In some cases memory-latency is the limiting resources. \nFor convenience we include a table of empirical/effective bandwidths on the KNL nodes (Cori/Theta) and Titan GPUs for \ndifferent patterns.   INSERT Table from Sam?  Finally, one may additional define an AI value and roofline-ceiling for data coming from off-node due to internode \ncommunication. The relevant bandwidth here is the injection bandwidth of the node:  INSERT Table from Sam?  The value of the roofline approach is that relative performance of an application kernel to relevant ceilings (those related to fundamental limitations in an algorithm \nthat cannot be overcome via optimization) allow us to define an absolute performance ratio for each architecture to quantity absolute performance and performance \nportability.", 
            "title": "2. Use the roofline approach to compare actual to expected performance"
        }, 
        {
            "location": "/perfport/measurements/knl/", 
            "text": "", 
            "title": "Measuring Roofline Data on KNL"
        }, 
        {
            "location": "/perfport/measurements/gpu/", 
            "text": "Measuring Rooflin Quantities on NVIDIA GPUs\n\n\nIt is possible to measure roofline quantities for a kernel on a GPU using the NVProf tool as described \nhere\n. \n\n\nIn general, one wants to compute arithmetic intensity as well as FLOPS which involves three quantities:\n\n\n\n\nNumber of floating point operations\n\n\nData volume moved to and from DRAM\n\n\nThe runtime in seconds\n\n\n\n\nHere are the steps to do this with NVProf:\n\n\n\n\nUse gpu-trace mode to collect the time spent in the kernel you are interested in\n\n\n\n\ncommand: nvprof --print-gpu-trace ./build/bin/hpgmg-fv \n6\n \n8\n\noutput: \nTime\n(\n%\n)\n      Time     Calls    Avg           Min                Max           Name\n \n51\n.96%  \n2\n.52256s   \n1764\n  \n1\n.4300ms  \n1\n.4099ms  \n1\n.4479ms      void smooth_kernel\nint\n=\n7\n, \nint\n=\n16\n, \nint\n=\n4\n, \nint\n=\n16\n(\nlevel_type, int, int, double, double, int, double*, double*\n)\n\n\n\n\n\n\n\n\n\nUse the metric summary mode (you can specify the target kernel) to collect information such as:\n\n\n\n\n\n\nFloating point ops\n\n\n\n\nDRAM R/W transactions\n\n\nDRAM R/W throughput\n\n\n\n\nnvprof command to watch:\n\n\nFP =double precision ops \nDR/ DW = dram read/write transactions\nTR/TW = dram read/write throughput\nfor the CUDA kernel -- smooth_kernel:\n\n\nnvprof  --kernels \nsmooth_kernel\n --metrics flop_count_dp  --metrics dram_read_throughput  --metrics dram_write_throughput --metrics dram_read_transactions --metrics \ndram_write_transactions ./build/bin/hpgmg-fv \n6\n \n8\n \n\n\n\n\nTo compute Arithmetic Intensity you can use the following methods:\n\n\nMethod I:  \n\n\nFP / ( DR + DW ) * (size of transaction = 32 Bytes)\n\n\nMethod II:\n\n\nFP / (TR + TW) * time taken by kernel (computed by step 1)", 
            "title": "Measuring Roofline Data on GPUs"
        }, 
        {
            "location": "/perfport/libraries/", 
            "text": "Libraries\n\n\nThe use of scientific libraries to achieve a measure of portable performance \nhas been used across many earlier computational platforms. \nThe ability to use higher-level abstractions via libraries allows developers to \nconcentrate their effort on algorithmic development, freeing them from having to \ndevote considerable effort to maximizing performance for many mathematical primitives. The most popular scientific libraries include \npackages designed to solve problems in linear algebra (both dense and sparse), compute fast Fourier transforms (FFT), multigrid \nmethods, and initial value problems for ordinary differential \nequations, along with other examples. Some of the best known scientific libraries include:\n\n\nThese tasks often represent a good measure of the computational work to be found in many scientific \ncodes. For some codes, almost all of the computational intensity can be found in the use of a\nlibrary for, e.g., eigenvalue solution or FFTs. If such a code makes use of libraries for their solution, \nportability is very often assured. Indeed, even if a particular library has not been ported to a new\narchitecture at a given time, the library source code is often available and can be compiled by the\nuser on the new platform. However, the best performance is realized when either the library maintainers or\nthe machine vendor (or both) undertake development to optimize a given library on a particular platform. \nThis obvious advantage has been realized by vendors, and for many of the libraries referred to earlier, this \noptimization is done as a matter of course.\n\n\nConversely, performance cannot be guaranteed with the same degree of certainty. First, although libraries often \ndo encapsulate a good measure of the required work, in most cases this is not \nall\n of the work, including \nwhat is often strictly serial work. This fundamental constraint is sometimes exacerbated by the\nfact that architecture-specific implementations are evolving, despite the best efforts of both vendors and\nlibrary maintainers. \n\n\nCodes with obvious \"hot spots\" can often make immediate use of libraries to acheive performance portability. \nThis is often easiest for codes written in Fortran and C, whereas bindings to many libraries in C++ can \nbe lacking or somewhat arcane to use. One of the biggest concerns in using libraries for extant codes is \nthe frequent requirement to recast data structures used in the code to the format used by the library. \nThe best approach to ameliorate this problem is often to simply use a memory copy: The relative cost of the \ncopy compared to the work done in the library is often small, and the use of a localized copy obviates the\nneed to change data structures pervasively throughout the code.  \n\n\nConsiderations for using libraries for portable performance\n\n\nPros\n\n\n\n\n\n\nOften encapsulate much of the computational intensity found in scientific codes\n\n\n\n\n\n\nCan allow immediate portability under some circumstances\n\n\n\n\n\n\nPerformance becomes a task for library authors/maintainers\n\n\n\n\n\n\nCons\n\n\n\n\n\n\nLimited set of portable libraries at present\n\n\n\n\n\n\nMay not capture all the important/expensive tasks in a given code  \n\n\n\n\n\n\nOften require recasting data structures to match library requirements\n\n\n\n\n\n\nOpaque interior threading models \n\n\n\n\n\n\nSome popular scientific libraries available on ASCR facilities\n\n\n\n\n\n\nBLAS/LAPACK\n - dense linear algebra\n\n\n\n\n\n\nBLAS and LAPACK are often contained in vendor-supplied library collections, like:\n\n\n\n\n\n\nMKL\n (Theta, Cori)\n\n\n\n\n\n\nCray LibSci\n (Theta, Cori, Titan) \n\n\n\n\n\n\n\n\n\n\nIn addition, other platform-specific implementations are avaialable, like:\n\n\n\n\n\n\nMAGMA\n (GPU; Titan)\n\n\n\n\n\n\nPLASMA\n (multicore; Theta, Cori)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFFTW\n - Fast Fourier Transform\n\n\n\n\nLike LAPACK/BLAS, FFTW-like APIs can be found in MKL and ACML\n\n\n\n\n\n\n\n\nPETSc\n - PDE solvers\n\n\n\n\nPETSc is much more like a framework, and often requires more extensive code changes to use efficiently", 
            "title": "Libraries"
        }, 
        {
            "location": "/perfport/libraries/#libraries", 
            "text": "The use of scientific libraries to achieve a measure of portable performance \nhas been used across many earlier computational platforms. \nThe ability to use higher-level abstractions via libraries allows developers to \nconcentrate their effort on algorithmic development, freeing them from having to \ndevote considerable effort to maximizing performance for many mathematical primitives. The most popular scientific libraries include \npackages designed to solve problems in linear algebra (both dense and sparse), compute fast Fourier transforms (FFT), multigrid \nmethods, and initial value problems for ordinary differential \nequations, along with other examples. Some of the best known scientific libraries include:  These tasks often represent a good measure of the computational work to be found in many scientific \ncodes. For some codes, almost all of the computational intensity can be found in the use of a\nlibrary for, e.g., eigenvalue solution or FFTs. If such a code makes use of libraries for their solution, \nportability is very often assured. Indeed, even if a particular library has not been ported to a new\narchitecture at a given time, the library source code is often available and can be compiled by the\nuser on the new platform. However, the best performance is realized when either the library maintainers or\nthe machine vendor (or both) undertake development to optimize a given library on a particular platform. \nThis obvious advantage has been realized by vendors, and for many of the libraries referred to earlier, this \noptimization is done as a matter of course.  Conversely, performance cannot be guaranteed with the same degree of certainty. First, although libraries often \ndo encapsulate a good measure of the required work, in most cases this is not  all  of the work, including \nwhat is often strictly serial work. This fundamental constraint is sometimes exacerbated by the\nfact that architecture-specific implementations are evolving, despite the best efforts of both vendors and\nlibrary maintainers.   Codes with obvious \"hot spots\" can often make immediate use of libraries to acheive performance portability. \nThis is often easiest for codes written in Fortran and C, whereas bindings to many libraries in C++ can \nbe lacking or somewhat arcane to use. One of the biggest concerns in using libraries for extant codes is \nthe frequent requirement to recast data structures used in the code to the format used by the library. \nThe best approach to ameliorate this problem is often to simply use a memory copy: The relative cost of the \ncopy compared to the work done in the library is often small, and the use of a localized copy obviates the\nneed to change data structures pervasively throughout the code.", 
            "title": "Libraries"
        }, 
        {
            "location": "/perfport/libraries/#considerations-for-using-libraries-for-portable-performance", 
            "text": "Pros    Often encapsulate much of the computational intensity found in scientific codes    Can allow immediate portability under some circumstances    Performance becomes a task for library authors/maintainers    Cons    Limited set of portable libraries at present    May not capture all the important/expensive tasks in a given code      Often require recasting data structures to match library requirements    Opaque interior threading models", 
            "title": "Considerations for using libraries for portable performance"
        }, 
        {
            "location": "/perfport/libraries/#some-popular-scientific-libraries-available-on-ascr-facilities", 
            "text": "BLAS/LAPACK  - dense linear algebra    BLAS and LAPACK are often contained in vendor-supplied library collections, like:    MKL  (Theta, Cori)    Cray LibSci  (Theta, Cori, Titan)       In addition, other platform-specific implementations are avaialable, like:    MAGMA  (GPU; Titan)    PLASMA  (multicore; Theta, Cori)        FFTW  - Fast Fourier Transform   Like LAPACK/BLAS, FFTW-like APIs can be found in MKL and ACML     PETSc  - PDE solvers   PETSc is much more like a framework, and often requires more extensive code changes to use efficiently", 
            "title": "Some popular scientific libraries available on ASCR facilities"
        }, 
        {
            "location": "/perfport/directives/openacc/", 
            "text": "(For Bronson to write)", 
            "title": "OpenACC"
        }, 
        {
            "location": "/perfport/directives/openacc/#for-bronson-to-write", 
            "text": "", 
            "title": "(For Bronson to write)"
        }, 
        {
            "location": "/perfport/directives/openmp/", 
            "text": "OpenMP\n\n\nOpenMP is a specification for a set of compiler directives, library routines,\nand environment variables that can be used to specify high-level parallelism in\nFortran and C/C++ programs. The OpenMP API uses the fork-join model of parallel\nexecution. Multiple threads of execution perform tasks defined implicitly or\nexplicitly by OpenMP directives. (\nText taken from \nOpenMP\nFAQ\n and \nAPI\nspecification\n.\n)\n\n\nAlthough the directives in early versions of the OpenMP specification focused\non thread-level parallelism, more recent versions (especially 4.0 and 4.5) have\ngeneralized the specification to address more complex types (and multiple\ntypes) of parallelism, reflecting the increasing degree of on-node parallelism\nin HPC architectures. In particular, OpenMP 4.0 introduced the \nsimd\n and\n\ntarget\n constructs. We discuss each of these in detail below.\n\n\nomp simd\n\n\nDecorating a loop with the \nsimd\n construct informs the compiler that the loop\niterations are independent and can be executed with SIMD instructions (e.g.,\nAVX-512 on Intel Xeon Phi), e.g.,\n\n\n!$omp simd\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end simd\n\n\n\n\n\nExample output from a compiler optimization report for this loop is as follows:\n\nLOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15388: vectorization support: reference C(i) has aligned access   [ main.f90(10,19) ]\n   remark #15305: vectorization support: vector length 16\n   remark #15399: vectorization support: unroll factor set to 4\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 2\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.310\n   remark #15478: estimated potential speedup: 19.200\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\nThe \nsimd\n construct can be combined with the traditional \nparallel for\n (or\n\nparallel do\n in Fortran) constructs in order to execute the loop with both\nmulti-threading and with SIMD instructions, e.g.,\n\n\n!$omp parallel do simd\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end parallel do simd\n\n\n\n\n\nThe optimization report for the above snippet is as follows:\n\n\nBegin optimization report for: MAIN\n\n    Report from: OpenMP optimizations [openmp]\n\nmain.f90(8:9-8:9):OMP:MAIN__:  OpenMP DEFINED LOOP WAS PARALLELIZED\n\n    Report from: Vector optimizations [vec]\n\nLOOP BEGIN at main.f90(8,9)\n   remark #15388: vectorization support: reference a(i) has aligned access   [ main.f90(10,5) ]\n   remark #15389: vectorization support: reference b(i) has unaligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference c(i) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.667\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.370\n   remark #15478: estimated potential speedup: 15.670\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\n\nIt is important to note that compilers generally analyze loops (even those\nundecorated with \nomp simd\n) to determine if they can be executed with SIMD\ninstructions; applying this OpenMP construct usually allows the compiler to\nskip its loop dependency checks and immediately generate a SIMD version of the\nloop. Consequently, improper use of \nomp simd\n, e.g., on a loop which indeed\ncarries dependencies between iterations, can generate wrong code. This\nconstruct shifts the burden of correctness from the compiler to the user.\n\n\nFor example, consider the following loop, with a write-after-read dependency:\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \na\n(\ni\n-\n1\n)\n\n\nend do\n\n\n\n\nAttempting to compile it without the \nsimd\n construct yields the following\noptimization report:\n\n\nLOOP BEGIN at main.f90(8,3)\n   remark #15344: loop was not vectorized: vector dependence prevents vectorization\n   remark #15346: vector dependence: assumed FLOW dependence between a(i) (9:5) and a(i-1) (9:5)\nLOOP END\n\n\n\n\nThe compiler has determined that the loop iterations cannot be executed in\nSIMD. However, if we introduce the \nsimd\n construct, this assures the compiler\n(incorrectly) that the loop iterations can be executed in SIMD. Using the\nconstruct results in the following report:\n\n\nLOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference A(i-1) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 1\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.340\n   remark #15478: estimated potential speedup: 17.450\n   remark #15488: --- end vector cost summary ---\nLOOP END\n\n\n\n\nThis example illustrates the prescriptive nature of OpenMP directives; they\nallow the user to instruct the compiler precisely how on-node parallelism\nshould be expressed, even if the compiler's own correctness-checking heuristics\nindicate that the desired approach will generate incorrect results.\n\n\nomp target\n\n\nThe OpenMP \ntarget\n device construct maps variables to a device data\nenvironment and executes the construct on that device. A region enclosed with\nthe \ntarget\n construct is assigned a target task to be executed on the device.\nThis construct supports several additional keywords which provide the user with\ncontrol of which data is moved to and from the device. Specifically, data\nmovement is achieved via the \nmap\n keyword, which accepts a list of variables\nto be copied between the host and device.\n\n\nConsider the following snippet:\n\n!$omp target map(to:b,c) map(from:a)\n\n\ndo \ni\n \n=\n \n1\n,\n \narray_size\n\n  \na\n(\ni\n)\n \n=\n \nb\n(\ni\n)\n \n*\n \nc\n(\ni\n)\n\n\nend do\n\n\n!$omp end target\n\n\n\n\nThe compiler report from the following code offloaded to an Intel Xeon Phi\ncoprocessor is as follows:\n\n\n    Report from: Offload optimizations [offload]\n\nOFFLOAD:main(8,9):  Offload to target MIC 1\n Evaluate length/align/alloc_if/free_if/alloc/into expressions\n   Modifier expression assigned to __offload_free_if.19\n   Modifier expression assigned to __offload_alloc_if.20\n   Modifier expression assigned to __offload_free_if.21\n   Modifier expression assigned to __offload_alloc_if.22\n   Modifier expression assigned to __offload_free_if.23\n   Modifier expression assigned to __offload_alloc_if.24\n Data sent from host to target\n       i, scalar size 4 bytes\n       __offload_stack_ptr_main_$C_V$5.0, pointer to array reference expression with base\n       __offload_stack_ptr_main_$B_V$6.0, pointer to array reference expression with base\n Data received by host from target\n       __offload_stack_ptr_MAIN__.34, pointer to array reference expression with base \n\nLOOP BEGIN at main.f90(12,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(13,5) ]\n   remark #15389: vectorization support: reference B(i) has unaligned access   [ main.f90(13,12) ]\n   remark #15389: vectorization support: reference C(i) has unaligned access   [ main.f90(13,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.654\n   remark #15300: LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 7\n   remark #15477: vector cost: 0.400\n   remark #15478: estimated potential speedup: 17.180\n   remark #15488: --- end vector cost summary ---\n   remark #25015: Estimate of max trip count of loop=1024\nLOOP END\n\n\n\n\nThe same code offloaded to an NVIDIA Tesla GPU shows the following compiler\nreport (from a different compiler than the ones shown above):\n\n\n    1.           program main\n    2.             implicit none\n    3.\n    4.             integer, parameter :: array_size = 65536\n    5.             real, dimension(array_size) :: a, b, c\n    6.             integer :: i\n    7.\n    8.    fA--\n   b(:) = 1.0\n    9.    f---\n   c(:) = 2.0\n   10.\n   11.  + G----\n   !$omp target map(to:b,c) map(from:a)\n   12.    G g--\n   do i = 1, array_size\n   13.    G g        a(i) = b(i) * c(i)\n   14.    G g--\n   end do\n   15.    G----\n   !$omp end target\n   16.\n   17.             print *, a(1)\n   18.\n   19.           end program main\n\nftn-6230 ftn: VECTOR MAIN, File = main.f90, Line = 8\n  A loop starting at line 8 was replaced with multiple library calls.\n\nftn-6004 ftn: SCALAR MAIN, File = main.f90, Line = 9\n  A loop starting at line 9 was fused with the loop starting at line 8.\n\nftn-6405 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  A region starting at line 11 and ending at line 15 was placed on the accelerator.\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array \nc\n to accelerator, free at line 15 (acc_copyin).\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array \nb\n to accelerator, free at line 15 (acc_copyin).\n\nftn-6420 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory for whole array \na\n on accelerator, copy back at line 15 (acc_copyout).\n\nftn-6430 ftn: ACCEL MAIN, File = main.f90, Line = 12\n  A loop starting at line 12 was partitioned across the 128 threads within a threadblock.\n\n\n\n\nNote in the last compiler report that OpenMP automatically threads the loop and\npartitions the threads into threadblocks of the appropriate size for the device\nexecuting the loop.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/perfport/directives/openmp/#openmp", 
            "text": "OpenMP is a specification for a set of compiler directives, library routines,\nand environment variables that can be used to specify high-level parallelism in\nFortran and C/C++ programs. The OpenMP API uses the fork-join model of parallel\nexecution. Multiple threads of execution perform tasks defined implicitly or\nexplicitly by OpenMP directives. ( Text taken from  OpenMP\nFAQ  and  API\nspecification . )  Although the directives in early versions of the OpenMP specification focused\non thread-level parallelism, more recent versions (especially 4.0 and 4.5) have\ngeneralized the specification to address more complex types (and multiple\ntypes) of parallelism, reflecting the increasing degree of on-node parallelism\nin HPC architectures. In particular, OpenMP 4.0 introduced the  simd  and target  constructs. We discuss each of these in detail below.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/perfport/directives/openmp/#omp-simd", 
            "text": "Decorating a loop with the  simd  construct informs the compiler that the loop\niterations are independent and can be executed with SIMD instructions (e.g.,\nAVX-512 on Intel Xeon Phi), e.g.,  !$omp simd  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end simd   Example output from a compiler optimization report for this loop is as follows: LOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15388: vectorization support: reference C(i) has aligned access   [ main.f90(10,19) ]\n   remark #15305: vectorization support: vector length 16\n   remark #15399: vectorization support: unroll factor set to 4\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 2\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.310\n   remark #15478: estimated potential speedup: 19.200\n   remark #15488: --- end vector cost summary ---\nLOOP END  The  simd  construct can be combined with the traditional  parallel for  (or parallel do  in Fortran) constructs in order to execute the loop with both\nmulti-threading and with SIMD instructions, e.g.,  !$omp parallel do simd  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end parallel do simd   The optimization report for the above snippet is as follows:  Begin optimization report for: MAIN\n\n    Report from: OpenMP optimizations [openmp]\n\nmain.f90(8:9-8:9):OMP:MAIN__:  OpenMP DEFINED LOOP WAS PARALLELIZED\n\n    Report from: Vector optimizations [vec]\n\nLOOP BEGIN at main.f90(8,9)\n   remark #15388: vectorization support: reference a(i) has aligned access   [ main.f90(10,5) ]\n   remark #15389: vectorization support: reference b(i) has unaligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference c(i) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.667\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.370\n   remark #15478: estimated potential speedup: 15.670\n   remark #15488: --- end vector cost summary ---\nLOOP END  It is important to note that compilers generally analyze loops (even those\nundecorated with  omp simd ) to determine if they can be executed with SIMD\ninstructions; applying this OpenMP construct usually allows the compiler to\nskip its loop dependency checks and immediately generate a SIMD version of the\nloop. Consequently, improper use of  omp simd , e.g., on a loop which indeed\ncarries dependencies between iterations, can generate wrong code. This\nconstruct shifts the burden of correctness from the compiler to the user.  For example, consider the following loop, with a write-after-read dependency: do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   a ( i - 1 )  end do   Attempting to compile it without the  simd  construct yields the following\noptimization report:  LOOP BEGIN at main.f90(8,3)\n   remark #15344: loop was not vectorized: vector dependence prevents vectorization\n   remark #15346: vector dependence: assumed FLOW dependence between a(i) (9:5) and a(i-1) (9:5)\nLOOP END  The compiler has determined that the loop iterations cannot be executed in\nSIMD. However, if we introduce the  simd  construct, this assures the compiler\n(incorrectly) that the loop iterations can be executed in SIMD. Using the\nconstruct results in the following report:  LOOP BEGIN at main.f90(9,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(10,5) ]\n   remark #15388: vectorization support: reference B(i) has aligned access   [ main.f90(10,12) ]\n   remark #15389: vectorization support: reference A(i-1) has unaligned access   [ main.f90(10,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15448: unmasked aligned unit stride loads: 1\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 1\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 6\n   remark #15477: vector cost: 0.340\n   remark #15478: estimated potential speedup: 17.450\n   remark #15488: --- end vector cost summary ---\nLOOP END  This example illustrates the prescriptive nature of OpenMP directives; they\nallow the user to instruct the compiler precisely how on-node parallelism\nshould be expressed, even if the compiler's own correctness-checking heuristics\nindicate that the desired approach will generate incorrect results.", 
            "title": "omp simd"
        }, 
        {
            "location": "/perfport/directives/openmp/#omp-target", 
            "text": "The OpenMP  target  device construct maps variables to a device data\nenvironment and executes the construct on that device. A region enclosed with\nthe  target  construct is assigned a target task to be executed on the device.\nThis construct supports several additional keywords which provide the user with\ncontrol of which data is moved to and from the device. Specifically, data\nmovement is achieved via the  map  keyword, which accepts a list of variables\nto be copied between the host and device.  Consider the following snippet: !$omp target map(to:b,c) map(from:a)  do  i   =   1 ,   array_size \n   a ( i )   =   b ( i )   *   c ( i )  end do  !$omp end target   The compiler report from the following code offloaded to an Intel Xeon Phi\ncoprocessor is as follows:      Report from: Offload optimizations [offload]\n\nOFFLOAD:main(8,9):  Offload to target MIC 1\n Evaluate length/align/alloc_if/free_if/alloc/into expressions\n   Modifier expression assigned to __offload_free_if.19\n   Modifier expression assigned to __offload_alloc_if.20\n   Modifier expression assigned to __offload_free_if.21\n   Modifier expression assigned to __offload_alloc_if.22\n   Modifier expression assigned to __offload_free_if.23\n   Modifier expression assigned to __offload_alloc_if.24\n Data sent from host to target\n       i, scalar size 4 bytes\n       __offload_stack_ptr_main_$C_V$5.0, pointer to array reference expression with base\n       __offload_stack_ptr_main_$B_V$6.0, pointer to array reference expression with base\n Data received by host from target\n       __offload_stack_ptr_MAIN__.34, pointer to array reference expression with base \n\nLOOP BEGIN at main.f90(12,3)\n   remark #15388: vectorization support: reference A(i) has aligned access   [ main.f90(13,5) ]\n   remark #15389: vectorization support: reference B(i) has unaligned access   [ main.f90(13,12) ]\n   remark #15389: vectorization support: reference C(i) has unaligned access   [ main.f90(13,19) ]\n   remark #15381: vectorization support: unaligned access used inside loop body\n   remark #15305: vectorization support: vector length 32\n   remark #15399: vectorization support: unroll factor set to 2\n   remark #15309: vectorization support: normalized vectorization overhead 0.654\n   remark #15300: LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 2\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 7\n   remark #15477: vector cost: 0.400\n   remark #15478: estimated potential speedup: 17.180\n   remark #15488: --- end vector cost summary ---\n   remark #25015: Estimate of max trip count of loop=1024\nLOOP END  The same code offloaded to an NVIDIA Tesla GPU shows the following compiler\nreport (from a different compiler than the ones shown above):      1.           program main\n    2.             implicit none\n    3.\n    4.             integer, parameter :: array_size = 65536\n    5.             real, dimension(array_size) :: a, b, c\n    6.             integer :: i\n    7.\n    8.    fA--    b(:) = 1.0\n    9.    f---    c(:) = 2.0\n   10.\n   11.  + G----    !$omp target map(to:b,c) map(from:a)\n   12.    G g--    do i = 1, array_size\n   13.    G g        a(i) = b(i) * c(i)\n   14.    G g--    end do\n   15.    G----    !$omp end target\n   16.\n   17.             print *, a(1)\n   18.\n   19.           end program main\n\nftn-6230 ftn: VECTOR MAIN, File = main.f90, Line = 8\n  A loop starting at line 8 was replaced with multiple library calls.\n\nftn-6004 ftn: SCALAR MAIN, File = main.f90, Line = 9\n  A loop starting at line 9 was fused with the loop starting at line 8.\n\nftn-6405 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  A region starting at line 11 and ending at line 15 was placed on the accelerator.\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array  c  to accelerator, free at line 15 (acc_copyin).\n\nftn-6418 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory and copy whole array  b  to accelerator, free at line 15 (acc_copyin).\n\nftn-6420 ftn: ACCEL MAIN, File = main.f90, Line = 11\n  If not already present: allocate memory for whole array  a  on accelerator, copy back at line 15 (acc_copyout).\n\nftn-6430 ftn: ACCEL MAIN, File = main.f90, Line = 12\n  A loop starting at line 12 was partitioned across the 128 threads within a threadblock.  Note in the last compiler report that OpenMP automatically threads the loop and\npartitions the threads into threadblocks of the appropriate size for the device\nexecuting the loop.", 
            "title": "omp target"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/", 
            "text": "Kokkos\n\n\nKokkos\n implements a programming model in\nC++ for writing performance portable applications targeting all major HPC\nplatforms. For that purpose it provides abstractions for both parallel\nexecution of code and data management. Kokkos is designed to target complex\nnode architectures with N-level memory hierarchies and multiple types of\nexecution resources. It currently can use OpenMP, Pthreads and CUDA as backend\nprogramming models. (\nText provided by \nREADME\n in Kokkos source code repository\n).\n\n\nKokkos provides two types of abstraction which insulate the application\ndeveloper from the details of expressing parallelism on a particular\narchitecture. One is a \"memory space\", which characterizes where data resides\nin memory, e.g., in high-bandwidth memory, in DRAM, on GPU memory, etc. The\nother type is an \"execution space\", which describes how execution of a kernel\nis parallelized.\n\n\nIn terms of implementation, Kokkos expresses its memory and execution spaces\nvia templated C++ code. One constructs memory spaces through \"Views\", which are\ntemplated multi-dimensional arrays. One then issues an execution policy on the\ndata. The following snippet shows matrix-vector multiplication using Kokkos\nviews and a \"reduction\" execution policy. It is taken from the Kokkos \nGTC2017\ntutorial\n#2\n.\n\n\n  \nKokkos\n::\nView\ndouble\n*\n  \nx\n(\n \nx\n,\n \n128\n \n);\n \n// a vector of length 128\n\n  \nKokkos\n::\nView\ndouble\n**\n \nA\n(\n \nA\n,\n \n128\n,\n \n128\n \n);\n \n// a matrix of size 128^2\n\n\n  \nKokkos\n::\nparallel_reduce\n(\n \nN\n,\n \nKOKKOS_LAMBDA\n \n(\n \nint\n \nj\n,\n \ndouble\n \nupdate\n \n)\n \n{\n\n    \ndouble\n \ntemp2\n \n=\n \n0\n;\n\n    \nfor\n \n(\n \nint\n \ni\n \n=\n \n0\n;\n \ni\n \n \nM\n;\n \n++\ni\n \n)\n \n{\n\n      \ntemp2\n \n+=\n \nA\n(\n \nj\n,\n \ni\n \n)\n \n*\n \nx\n(\n \ni\n \n);\n\n    \n}\n\n    \nupdate\n \n+=\n \ny\n(\n \nj\n \n)\n \n*\n \ntemp2\n;\n\n  \n},\n \nresult\n \n);\n\n\n\n\n\nNested parallelism\n\n\nModern CPU architectures exhibit a hierarchy of parallelism, and an application\nmust exploit the complete hierarchy in order to achieve good performance. Each\nlevel of the hierarchy is generally characterized by a group of execution\nresources which share a pool of memory.\n\n\nOn manycore CPUs such as Intel Xeon Phi, each processor contains ~70 cores,\neach of which supports 512 bit-wide SIMD instructions, and supports execution\nof 4 simultaneous hardware threads. On GPU-accelerated architectures, the host\nCPU has most of these same features, and the GPU often exhibits a very\ndifferent type of parallelism - a GPU may feature many streaming\nmultiprocessors, each of which executes a large number of threads, which are\ngrouped into clusters which execute synchronously.\n\n\nKokkos addresses this hierarchy via nested parallelism. In particular, at each\nlevel of a loop nest one can choose which execution policy to use. For example,\non Xeon Phi, one may wish to use multi-threading for the coarsest level of\nparallelism, and SIMD instructions for the finest level. On a GPU, one may wish\nto use multiple streaming multiprocessors as the coarsest level, and warps of\nthreads as the finest level. One can achieve this with the following example\ncode (taken from \nExercise 6 of the GTC2017\ntutorials\n):\n\n\nfor\n \n(\n \nint\n \nrepeat\n \n=\n \n0\n;\n \nrepeat\n \n \nnrepeat\n;\n \nrepeat\n++\n \n)\n \n{\n\n  \n// Application: \ny,Ax\n = y^T*A*x\n\n  \ndouble\n \nresult\n \n=\n \n0\n;\n\n\n  \nKokkos\n::\nparallel_reduce\n(\n \nteam_policy\n(\n \nE\n,\n \nKokkos\n::\nAUTO\n,\n \n32\n \n),\n \nKOKKOS_LAMBDA\n \n(\n\n  \nconst\n \nmember_type\n \nteamMember\n,\n \ndouble\n \nupdate\n \n)\n \n{\n\n    \nconst\n \nint\n \ne\n \n=\n \nteamMember\n.\nleague_rank\n();\n\n    \ndouble\n \ntempN\n \n=\n \n0\n;\n\n\n    \nKokkos\n::\nparallel_reduce\n(\n \nKokkos\n::\nTeamThreadRange\n(\n \nteamMember\n,\n \nN\n \n),\n \n[\n]\n \n(\n\n    \nconst\n \nint\n \nj\n,\n \ndouble\n \ninnerUpdateN\n \n)\n \n{\n\n      \ndouble\n \ntempM\n \n=\n \n0\n;\n\n\n      \nKokkos\n::\nparallel_reduce\n(\n \nKokkos\n::\nThreadVectorRange\n(\n \nteamMember\n,\n \nM\n \n),\n \n[\n]\n\n      \n(\n \nconst\n \nint\n \ni\n,\n \ndouble\n \ninnerUpdateM\n \n)\n \n{\n\n\n        \ninnerUpdateM\n \n+=\n \nA\n(\n \ne\n,\n \nj\n,\n \ni\n \n)\n \n*\n \nx\n(\n \ne\n,\n \ni\n \n);\n\n      \n},\n \ntempM\n \n);\n\n\n      \ninnerUpdateN\n \n+=\n \ny\n(\n \ne\n,\n \nj\n \n)\n \n*\n \ntempM\n;\n\n    \n},\n \ntempN\n \n);\n\n\n    \nKokkos\n::\nsingle\n(\n \nKokkos\n::\nPerTeam\n(\n \nteamMember\n \n),\n \n[\n]\n \n()\n \n{\n\n      \nupdate\n \n+=\n \ntempN\n;\n\n    \n});\n\n  \n},\n \nresult\n \n);", 
            "title": "Kokkos"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#kokkos", 
            "text": "Kokkos  implements a programming model in\nC++ for writing performance portable applications targeting all major HPC\nplatforms. For that purpose it provides abstractions for both parallel\nexecution of code and data management. Kokkos is designed to target complex\nnode architectures with N-level memory hierarchies and multiple types of\nexecution resources. It currently can use OpenMP, Pthreads and CUDA as backend\nprogramming models. ( Text provided by  README  in Kokkos source code repository ).  Kokkos provides two types of abstraction which insulate the application\ndeveloper from the details of expressing parallelism on a particular\narchitecture. One is a \"memory space\", which characterizes where data resides\nin memory, e.g., in high-bandwidth memory, in DRAM, on GPU memory, etc. The\nother type is an \"execution space\", which describes how execution of a kernel\nis parallelized.  In terms of implementation, Kokkos expresses its memory and execution spaces\nvia templated C++ code. One constructs memory spaces through \"Views\", which are\ntemplated multi-dimensional arrays. One then issues an execution policy on the\ndata. The following snippet shows matrix-vector multiplication using Kokkos\nviews and a \"reduction\" execution policy. It is taken from the Kokkos  GTC2017\ntutorial\n#2 .     Kokkos :: View double *    x (   x ,   128   );   // a vector of length 128 \n   Kokkos :: View double **   A (   A ,   128 ,   128   );   // a matrix of size 128^2 \n\n   Kokkos :: parallel_reduce (   N ,   KOKKOS_LAMBDA   (   int   j ,   double   update   )   { \n     double   temp2   =   0 ; \n     for   (   int   i   =   0 ;   i     M ;   ++ i   )   { \n       temp2   +=   A (   j ,   i   )   *   x (   i   ); \n     } \n     update   +=   y (   j   )   *   temp2 ; \n   },   result   );", 
            "title": "Kokkos"
        }, 
        {
            "location": "/perfport/frameworks/kokkos/#nested-parallelism", 
            "text": "Modern CPU architectures exhibit a hierarchy of parallelism, and an application\nmust exploit the complete hierarchy in order to achieve good performance. Each\nlevel of the hierarchy is generally characterized by a group of execution\nresources which share a pool of memory.  On manycore CPUs such as Intel Xeon Phi, each processor contains ~70 cores,\neach of which supports 512 bit-wide SIMD instructions, and supports execution\nof 4 simultaneous hardware threads. On GPU-accelerated architectures, the host\nCPU has most of these same features, and the GPU often exhibits a very\ndifferent type of parallelism - a GPU may feature many streaming\nmultiprocessors, each of which executes a large number of threads, which are\ngrouped into clusters which execute synchronously.  Kokkos addresses this hierarchy via nested parallelism. In particular, at each\nlevel of a loop nest one can choose which execution policy to use. For example,\non Xeon Phi, one may wish to use multi-threading for the coarsest level of\nparallelism, and SIMD instructions for the finest level. On a GPU, one may wish\nto use multiple streaming multiprocessors as the coarsest level, and warps of\nthreads as the finest level. One can achieve this with the following example\ncode (taken from  Exercise 6 of the GTC2017\ntutorials ):  for   (   int   repeat   =   0 ;   repeat     nrepeat ;   repeat ++   )   { \n   // Application:  y,Ax  = y^T*A*x \n   double   result   =   0 ; \n\n   Kokkos :: parallel_reduce (   team_policy (   E ,   Kokkos :: AUTO ,   32   ),   KOKKOS_LAMBDA   ( \n   const   member_type   teamMember ,   double   update   )   { \n     const   int   e   =   teamMember . league_rank (); \n     double   tempN   =   0 ; \n\n     Kokkos :: parallel_reduce (   Kokkos :: TeamThreadRange (   teamMember ,   N   ),   [ ]   ( \n     const   int   j ,   double   innerUpdateN   )   { \n       double   tempM   =   0 ; \n\n       Kokkos :: parallel_reduce (   Kokkos :: ThreadVectorRange (   teamMember ,   M   ),   [ ] \n       (   const   int   i ,   double   innerUpdateM   )   { \n\n         innerUpdateM   +=   A (   e ,   j ,   i   )   *   x (   e ,   i   ); \n       },   tempM   ); \n\n       innerUpdateN   +=   y (   e ,   j   )   *   tempM ; \n     },   tempN   ); \n\n     Kokkos :: single (   Kokkos :: PerTeam (   teamMember   ),   [ ]   ()   { \n       update   +=   tempN ; \n     }); \n   },   result   );", 
            "title": "Nested parallelism"
        }, 
        {
            "location": "/perfport/frameworks/raja/", 
            "text": "RAJA\n\n\nRAJA is a collection of C++ software abstractions, being developed at\nLawrence Livermore National Laboratory (LLNL), that enable architecture\nportability for HPC applications. The overarching goals of RAJA are to:\n\n\n\n\nMake existing (production) applications \nportable with minimal disruption\n\n\nProvide a model for new applications so that they are portable from\n    inception.\n\n\n\n\n(Text taken from RAJA \nREADME\n.)\n\n\nThe main conceptual abstraction in RAJA is a loop. A typical large multiphysics\ncode may contain O(10K) loops and these are where most computational work is\nperformed and where most fine-grained parallelism is available. RAJA defines a\nsystematic loop encapsulation paradigm that helps insulate application\ndevelopers from implementation details associated with software and hardware\nplatform choices. Such details include: non-portable compiler and\nplatform-specific directives, parallel programming model usage and constraints,\nand hardware-specific data management. \n(Text taken from \nRAJA\nPrimer\n.)\n\n\nRAJA implements three primary encapsulations: \nexecution policies\n,\n\nIndexSets\n, and \ndata type encapsulation\n. The execution policy instructs the\ncompiler regarding how the loop should execute and/or parallelized. IndexSets\ndescribe how the loop iteration space is traversed, e.g., stride-1, stride-2,\ntiled, etc. Data type encapsulation describes where and how the data is located\nin memory, e.g., its alignment on cache line boundaries, and aliasing\nproperties.\n\n\nAn example loop which adds two vectors, ported to RAJA and parallelized with\nOpenMP, is shown below (taken from the \nRAJA\nexamples\n):\n\n\n/*\n\n\n  RAJA::omp_parallel_for_exec - executes the forall loop using the\n\n\n  #pragma omp parallel for directive\n\n\n*/\n\n\nRAJA\n::\nforall\nRAJA\n::\nomp_parallel_for_exec\n\n  \n(\nRAJA\n::\nRangeSegment\n(\n0\n,\n \nN\n),\n \n[\n=\n](\nRAJA\n::\nIndex_type\n \ni\n)\n \n{\n\n    \nC\n[\ni\n]\n \n=\n \nA\n[\ni\n]\n \n+\n \nB\n[\ni\n];\n\n  \n});\n\n\n\n\n\nwhere \nRangeSegment(0, N)\n generates a sequential list of numbers from 0 to\n\nN\n. The same loop parallelized and executed on a GPU with CUDA looks similar:\n\n\nRAJA\n::\nforall\nRAJA\n::\ncuda_exec\nCUDA_BLOCK_SIZE\n\n  \n(\nRAJA\n::\nRangeSegment\n(\n0\n,\n \nN\n),\n \n[\n=\n]\n \n__device__\n(\nRAJA\n::\nIndex_type\n \ni\n)\n \n{\n\n    \nC\n[\ni\n]\n \n=\n \nA\n[\ni\n]\n \n+\n \nB\n[\ni\n];\n\n  \n});\n\n\ncheckSolution\n(\nC\n,\n \nN\n);", 
            "title": "RAJA"
        }, 
        {
            "location": "/perfport/frameworks/raja/#raja", 
            "text": "RAJA is a collection of C++ software abstractions, being developed at\nLawrence Livermore National Laboratory (LLNL), that enable architecture\nportability for HPC applications. The overarching goals of RAJA are to:   Make existing (production) applications  portable with minimal disruption  Provide a model for new applications so that they are portable from\n    inception.   (Text taken from RAJA  README .)  The main conceptual abstraction in RAJA is a loop. A typical large multiphysics\ncode may contain O(10K) loops and these are where most computational work is\nperformed and where most fine-grained parallelism is available. RAJA defines a\nsystematic loop encapsulation paradigm that helps insulate application\ndevelopers from implementation details associated with software and hardware\nplatform choices. Such details include: non-portable compiler and\nplatform-specific directives, parallel programming model usage and constraints,\nand hardware-specific data management.  (Text taken from  RAJA\nPrimer .)  RAJA implements three primary encapsulations:  execution policies , IndexSets , and  data type encapsulation . The execution policy instructs the\ncompiler regarding how the loop should execute and/or parallelized. IndexSets\ndescribe how the loop iteration space is traversed, e.g., stride-1, stride-2,\ntiled, etc. Data type encapsulation describes where and how the data is located\nin memory, e.g., its alignment on cache line boundaries, and aliasing\nproperties.  An example loop which adds two vectors, ported to RAJA and parallelized with\nOpenMP, is shown below (taken from the  RAJA\nexamples ):  /*    RAJA::omp_parallel_for_exec - executes the forall loop using the    #pragma omp parallel for directive  */  RAJA :: forall RAJA :: omp_parallel_for_exec \n   ( RAJA :: RangeSegment ( 0 ,   N ),   [ = ]( RAJA :: Index_type   i )   { \n     C [ i ]   =   A [ i ]   +   B [ i ]; \n   });   where  RangeSegment(0, N)  generates a sequential list of numbers from 0 to N . The same loop parallelized and executed on a GPU with CUDA looks similar:  RAJA :: forall RAJA :: cuda_exec CUDA_BLOCK_SIZE \n   ( RAJA :: RangeSegment ( 0 ,   N ),   [ = ]   __device__ ( RAJA :: Index_type   i )   { \n     C [ i ]   =   A [ i ]   +   B [ i ]; \n   });  checkSolution ( C ,   N );", 
            "title": "RAJA"
        }, 
        {
            "location": "/perfport/dsl/", 
            "text": "Tim and Bronson to Write", 
            "title": "DSL"
        }, 
        {
            "location": "/perfport/summary/", 
            "text": "This will be a comparison of all the approaches we tried.\n\n\n\n\n\n\n\n\nApproach\n\n\nBenefits\n\n\nChallenges\n\n\n\n\n\n\n\n\n\n\nOpenMP 4.5\n\n\nSupport for C, C++, Fortran\n\n\nreliant on quality of compiler implementation\n\n\n\n\n\n\nKokkos\n\n\n\n\n\n\n\n\n\n\nRAJA", 
            "title": "Summary"
        }, 
        {
            "location": "/case_studies/amr/overview/", 
            "text": "Thorsten / Brian to Write\n\n\nOverview of BoxLib/AMReX\n\n\nBoxLib\n is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the \nExascale\nComputing Project\n's\n\nBlock Structured Adaptive Mesh Refinement Co-Design\nCenter\n,\nBoxLib has since been superseded by\n\nAMReX\n. Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.\n\n\nBoxLib contains a wide variety of functionality:\n\n\n\n\nboundary condition exchange among boxes\n\n\nload balancing through regridding boxes among MPI processes\n\n\nmetadata operations such as computing volume intersections among boxes\n\n\nmemory management through pool allocators\n\n\n\n\nIn addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/amr/overview/#overview-of-boxlibamrex", 
            "text": "BoxLib  is a framework for developing\nparallel, block-structured, adaptive mesh refinement (AMR) applications. It is\nwritten primarily in C++, and enables scientific application development\nthrough compute kernels written primarily in Fortran. Through the  Exascale\nComputing Project 's Block Structured Adaptive Mesh Refinement Co-Design\nCenter ,\nBoxLib has since been superseded by AMReX . Both frameworks are publicly\navailable. The DOE COE for Performance Portability began prior to the formation\nof the Co-Design Center; consequently,the efforts described here focus on\nBoxLib, although the functionality described here is largely the same between\nthe two frameworks.  BoxLib contains a wide variety of functionality:   boundary condition exchange among boxes  load balancing through regridding boxes among MPI processes  metadata operations such as computing volume intersections among boxes  memory management through pool allocators   In addition these, BoxLib also provides linear solvers which use geometric\nmultigrid methods to solve problems on both cell-centered and nodal data. Our\nperformance portability efforts described here focus on the cell-centered\nsolver, which is algorithmically simpler than the nodal solver.", 
            "title": "Overview of BoxLib/AMReX"
        }, 
        {
            "location": "/case_studies/amr/parallelism/", 
            "text": "Parallelization\n\n\nBoxLib implements parallelization through a hybrid MPI+OpenMP approach.\n\n\nMPI\n\n\nAt the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series. An example is shown in the figure below, where\nthe red and green boxes are assigned to the same MPI process.\n\n\n\n\nOpenMP\n\n\nBoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.\n\n\nThis OpenMP parallelism is illustrated in the figure below. The box\ndistribution is the same as in the figure above, except in this case each box\nis further decomposed into smaller tiles. BoxLib then builds a list of all\ntiles comprising all boxes owned by a given MPI process, and distributes the\nlist among the OpenMP threads in the process. The figure below illustrates this\nprocess by color-coding each tile, with unique threads assigned to each color,\nsuch that the same thread may operate on tiles spanning different boxes. This\napproach avoids unnecessary thread synchronization which would occur if threads\nwere distributed among tiles within each box.\n\n\n\n\nThe figures on this page are taken from the AMReX User's Guide.", 
            "title": "Parallelism"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#parallelization", 
            "text": "BoxLib implements parallelization through a hybrid MPI+OpenMP approach.", 
            "title": "Parallelization"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#mpi", 
            "text": "At the coarsest level, BoxLib decomposes the problem domain into rectangular\nboxes, and distributes these among MPI processes. Each process follows an\n\"owner computes\" model, wherein it loops over its own boxes, executing Fortran\nkernels on each box in series. An example is shown in the figure below, where\nthe red and green boxes are assigned to the same MPI process.", 
            "title": "MPI"
        }, 
        {
            "location": "/case_studies/amr/parallelism/#openmp", 
            "text": "BoxLib adds an additional layer of parallelism within each MPI process through\nOpenMP threading, specifically by decomposing its set of boxes into a set of\nsmaller \"tiles\", which are then distributed among OpenMP threads. Although\nthese tiles can be arbitrarily shaped, by default they are pencil-shaped, being\nlong in the stride-1 memory access dimension (the x-dimension in Fortran\nkernels), and short in the other two dimensions, in order to attain high cache\nreuse and optimal hardware memory prefetching. As with the MPI parallelism, the\nOpenMP tile box parallelism also follows an \"owner computes\" model, but at the\nfiner-grained thread level, rather than at the process level.  This OpenMP parallelism is illustrated in the figure below. The box\ndistribution is the same as in the figure above, except in this case each box\nis further decomposed into smaller tiles. BoxLib then builds a list of all\ntiles comprising all boxes owned by a given MPI process, and distributes the\nlist among the OpenMP threads in the process. The figure below illustrates this\nprocess by color-coding each tile, with unique threads assigned to each color,\nsuch that the same thread may operate on tiles spanning different boxes. This\napproach avoids unnecessary thread synchronization which would occur if threads\nwere distributed among tiles within each box.   The figures on this page are taken from the AMReX User's Guide.", 
            "title": "OpenMP"
        }, 
        {
            "location": "/case_studies/amr/code_structure/", 
            "text": "Code Structure\n\n\nThe typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:\n\n\n// Advance the solution one grid at a time\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \nbx\n \n=\n \nmfi\n.\nvalidbox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \nbx\n.\nloVect\n(),\n \nbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nHere the \nMFIter\n object is an iterator over boxes owned by an MPI process. The\n\nBox\n object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables \nold_phi\n, \nnew_phi\n,\nand \nflux\n contain pointers to the arrays which contain the floating point data\non the grid. The \nupdate_phi\n function is a Fortran function which uses the\ndata from the \nBox\n object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:\n\n\nsubroutine \nupdate_phi\n(\nphiold\n,\n \nphinew\n,\n \nng_p\n,\n \nfluxx\n,\n \nfluxy\n,\n \nfluxz\n,\n \nng_f\n,\n \nlo\n,\n \nhi\n,\n \ndx\n,\n \ndt\n)\n \nbind\n(\nC\n,\n \nname\n=\nupdate_phi\n)\n\n\n  \ninteger\n          \n::\n \nlo\n(\n3\n),\n \nhi\n(\n3\n),\n \nng_p\n,\n \nng_f\n\n  \ndouble precision\n \n::\n \nphiold\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n \nphinew\n(\nlo\n(\n1\n)\n-\nng_p\n:\nhi\n(\n1\n)\n+\nng_p\n,\nlo\n(\n2\n)\n-\nng_p\n:\nhi\n(\n2\n)\n+\nng_p\n,\nlo\n(\n3\n)\n-\nng_p\n:\nhi\n(\n3\n)\n+\nng_p\n)\n\n  \ndouble precision\n \n::\n  \nfluxx\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n+\n1\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxy\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n+\n1\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n)\n\n  \ndouble precision\n \n::\n  \nfluxz\n(\nlo\n(\n1\n)\n-\nng_f\n:\nhi\n(\n1\n)\n+\nng_f\n,\nlo\n(\n2\n)\n-\nng_f\n:\nhi\n(\n2\n)\n+\nng_f\n,\nlo\n(\n3\n)\n-\nng_f\n:\nhi\n(\n3\n)\n+\nng_f\n+\n1\n)\n\n  \ndouble precision\n \n::\n \ndx\n,\n \ndt\n\n\n  \ninteger \ni\n,\nj\n,\nk\n\n\n  \ndo \nk\n=\nlo\n(\n3\n),\nhi\n(\n3\n)\n\n     \ndo \nj\n=\nlo\n(\n2\n),\nhi\n(\n2\n)\n\n        \ndo \ni\n=\nlo\n(\n1\n),\nhi\n(\n1\n)\n\n\n           \nphinew\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphiold\n(\ni\n,\nj\n,\nk\n)\n \n+\n \ndt\n \n*\n \n\n                \n(\n \nfluxx\n(\ni\n+\n1\n,\nj\n,\nk\n)\n-\nfluxx\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxy\n(\ni\n,\nj\n+\n1\n,\nk\n)\n-\nfluxy\n(\ni\n,\nj\n,\nk\n)\n \n\n                \n+\nfluxz\n(\ni\n,\nj\n,\nk\n+\n1\n)\n-\nfluxz\n(\ni\n,\nj\n,\nk\n)\n \n)\n \n/\n \ndx\n\n\n        \nend do\n\n\n     end do\n\n\n  end do\n\n\n\n\n\nThe Fortran function constructs the appropriate \"view\" into each box using the\ndata from the \nBox\n object from the C++ function, as well as from the number of\nghost zones (\nng_p\n for \nold_phi\n and \nnew_phi\n, and \nng_f\n for \nflux\n).\n\n\nThe above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:\n\n\n// Advance the solution one grid at a time\n\n\n#ifdef _OPENMP\n\n\n#pragma omp parallel\n\n\n#endif\n\n\nfor\n \n(\n \nMFIter\n \nmfi\n(\nold_phi\n,\ntrue\n);\n \nmfi\n.\nisValid\n();\n \n++\nmfi\n \n)\n\n\n{\n\n  \nconst\n \nBox\n \ntbx\n \n=\n \nmfi\n.\ntilebox\n();\n\n\n  \nupdate_phi\n(\nold_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nnew_phi\n[\nmfi\n].\ndataPtr\n(),\n\n             \nng_p\n,\n\n             \nflux\n[\n0\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n1\n][\nmfi\n].\ndataPtr\n(),\n\n             \nflux\n[\n2\n][\nmfi\n].\ndataPtr\n(),\n\n             \nng_f\n,\n \ntbx\n.\nloVect\n(),\n \ntbx\n.\nhiVect\n(),\n \ndx\n[\n0\n],\n \ndt\n);\n\n  \n}\n\n\n}\n\n\n\n\n\nThe OpenMP parallelism is coarse-grained; rather than constructing a large\n\nBox\n from \nmfi.validbox()\n, it constructs a smaller \nBox\n from\n\nmfi.tilebox()\n. The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.\n\n\nMemory Management\n\n\nBoxLib abstracts the memory management by using the abstract \nArena\n class.\n\n\n#ifndef BL_ARENA_H\n\n\n#define BL_ARENA_H\n\n\n\n#include\n \nwinstd.H\n\n\n#include\n \ncstddef\n\n\n\nclass\n \nArena\n;\n\n\n\nnamespace\n \nBoxLib\n\n\n{\n\n    \nArena\n*\n \nThe_Arena\n \n();\n\n\n}\n\n\n\n//\n\n\n// A Virtual Base Class for Dynamic Memory Management\n\n\n//\n\n\n// This is a virtual base class for objects that manage their own dynamic\n\n\n// memory allocation.  Since it is a virtual base class, you have to derive\n\n\n// something from it to use it.\n\n\n//\n\n\n\nclass\n \nArena\n\n\n{\n\n\npublic\n:\n\n\n    \nvirtual\n \n~\nArena\n \n();\n\n    \n//\n\n    \n// Allocate a dynamic memory arena of size sz.\n\n    \n// A pointer to this memory should be returned.\n\n    \n//\n\n    \nvirtual\n \nvoid\n*\n \nalloc\n \n(\nstd\n::\nsize_t\n \nsz\n)\n \n=\n \n0\n;\n\n    \n//\n\n    \n// A pure virtual function for deleting the arena pointed to by pt.\n\n    \n//\n\n    \nvirtual\n \nvoid\n \nfree\n \n(\nvoid\n*\n \npt\n)\n \n=\n \n0\n;\n\n    \n//\n\n    \n// Given a minimum required arena size of sz bytes, this returns\n\n    \n// the next largest arena size that will align to align_size bytes.\n\n    \n//\n\n    \nstatic\n \nstd\n::\nsize_t\n \nalign\n \n(\nstd\n::\nsize_t\n \nsz\n);\n\n\n\nprotected\n:\n\n\n    \nstatic\n \nconst\n \nunsigned\n \nint\n \nalign_size\n \n=\n \n16\n;\n\n\n};\n\n\n\n#endif \n/*BL_ARENA_H*/\n\n\n\nThe most general container class in BoxLib, the \nBaseFab\n, calls the \nArena\n data allocator to allocate its memory. Therefore, by providing a specialized Arena-descendant, the user can easily plug in his own data containers or decorate his allocations with alignment or memory placing directives.\n\n\nC++ Kernel Rewrites\n\n\nSome programming models do not support Fortran and thus for using those, we need to port our kernels to C++. Below we show the ported GSRB kernel. For the sake of simplicity, we work directly with the fabs and not with the data pointers as we do in Fortran, so that we can use the access operator to index into our data containers. \n\n\nvoid\n \nC_GSRB_3D\n(\n\n\nconst\n \nBox\n \nbx\n,\n\n\nconst\n \nBox\n \nbbx\n,\n\n\nconst\n \nint\n \nnc\n,\n\n\nconst\n \nint\n \nrb\n,\n\n\nconst\n \nReal\n \nalpha\n,\n\n\nconst\n \nReal\n \nbeta\n,\n\n\nFArrayBox\n \nphi\n,\n\n\nconst\n \nFArrayBox\n \nrhs\n,\n\n\nconst\n \nFArrayBox\n \na\n,\n\n\nconst\n \nFArrayBox\n \nbX\n,\n\n\nconst\n \nFArrayBox\n \nbY\n,\n\n\nconst\n \nFArrayBox\n \nbZ\n,\n\n\nconst\n \nFArrayBox\n \nf0\n,\n\n\nconst\n \nMask\n \nm0\n,\n\n\nconst\n \nFArrayBox\n \nf1\n,\n\n\nconst\n \nMask\n \nm1\n,\n\n\nconst\n \nFArrayBox\n \nf2\n,\n\n\nconst\n \nMask\n \nm2\n,\n\n\nconst\n \nFArrayBox\n \nf3\n,\n\n\nconst\n \nMask\n \nm3\n,\n\n\nconst\n \nFArrayBox\n \nf4\n,\n\n\nconst\n \nMask\n \nm4\n,\n\n\nconst\n \nFArrayBox\n \nf5\n,\n\n\nconst\n \nMask\n \nm5\n,\n\n\nconst\n \nReal\n*\n \nh\n)\n\n\n{\n\n    \n//box extends:\n\n    \nconst\n \nint\n \n*\nlo\n \n=\n \nbx\n.\nloVect\n();\n\n    \nconst\n \nint\n \n*\nhi\n \n=\n \nbx\n.\nhiVect\n();\n\n    \n//blo\n\n    \nconst\n \nint\n \n*\nblo\n \n=\n \nbbx\n.\nloVect\n();\n\n    \nconst\n \nint\n \n*\nbhi\n \n=\n \nbbx\n.\nhiVect\n();\n\n\n    \n//some parameters\n\n    \nReal\n \nomega\n=\n \n1.15\n;\n\n    \nReal\n \ndhx\n \n=\n \nbeta\n/\n(\nh\n[\n0\n]\n*\nh\n[\n0\n]);\n\n    \nReal\n \ndhy\n \n=\n \nbeta\n/\n(\nh\n[\n1\n]\n*\nh\n[\n1\n]);\n\n    \nReal\n \ndhz\n \n=\n \nbeta\n/\n(\nh\n[\n2\n]\n*\nh\n[\n2\n]);\n\n\n    \nfor\n \n(\nint\n \nn\n \n=\n \n0\n;\n \nn\nnc\n;\n \nn\n++\n){\n\n        \nfor\n \n(\nint\n \nk\n \n=\n \nlo\n[\n2\n];\n \nk\n \n=\n \nhi\n[\n2\n];\n \n++\nk\n)\n \n{\n\n            \nfor\n \n(\nint\n \nj\n \n=\n \nlo\n[\n1\n];\n \nj\n \n=\n \nhi\n[\n1\n];\n \n++\nj\n)\n \n{\n\n                \nint\n \nioff\n \n=\n \n(\nlo\n[\n0\n]\n \n+\n \nj\n \n+\n \nk\n \n+\n \nrb\n)\n%\n2\n;\n\n                \nfor\n \n(\nint\n \ni\n \n=\n \nlo\n[\n0\n]\n \n+\n \nioff\n;\n \ni\n \n=\n \nhi\n[\n0\n];\n \ni\n+=\n2\n)\n \n{\n\n\n                    \n//BC terms\n\n                    \nReal\n \ncf0\n \n=\n \n(\n \n(\ni\n==\nblo\n[\n0\n])\n \n \n(\nm0\n(\nIntVect\n(\nblo\n[\n0\n]\n-\n1\n,\nj\n,\nk\n))\n0\n)\n \n?\n \nf0\n(\nIntVect\n(\nblo\n[\n0\n],\nj\n,\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf1\n \n=\n \n(\n \n(\nj\n==\nblo\n[\n1\n])\n \n \n(\nm1\n(\nIntVect\n(\ni\n,\nblo\n[\n1\n]\n-\n1\n,\nk\n))\n0\n)\n \n?\n \nf1\n(\nIntVect\n(\ni\n,\nblo\n[\n1\n],\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf2\n \n=\n \n(\n \n(\nk\n==\nblo\n[\n2\n])\n \n \n(\nm2\n(\nIntVect\n(\ni\n,\nj\n,\nblo\n[\n2\n]\n-\n1\n))\n0\n)\n \n?\n \nf2\n(\nIntVect\n(\ni\n,\nj\n,\nblo\n[\n2\n]))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf3\n \n=\n \n(\n \n(\ni\n==\nbhi\n[\n0\n])\n \n \n(\nm3\n(\nIntVect\n(\nbhi\n[\n0\n]\n+\n1\n,\nj\n,\nk\n))\n0\n)\n \n?\n \nf3\n(\nIntVect\n(\nbhi\n[\n0\n],\nj\n,\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf4\n \n=\n \n(\n \n(\nj\n==\nbhi\n[\n1\n])\n \n \n(\nm4\n(\nIntVect\n(\ni\n,\nbhi\n[\n1\n]\n+\n1\n,\nk\n))\n0\n)\n \n?\n \nf4\n(\nIntVect\n(\ni\n,\nbhi\n[\n1\n],\nk\n))\n \n:\n \n0.\n \n);\n\n                    \nReal\n \ncf5\n \n=\n \n(\n \n(\nk\n==\nbhi\n[\n2\n])\n \n \n(\nm5\n(\nIntVect\n(\ni\n,\nj\n,\nbhi\n[\n2\n]\n+\n1\n))\n0\n)\n \n?\n \nf5\n(\nIntVect\n(\ni\n,\nj\n,\nbhi\n[\n2\n]))\n \n:\n \n0.\n \n);\n\n\n                    \n//assign ORA constants\n\n                    \ndouble\n \ngamma\n \n=\n \nalpha\n \n*\n \na\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n\n                                    \n+\n \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n)))\n\n                                    \n+\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n)))\n\n                                    \n+\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n)));\n\n\n                    \ndouble\n \ng_m_d\n \n=\n \ngamma\n\n                                    \n-\n \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf0\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n))\n*\ncf3\n)\n\n                                    \n-\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf1\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n))\n*\ncf4\n)\n\n                                    \n-\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\ncf2\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n))\n*\ncf5\n);\n\n\n                    \ndouble\n \nrho\n \n=\n  \ndhx\n \n*\n \n(\nbX\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n-\n1\n,\nj\n,\nk\n),\nn\n)\n \n+\n \nbX\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n+\n1\n,\nj\n,\nk\n),\nn\n))\n\n                                \n+\n \ndhy\n \n*\n \n(\nbY\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n-\n1\n,\nk\n),\nn\n)\n \n+\n \nbY\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n+\n1\n,\nk\n),\nn\n))\n\n                                \n+\n \ndhz\n \n*\n \n(\nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n-\n1\n),\nn\n)\n \n+\n \nbZ\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n))\n*\nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n+\n1\n),\nn\n));\n\n\n                    \ndouble\n \nres\n \n=\n \nrhs\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n-\n \ngamma\n \n*\n \nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n+\n \nrho\n;\n\n                    \nphi\n(\nIntVect\n(\ni\n,\nj\n,\nk\n),\nn\n)\n \n+=\n \nomega\n/\ng_m_d\n \n*\n \nres\n;\n\n                \n}\n\n            \n}\n\n        \n}\n\n    \n}\n\n\n}\n\n\n\n\n\nWe try to avoid porting all Fortran kernels for our explorations but some of the frameworks would basically require that. We will make comments about this in appropriate places.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#code-structure", 
            "text": "The typical form of a BoxLib application is a C++ \"driver\" code which manages\nthe boxes on the domain, and calls Fortran kernels in a loop over the boxes\nowned by each MPI process. An example of this layout is shown below:  // Advance the solution one grid at a time  for   (   MFIter   mfi ( old_phi );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   bx   =   mfi . validbox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   bx . loVect (),   bx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   Here the  MFIter  object is an iterator over boxes owned by an MPI process. The Box  object contains the geometric metadata describing a particular box, e.g.,\nthe indices of the lower and upper corners. The variables  old_phi ,  new_phi ,\nand  flux  contain pointers to the arrays which contain the floating point data\non the grid. The  update_phi  function is a Fortran function which uses the\ndata from the  Box  object to construct 3-D loops over the appropriate section\nof the three floating-point arrays. The function may look like the following:  subroutine  update_phi ( phiold ,   phinew ,   ng_p ,   fluxx ,   fluxy ,   fluxz ,   ng_f ,   lo ,   hi ,   dx ,   dt )   bind ( C ,   name = update_phi ) \n\n   integer            ::   lo ( 3 ),   hi ( 3 ),   ng_p ,   ng_f \n   double precision   ::   phiold ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::   phinew ( lo ( 1 ) - ng_p : hi ( 1 ) + ng_p , lo ( 2 ) - ng_p : hi ( 2 ) + ng_p , lo ( 3 ) - ng_p : hi ( 3 ) + ng_p ) \n   double precision   ::    fluxx ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f + 1 , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxy ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f + 1 , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f ) \n   double precision   ::    fluxz ( lo ( 1 ) - ng_f : hi ( 1 ) + ng_f , lo ( 2 ) - ng_f : hi ( 2 ) + ng_f , lo ( 3 ) - ng_f : hi ( 3 ) + ng_f + 1 ) \n   double precision   ::   dx ,   dt \n\n   integer  i , j , k \n\n   do  k = lo ( 3 ), hi ( 3 ) \n      do  j = lo ( 2 ), hi ( 2 ) \n         do  i = lo ( 1 ), hi ( 1 ) \n\n            phinew ( i , j , k )   =   phiold ( i , j , k )   +   dt   *   \n                 (   fluxx ( i + 1 , j , k ) - fluxx ( i , j , k )   \n                 + fluxy ( i , j + 1 , k ) - fluxy ( i , j , k )   \n                 + fluxz ( i , j , k + 1 ) - fluxz ( i , j , k )   )   /   dx \n\n         end do       end do    end do   The Fortran function constructs the appropriate \"view\" into each box using the\ndata from the  Box  object from the C++ function, as well as from the number of\nghost zones ( ng_p  for  old_phi  and  new_phi , and  ng_f  for  flux ).  The above example demonstrates pure MPI parallelism; the analogous C++ code\nwhich uses OpenMP tiling as described above would look like the following:  // Advance the solution one grid at a time  #ifdef _OPENMP  #pragma omp parallel  #endif  for   (   MFIter   mfi ( old_phi , true );   mfi . isValid ();   ++ mfi   )  { \n   const   Box   tbx   =   mfi . tilebox (); \n\n   update_phi ( old_phi [ mfi ]. dataPtr (), \n              new_phi [ mfi ]. dataPtr (), \n              ng_p , \n              flux [ 0 ][ mfi ]. dataPtr (), \n              flux [ 1 ][ mfi ]. dataPtr (), \n              flux [ 2 ][ mfi ]. dataPtr (), \n              ng_f ,   tbx . loVect (),   tbx . hiVect (),   dx [ 0 ],   dt ); \n   }  }   The OpenMP parallelism is coarse-grained; rather than constructing a large Box  from  mfi.validbox() , it constructs a smaller  Box  from mfi.tilebox() . The metadata format remains unchanged, allowing the Fortran\nfunction to remain unchanged as well.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#memory-management", 
            "text": "BoxLib abstracts the memory management by using the abstract  Arena  class.  #ifndef BL_ARENA_H  #define BL_ARENA_H  #include   winstd.H  #include   cstddef  class   Arena ;  namespace   BoxLib  { \n     Arena *   The_Arena   ();  }  //  // A Virtual Base Class for Dynamic Memory Management  //  // This is a virtual base class for objects that manage their own dynamic  // memory allocation.  Since it is a virtual base class, you have to derive  // something from it to use it.  //  class   Arena  {  public : \n\n     virtual   ~ Arena   (); \n     // \n     // Allocate a dynamic memory arena of size sz. \n     // A pointer to this memory should be returned. \n     // \n     virtual   void *   alloc   ( std :: size_t   sz )   =   0 ; \n     // \n     // A pure virtual function for deleting the arena pointed to by pt. \n     // \n     virtual   void   free   ( void *   pt )   =   0 ; \n     // \n     // Given a minimum required arena size of sz bytes, this returns \n     // the next largest arena size that will align to align_size bytes. \n     // \n     static   std :: size_t   align   ( std :: size_t   sz );  protected : \n\n     static   const   unsigned   int   align_size   =   16 ;  };  #endif  /*BL_ARENA_H*/  \nThe most general container class in BoxLib, the  BaseFab , calls the  Arena  data allocator to allocate its memory. Therefore, by providing a specialized Arena-descendant, the user can easily plug in his own data containers or decorate his allocations with alignment or memory placing directives.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/case_studies/amr/code_structure/#c-kernel-rewrites", 
            "text": "Some programming models do not support Fortran and thus for using those, we need to port our kernels to C++. Below we show the ported GSRB kernel. For the sake of simplicity, we work directly with the fabs and not with the data pointers as we do in Fortran, so that we can use the access operator to index into our data containers.   void   C_GSRB_3D (  const   Box   bx ,  const   Box   bbx ,  const   int   nc ,  const   int   rb ,  const   Real   alpha ,  const   Real   beta ,  FArrayBox   phi ,  const   FArrayBox   rhs ,  const   FArrayBox   a ,  const   FArrayBox   bX ,  const   FArrayBox   bY ,  const   FArrayBox   bZ ,  const   FArrayBox   f0 ,  const   Mask   m0 ,  const   FArrayBox   f1 ,  const   Mask   m1 ,  const   FArrayBox   f2 ,  const   Mask   m2 ,  const   FArrayBox   f3 ,  const   Mask   m3 ,  const   FArrayBox   f4 ,  const   Mask   m4 ,  const   FArrayBox   f5 ,  const   Mask   m5 ,  const   Real *   h )  { \n     //box extends: \n     const   int   * lo   =   bx . loVect (); \n     const   int   * hi   =   bx . hiVect (); \n     //blo \n     const   int   * blo   =   bbx . loVect (); \n     const   int   * bhi   =   bbx . hiVect (); \n\n     //some parameters \n     Real   omega =   1.15 ; \n     Real   dhx   =   beta / ( h [ 0 ] * h [ 0 ]); \n     Real   dhy   =   beta / ( h [ 1 ] * h [ 1 ]); \n     Real   dhz   =   beta / ( h [ 2 ] * h [ 2 ]); \n\n     for   ( int   n   =   0 ;   n nc ;   n ++ ){ \n         for   ( int   k   =   lo [ 2 ];   k   =   hi [ 2 ];   ++ k )   { \n             for   ( int   j   =   lo [ 1 ];   j   =   hi [ 1 ];   ++ j )   { \n                 int   ioff   =   ( lo [ 0 ]   +   j   +   k   +   rb ) % 2 ; \n                 for   ( int   i   =   lo [ 0 ]   +   ioff ;   i   =   hi [ 0 ];   i += 2 )   { \n\n                     //BC terms \n                     Real   cf0   =   (   ( i == blo [ 0 ])     ( m0 ( IntVect ( blo [ 0 ] - 1 , j , k )) 0 )   ?   f0 ( IntVect ( blo [ 0 ], j , k ))   :   0.   ); \n                     Real   cf1   =   (   ( j == blo [ 1 ])     ( m1 ( IntVect ( i , blo [ 1 ] - 1 , k )) 0 )   ?   f1 ( IntVect ( i , blo [ 1 ], k ))   :   0.   ); \n                     Real   cf2   =   (   ( k == blo [ 2 ])     ( m2 ( IntVect ( i , j , blo [ 2 ] - 1 )) 0 )   ?   f2 ( IntVect ( i , j , blo [ 2 ]))   :   0.   ); \n                     Real   cf3   =   (   ( i == bhi [ 0 ])     ( m3 ( IntVect ( bhi [ 0 ] + 1 , j , k )) 0 )   ?   f3 ( IntVect ( bhi [ 0 ], j , k ))   :   0.   ); \n                     Real   cf4   =   (   ( j == bhi [ 1 ])     ( m4 ( IntVect ( i , bhi [ 1 ] + 1 , k )) 0 )   ?   f4 ( IntVect ( i , bhi [ 1 ], k ))   :   0.   ); \n                     Real   cf5   =   (   ( k == bhi [ 2 ])     ( m5 ( IntVect ( i , j , bhi [ 2 ] + 1 )) 0 )   ?   f5 ( IntVect ( i , j , bhi [ 2 ]))   :   0.   ); \n\n                     //assign ORA constants \n                     double   gamma   =   alpha   *   a ( IntVect ( i , j , k )) \n                                     +   dhx   *   ( bX ( IntVect ( i , j , k ))   +   bX ( IntVect ( i + 1 , j , k ))) \n                                     +   dhy   *   ( bY ( IntVect ( i , j , k ))   +   bY ( IntVect ( i , j + 1 , k ))) \n                                     +   dhz   *   ( bZ ( IntVect ( i , j , k ))   +   bZ ( IntVect ( i , j , k + 1 ))); \n\n                     double   g_m_d   =   gamma \n                                     -   dhx   *   ( bX ( IntVect ( i , j , k )) * cf0   +   bX ( IntVect ( i + 1 , j , k )) * cf3 ) \n                                     -   dhy   *   ( bY ( IntVect ( i , j , k )) * cf1   +   bY ( IntVect ( i , j + 1 , k )) * cf4 ) \n                                     -   dhz   *   ( bZ ( IntVect ( i , j , k )) * cf2   +   bZ ( IntVect ( i , j , k + 1 )) * cf5 ); \n\n                     double   rho   =    dhx   *   ( bX ( IntVect ( i , j , k )) * phi ( IntVect ( i - 1 , j , k ), n )   +   bX ( IntVect ( i + 1 , j , k )) * phi ( IntVect ( i + 1 , j , k ), n )) \n                                 +   dhy   *   ( bY ( IntVect ( i , j , k )) * phi ( IntVect ( i , j - 1 , k ), n )   +   bY ( IntVect ( i , j + 1 , k )) * phi ( IntVect ( i , j + 1 , k ), n )) \n                                 +   dhz   *   ( bZ ( IntVect ( i , j , k )) * phi ( IntVect ( i , j , k - 1 ), n )   +   bZ ( IntVect ( i , j , k + 1 )) * phi ( IntVect ( i , j , k + 1 ), n )); \n\n                     double   res   =   rhs ( IntVect ( i , j , k ), n )   -   gamma   *   phi ( IntVect ( i , j , k ), n )   +   rho ; \n                     phi ( IntVect ( i , j , k ), n )   +=   omega / g_m_d   *   res ; \n                 } \n             } \n         } \n     }  }   We try to avoid porting all Fortran kernels for our explorations but some of the frameworks would basically require that. We will make comments about this in appropriate places.", 
            "title": "C++ Kernel Rewrites"
        }, 
        {
            "location": "/case_studies/amr/multigrid/", 
            "text": "Geometric Multigrid\n\n\nMany problems encountered in BoxLib applications require solutions to linear\nsystem, e.g., \nelliptic partial differential equations\n such as the \nPoisson\nequation\n for self-gravity, and the \ndiffusion equation\n. BoxLib therefore\nincludes \ngeometric multigrid solvers\n for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.\n\n\nGeometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:\n\n\n\n\nrelaxation\n\n\nrestriction\n\n\nprolongation\n\n\ncoarse-grid linear solve (either approximate or exact)\n\n\n\n\nAlthough here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms. Although these steps are algorithmically unique, we note that all\nof them feature low arithmetic intensity and are thus sensitive to cache and\nmemory bandwidth.\n\n\nRelaxation\n\n\nA relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original \nGauss-Seidel method\n by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n     \nioff\n \n=\n \nMOD\n(\nlo\n(\n1\n)\n \n+\n \nj\n \n+\n \nk\n \n+\n \nredblack\n,\n2\n)\n\n     \ndo \ni\n \n=\n \nlo\n(\n1\n)\n \n+\n \nioff\n,\nhi\n(\n1\n),\n2\n\n        \ngamma\n \n=\n \nalpha\n*\na\n(\ni\n,\nj\n,\nk\n)\n \n\n              \n+\n   \ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n+\nbX\n(\ni\n+\n1\n,\nj\n,\nk\n))\n \n\n              \n+\n   \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n+\nbY\n(\ni\n,\nj\n+\n1\n,\nk\n))\n \n\n              \n+\n   \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n+\nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n))\n\n\n        \ng_m_d\n \n=\n \ngamma\n \n\n              \n-\n \n(\ndhx\n*\n(\nbX\n(\ni\n,\nj\n,\nk\n)\n*\ncf0\n \n+\n \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\ncf3\n)\n \n\n              \n+\n  \ndhy\n*\n(\nbY\n(\ni\n,\nj\n,\nk\n)\n*\ncf1\n \n+\n \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\ncf4\n)\n \n\n              \n+\n  \ndhz\n*\n(\nbZ\n(\ni\n,\nj\n,\nk\n)\n*\ncf2\n \n+\n \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\ncf5\n))\n \n\n\n        \nrho\n \n=\n \ndhx\n*\n(\n \nbX\n(\ni\n  \n,\nj\n,\nk\n)\n*\nphi\n(\ni\n-\n1\n,\nj\n,\nk\n)\n \n\n            \n+\n       \nbX\n(\ni\n+\n1\n,\nj\n,\nk\n)\n*\nphi\n(\ni\n+\n1\n,\nj\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhy\n*\n(\n \nbY\n(\ni\n,\nj\n  \n,\nk\n)\n*\nphi\n(\ni\n,\nj\n-\n1\n,\nk\n)\n \n\n            \n+\n       \nbY\n(\ni\n,\nj\n+\n1\n,\nk\n)\n*\nphi\n(\ni\n,\nj\n+\n1\n,\nk\n)\n \n)\n \n\n            \n+\n \ndhz\n*\n(\n \nbZ\n(\ni\n,\nj\n,\nk\n  \n)\n*\nphi\n(\ni\n,\nj\n,\nk\n-\n1\n)\n \n\n            \n+\n       \nbZ\n(\ni\n,\nj\n,\nk\n+\n1\n)\n*\nphi\n(\ni\n,\nj\n,\nk\n+\n1\n)\n \n)\n \n\n\n        \nres\n \n=\n  \nrhs\n(\ni\n,\nj\n,\nk\n)\n \n-\n \n(\ngamma\n*\nphi\n(\ni\n,\nj\n,\nk\n)\n \n-\n \nrho\n)\n\n        \nphi\n(\ni\n,\nj\n,\nk\n)\n \n=\n \nphi\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nomega\n/\ng_m_d\n \n*\n \nres\n\n     \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nThe algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.\n\n\nThe GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.\n\n\nThe relaxation step and the coarse grid solve (discussed belowed) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.\n\n\nRestriction\n\n\nDuring a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:\n\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n      \nc\n(\ni\n,\nj\n,\nk\n)\n \n=\n  \n(\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2\n  \n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2p1\n,\nk2p1\n)\n\n\n$\n                 \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n+\n \nf\n(\ni2\n,\nj2\n  \n,\nk2p1\n)\n\n\n$\n                 \n)\n*\neighth\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\n\nwhere \nf\n is the field on the fine grid and \nc\n is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point\n\nc(i,j,k)\n also contribute to the point \nc(i+1,j,k)\n.\n\n\nProlongation\n\n\nProlongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows:\n\ndo \nk\n \n=\n \nlo\n(\n3\n),\n \nhi\n(\n3\n)\n\n  \nk2\n \n=\n \n2\n*\nk\n\n  \nk2p1\n \n=\n \nk2\n \n+\n \n1\n\n  \ndo \nj\n \n=\n \nlo\n(\n2\n),\n \nhi\n(\n2\n)\n\n    \nj2\n \n=\n \n2\n*\nj\n\n    \nj2p1\n \n=\n \nj2\n \n+\n \n1\n\n    \ndo \ni\n \n=\n \nlo\n(\n1\n),\n \nhi\n(\n1\n)\n\n      \ni2\n \n=\n \n2\n*\ni\n\n      \ni2p1\n \n=\n \ni2\n \n+\n \n1\n\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2\n  \n)\n\n      \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2p1\n,\nk2p1\n)\n\n      \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2p1\n,\nj2\n  \n,\nk2p1\n)\n\n      \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n \n=\n \nc\n(\ni\n,\nj\n,\nk\n)\n \n+\n \nf\n(\ni2\n  \n,\nj2\n  \n,\nk2p1\n)\n\n\n    \nend do\n\n\n  end do\n\n\nend do\n\n\n\n\nIn 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)\n\n\nExact linear solve\n\n\nThe multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often \n\\(2^3\\)\n if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is \nBiCGSTAB\n, a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#geometric-multigrid", 
            "text": "Many problems encountered in BoxLib applications require solutions to linear\nsystem, e.g.,  elliptic partial differential equations  such as the  Poisson\nequation  for self-gravity, and the  diffusion equation . BoxLib therefore\nincludes  geometric multigrid solvers  for solving problems which use both\ncell-centered and nodal data. For this project, we have focused on the\ncell-centered solver due to its relative simplicity compared to the nodal\nsolver.  Geometric multigrid is an iterative method for solving linear problems which\ncontains roughly 4 steps:   relaxation  restriction  prolongation  coarse-grid linear solve (either approximate or exact)   Although here we will not discuss the details of the geometric multigrid\nmethod, we summarize each of these steps below as they pertain to computational\nalgorithms. Although these steps are algorithmically unique, we note that all\nof them feature low arithmetic intensity and are thus sensitive to cache and\nmemory bandwidth.", 
            "title": "Geometric Multigrid"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#relaxation", 
            "text": "A relaxation consists of one or more iterations of an approximate solution to\nthe system of linear equations. In geometric multigrid, common algorithms used\nhere include Jacobi and Gauss-Seidel. By default, the BoxLib solver uses a\nvariation on Gauss-Seidel called Gauss-Seidel red-black (\"GSRB\"). GSRB deviates\nfrom the original  Gauss-Seidel method  by exploiting a symmetry in the data\ndependence among matrix elements, such that an update sweep of all matrix\nelements follows a stride-2 pattern rather than stride-1. (This property\nmanifests in the innermost loop of the kernel shown below).  do  k   =   lo ( 3 ),   hi ( 3 ) \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n      ioff   =   MOD ( lo ( 1 )   +   j   +   k   +   redblack , 2 ) \n      do  i   =   lo ( 1 )   +   ioff , hi ( 1 ), 2 \n         gamma   =   alpha * a ( i , j , k )   \n               +     dhx * ( bX ( i , j , k ) + bX ( i + 1 , j , k ))   \n               +     dhy * ( bY ( i , j , k ) + bY ( i , j + 1 , k ))   \n               +     dhz * ( bZ ( i , j , k ) + bZ ( i , j , k + 1 )) \n\n         g_m_d   =   gamma   \n               -   ( dhx * ( bX ( i , j , k ) * cf0   +   bX ( i + 1 , j , k ) * cf3 )   \n               +    dhy * ( bY ( i , j , k ) * cf1   +   bY ( i , j + 1 , k ) * cf4 )   \n               +    dhz * ( bZ ( i , j , k ) * cf2   +   bZ ( i , j , k + 1 ) * cf5 ))   \n\n         rho   =   dhx * (   bX ( i    , j , k ) * phi ( i - 1 , j , k )   \n             +         bX ( i + 1 , j , k ) * phi ( i + 1 , j , k )   )   \n             +   dhy * (   bY ( i , j    , k ) * phi ( i , j - 1 , k )   \n             +         bY ( i , j + 1 , k ) * phi ( i , j + 1 , k )   )   \n             +   dhz * (   bZ ( i , j , k    ) * phi ( i , j , k - 1 )   \n             +         bZ ( i , j , k + 1 ) * phi ( i , j , k + 1 )   )   \n\n         res   =    rhs ( i , j , k )   -   ( gamma * phi ( i , j , k )   -   rho ) \n         phi ( i , j , k )   =   phi ( i , j , k )   +   omega / g_m_d   *   res \n      end do    end do  end do   The algorithm above uses a 7-point cell-centered discretization of the 3-D\nvariable-coefficient Helmholtz operator. The diffusion operator is one type of\nHelmholtz operator; the Laplace operator, which appears in the Poisson equation\nfor self-gravity, is a simplified version, with constant coefficients.  The GSRB method for a 7-point discretization of the Helmholtz operator exhibits\na low arithmetic intensity, requiring several non-contiguous loads from memory\nto evaluate the operator.  The relaxation step and the coarse grid solve (discussed belowed) often feature\nsimilar computational and data access patterns, because both are effectively\ndoing the same thing - solving a linear system. The primary difference between\nthem is that the relaxation method applies the iterative kernel only a handful\nof times, whereas the coarse grid solve often iterates all the way to\nconvergence.", 
            "title": "Relaxation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#restriction", 
            "text": "During a restriction, the value of a field on a fine grid is approximated on a\ncoarser grid. This is typically done by averaging values of the field on fine\ngrid points onto the corresponding grid points on the coarse grid. In BoxLib,\nthe algorithm is the following:  do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n       c ( i , j , k )   =    (  $                   +   f ( i2p1 , j2p1 , k2    )   +   f ( i2 , j2p1 , k2    )  $                   +   f ( i2p1 , j2    , k2    )   +   f ( i2 , j2    , k2    )  $                   +   f ( i2p1 , j2p1 , k2p1 )   +   f ( i2 , j2p1 , k2p1 )  $                   +   f ( i2p1 , j2    , k2p1 )   +   f ( i2 , j2    , k2p1 )  $                   ) * eighth \n     end do    end do  end do   where  f  is the field on the fine grid and  c  is the field on the coarse\ngrid. (This multigrid solver always coarsens grids by factors of two in each\ndimension.) For each evaluation of a coarse grid point, the algorithm must load\n8 values from the fine grid. However, there is significant memory locality in\nthis algorithm, as many of the fine grid points for coarse grid point c(i,j,k)  also contribute to the point  c(i+1,j,k) .", 
            "title": "Restriction"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#prolongation", 
            "text": "Prolongation (also called interpolation) is the opposite of restriction: one\napproximates the value of a field on a coarse grid on a finer grid. The\nprolongation kernel in the BoxLib solver is as follows: do  k   =   lo ( 3 ),   hi ( 3 ) \n   k2   =   2 * k \n   k2p1   =   k2   +   1 \n   do  j   =   lo ( 2 ),   hi ( 2 ) \n     j2   =   2 * j \n     j2p1   =   j2   +   1 \n     do  i   =   lo ( 1 ),   hi ( 1 ) \n       i2   =   2 * i \n       i2p1   =   i2   +   1 \n\n       f ( i2p1 , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2    ) \n       f ( i2    , j2p1 , k2    )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2    ) \n       f ( i2p1 , j2    , k2    )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2    ) \n       f ( i2    , j2    , k2    )   =   c ( i , j , k )   +   f ( i2    , j2    , k2    ) \n       f ( i2p1 , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2p1 , k2p1 ) \n       f ( i2    , j2p1 , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2p1 , k2p1 ) \n       f ( i2p1 , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2p1 , j2    , k2p1 ) \n       f ( i2    , j2    , k2p1 )   =   c ( i , j , k )   +   f ( i2    , j2    , k2p1 ) \n\n     end do    end do  end do   In 3-D, the same value on the coarse grid contributes equally to eight\nneighboring points in the fine grid. (The symmetry arises from the constraint\nin the solver that the cells must be cubic.)", 
            "title": "Prolongation"
        }, 
        {
            "location": "/case_studies/amr/multigrid/#exact-linear-solve", 
            "text": "The multigrid solver in BoxLib recursively coarsens grids until the grid\nreaches a sufficiently small size, often  \\(2^3\\)  if the problem domain is cubic.\nOn the coarsest grid, the solver then solves the linear system exactly, before\npropagating the solution back up to finer grids. The solution algorithm chosen\nfor this step is rarely influential on the overall performance of the multigrid\nalgorithm, because the problem size at the coarsest grid is so small. In\nBoxLib, the default coarse grid solver algorithm is  BiCGSTAB , a variation on\nthe conjugate-gradient iterative method.", 
            "title": "Exact linear solve"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/", 
            "text": "Kokkos Implementation\n\n\nThis section is written as some kind of lab report, since we think that it illustrates best what challenges users might face when making complicated framworks such as BoxLib performance portable.\n\n\nFirst Attempt\n\n\nThis section will describe our first, unfinished attempt, to port BoxLib over to Kokkos. The approach was designed to be the cleanest but also the most difficult one. We learned some lessons in the process which we want to share with the reader of this case study.\n\n\nMemory Management\n\n\nIn theory, one would like to make the data resident on the \ndevice\n all the time to avoid unneccessary data transfer. We thus implemented a \nKArenaND\n class which allows us to use Kokkos views instead of plain arrays for storing the data. This is the abstract, N-dimensional class which provides a part of the interface:\n\n\n//generic ND template\n\n\ntemplate\n \ntypename\n \nT\n,\n \nint\n \nD\n\n\nclass\n \nKArenaND\n \n{\n\n\npublic\n:\n\n    \n//\n\n    \n// Allocates a dynamic memory arena of size sz.\n\n    \n// Returns a pointer to this memory.\n\n    \n//\n\n    \nvirtual\n \nvoid\n*\n \nalloc\n \n(\nconst\n \nstd\n::\nvector\nstd\n::\nsize_t\n \n_sz\n)\n \n=\n \n0\n;\n\n\nprotected\n:\n\n    \nvirtual\n \nvoid\n \nfree\n()\n \n=\n \n0\n;\n\n\n};\n\n\n\n\n\nWe then (partially) specialize this class in order generate a \nKArena3D\n-variant, as this is what we mostly use in our code. This class is defined as\n\n\ntemplate\n \ntypename\n \nT\n\n\nclass\n \nKArenaND\nT\n,\n3\n \n{\n\n\npublic\n:\n\n    \n//\n\n    \n// Allocates a dynamic memory arena of size sz.\n\n    \n// Returns a pointer to this memory.\n\n    \n//\n\n    \nvoid\n*\n \nalloc\n \n(\nconst\n \nstd\n::\nvector\nsize_t\n \nsz_vec\n);\n\n    \n//pass access operator through to simplify things\n\n    \nT\n \noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n);\n\n    \nconst\n \nT\n \noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n)\n \nconst\n;\n\n    \nT\n \noperator\n()(\nconst\n \nIntVect\n \na\n);\n\n    \nconst\n \nT\n \noperator\n()(\nconst\n \nIntVect\n \na\n)\n \nconst\n;\n\n    \n//and return the view\n\n    \nKokkos\n::\nView\nT\n***\n \nviewData\n(){\n \nreturn\n \nview\n;\n \n}\n\n\nprivate\n:\n\n    \nvoid\n \nfree\n();\n\n    \nKokkos\n::\nView\nT\n***\n \nview\n;\n\n\n};\n\n\n\n\n\nThe corresponding allocator looks like then:\n\n\ntemplate\ntypename\n \nT\n\n\nvoid\n*\n \nKArenaND\nT\n,\n3\n::\nalloc\n \n(\nconst\n \nstd\n::\nvector\nsize_t\n \n_sz_vec\n)\n\n\n{\n\n    \nif\n(\n_sz_vec\n.\nsize\n()\n!=\n3\n){\n\n        \nBoxLib\n::\nAbort\n(\nError, the vector size passed to KArenaND has to be equal to its dimension!\n);\n\n    \n}\n\n    \n//important: reverse dimensions for optimal access\n\n    \nview\n \n=\n \nKokkos\n::\nView\nT\n***\n(\nKArena_view3D\n,\n_sz_vec\n[\n2\n],\n_sz_vec\n[\n1\n],\n_sz_vec\n[\n0\n]);\n\n    \n//provide interface compatibility with the rest of boxlib, but never use that pointer.\n\n    \nreturn\n \nreinterpret_cast\nvoid\n*\n(\nview\n.\nptr_on_device\n());\n\n\n}\n\n\n\n\n\n\n\nWarning\n\n\nWhen packing Views into classes, avoid using pointers and the \nnew\n operator to instantiate a new View. \nThis pointer will be a host-only pointer and will have NULL value when accessed from a device which uses a different address space.\n The reference counting is only guaranteed to work properly if the View is stored and passed by value. In that sense, also avoid passing references to views. Note that Kokkos::Views are lighweight objects so there is no performance reason to use pointers/references instead of values in this case.\n\n\n\n\nWe further implemented operators to access the memory, which basically pass the View access operator to the outside. For example:\n\n\ntemplate\ntypename\n \nT\n\n\nT\n \nKArenaND\nT\n,\n3\n::\noperator\n()(\nint\n \na0\n,\n \nint\n \na1\n,\n \nint\n \na2\n)\n\n\n{\n\n    \n//indices reversed compared to BoxLib\n\n    \n//in roder to ensure interface compatibility\n\n    \nreturn\n \nview\n(\na2\n,\na1\n,\na0\n);\n\n\n}\n\n\n\n\n\nNote that we reverse the order of indices here. This is because BoxLib uses the indexing \n(x,y,z)\n whereas for Kokkos views it is more convenient if the order \n(z,y,x)\n is used so that one can use Kokkos default layouts, i.e. \nLayout::Left\n on GPU and \nLayout::Right\n on CPU. If one would use the BoxLib indexing on the view level, this logic would need to be inverted whenever a Kokkos parallel dispatch is used. Instead, we invert it on the access operator level so that we neither need to explicitly specify an iteration policy nor break the BoxLib indexing order in the rest of the code.\n\n\nThere are advantages and disadvantages to burying the Kokkos data containers deep into the framework. The obvious advantage is that once it works, basically the majority of the framwork will already be Kokkos compatible. The disadvantage is that incremental porting is not possible, it is an all-or-nothing approach.\n\n\nRewriting Fortran Kernels\n\n\nA big difficulty with porting the BoxLib GMG to Kokkos is that most of the kernels are written in Fortran. For Kokkos, we need those kernels in C++ so we have started rewriting those kernels accordingly. As it turns out, the Kokkos GMG tutorial touches many of these Fortran kernels, and so we stopped after altering almost 10K lines of code in about 50 files. Below you find the comparison between the BoxLib master branch from which we started to the current state of the Kokkos port branch.\n\n\n~/BoxLib\n git diff --stat cpp_kernels_kokkos-views \n\n Src/C_BaseLib/FArrayBox.H                            |    2 -\n Src/C_BaseLib/FabArray.H                             |    8 +-\n Src/C_BaseLib/IArrayBox.H                            |    1 -\n Src/C_BaseLib/KArena.H                               |  265 ---------\n Src/C_BaseLib/KBaseFab.H                             | 3615 -----------------------------------------------------------------------------------------------------------------\n Src/C_BaseLib/Looping.H                              |  770 +-----------------------\n Src/C_BaseLib/Make.package                           |    6 +-\n Src/C_BaseLib/MultiFabUtil.cpp                       |  284 +++++----\n Src/C_BaseLib/MultiFabUtil_3d.cpp                    |  247 --------\n Src/C_BaseLib/MultiFabUtil_F.H                       |    8 -\n Src/C_BoundaryLib/Mask.H                             |    1 -\n Src/C_BoundaryLib/Mask.cpp                           |    2 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.H           |    9 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.cpp         | 1119 ++++++++++++++++-------------------\n Src/LinearSolvers/C_CellMG/ABec_3D.F                 |    4 +-\n Src/LinearSolvers/C_CellMG/CGSolver.H                |    6 +-\n Src/LinearSolvers/C_CellMG/CGSolver.cpp              |   13 +-\n Src/LinearSolvers/C_CellMG/LO_3D_cpp.cpp             |  235 --------\n Src/LinearSolvers/C_CellMG/LO_F.H                    |    5 -\n Src/LinearSolvers/C_CellMG/Laplacian.H               |    3 +-\n Src/LinearSolvers/C_CellMG/Laplacian.cpp             |  343 ++++++-----\n Src/LinearSolvers/C_CellMG/LinOp.H                   |   12 +-\n Src/LinearSolvers/C_CellMG/LinOp.cpp                 |   85 +--\n Src/LinearSolvers/C_CellMG/MG_3D_cpp.cpp             |  464 ---------------\n Src/LinearSolvers/C_CellMG/MG_3D_fortran.F           |   96 ---\n Src/LinearSolvers/C_CellMG/MG_3D_old.cpp             |  222 -------\n Src/LinearSolvers/C_CellMG/MG_F.H                    |   81 ---\n Src/LinearSolvers/C_CellMG/Make.package              |    6 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.H               |    4 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.cpp             | 1463 +++++++++++++++++++++++-----------------------\n Src/LinearSolvers/C_CellMG/old/MG_3D_cpp.cpp-average |   39 --\n Src/LinearSolvers/C_CellMG4/ABec2.H                  |    5 +-\n Src/LinearSolvers/C_CellMG4/ABec4.H                  |    3 +-\n Src/LinearSolvers/C_CellMG4/ABec4.cpp                |    7 +-\n Tools/C_mk/Make.rules                                |    4 +-\n Tools/Postprocessing/F_Src/GNUmakefile               |    2 +-\n Tutorials/MultiGrid_C/COEF_3D.F90                    |   14 +-\n Tutorials/MultiGrid_C/COEF_F.H                       |   10 +-\n Tutorials/MultiGrid_C/GNUmakefile                    |   28 +-\n Tutorials/MultiGrid_C/KokkosCore_config.h            |   11 -\n Tutorials/MultiGrid_C/KokkosCore_config.tmp          |   11 -\n Tutorials/MultiGrid_C/MG_helpers_cpp.cpp             |  162 ------\n Tutorials/MultiGrid_C/Make.package                   |    2 +-\n Tutorials/MultiGrid_C/RHS_3D.F90                     |  143 ++---\n Tutorials/MultiGrid_C/RHS_F.H                        |    3 +-\n Tutorials/MultiGrid_C/fcompare                       |  Bin 3475616 -\n 0 bytes\n Tutorials/MultiGrid_C/inputs                         |    6 +-\n Tutorials/MultiGrid_C/main.cpp                       | 1530 ++++++++++++++++++++++++------------------------\n Tutorials/MultiGrid_C/out-F                          |  522 -----------------\n Tutorials/MultiGrid_C/out-cpp                        |  522 -----------------\n 55 files changed, 2455 insertions(+), 10764 deletions(-)\n\n\n\n\nClearly, this is a major endeavour and we stopped our explorations for the moment at this point. \nFurthermore, it is not clear what performance Kokkos can deliver for the tasks at hand. To assess that, we abandoned the full port and continued with a partial port described below.\n\n\nSecond Attempt\n\n\nSince porting the full application is a major effort but we still want to assess Kokkos' potential for BoxLib, we followed a different strategy in this attempt: we will port all performance relevant GMG kernels to Kokkos, copying data into a suitable view before calling the kernel, then using Kokkos' parallel dispatcher to launch the kernels, and then fill the results back into BoxLib's own \nBaseFab\n datatype. \nIt is important to note that BoxLib uses many offsets for indexing and those can be different for different fields. \n\n\nWe thus decided to encapsulate this complexity into a new class. Below we show the declaration of the one specialized for \nFArraBox\n datatypes, i.e. \nBaseFab\n instances with types \nReal\n.\n\n\ntemplate\n\n\nclass\n \nViewFab\nReal\n \n{\n\n\npublic\n:\n\n\n    \n//swap indices here to get kokkos\n-canonical layout\n\n    \nKOKKOS_INLINE_FUNCTION\n\n    \nReal\n \noperator\n()(\nconst\n \nint\n \ni\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nn\n \n=\n \n0\n){\n\n      \nreturn\n \ndata\n(\nn\n,\n \nk\n-\nsmallend\n[\n2\n],\n \nj\n-\nsmallend\n[\n1\n],\n \ni\n-\nsmallend\n[\n0\n]);\n\n    \n}\n\n\n    \nKOKKOS_INLINE_FUNCTION\n\n    \nReal\n \noperator\n()(\nconst\n \nint\n \ni\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nn\n \n=\n \n0\n)\n \nconst\n \n{\n\n        \nreturn\n \ndata\n(\nn\n,\n \nk\n-\nsmallend\n[\n2\n],\n \nj\n-\nsmallend\n[\n1\n],\n \ni\n-\nsmallend\n[\n0\n]);\n\n    \n}\n\n\n    \nvoid\n \ninit\n(\nconst\n \nFArrayBox\n \nrhs_\n,\n \nconst\n \nstd\n::\nstring\n \nname_\n);\n\n\n    \nViewFab\n(){}\n\n\n    \nViewFab\n(\nconst\n \nFArrayBox\n \nrhs_\n,\n \nconst\n \nstd\n::\nstring\n \nname_\n){\n\n        \ninit\n(\nrhs_\n,\nname_\n);\n\n    \n}\n\n\n    \nViewFab\nReal\n \noperator\n=\n(\nconst\n \nViewFab\nReal\n \nrhs_\n);\n\n\n    \n//write the view data into a FArrayBox\n\n    \nvoid\n \nfill\n(\nFArrayBox\n \nlhs_\n)\n \nconst\n;\n\n\nprivate\n:\n\n    \nstd\n::\nstring\n \nname\n;\n\n    \nint\n \nnumvars\n;\n\n    \nIntVect\n \nsmallend\n,\n \nbigend\n,\n \nlength\n;\n\n    \nKokkos\n::\nView\nReal\n****\n \ndata\n;\n\n\n};\n\n\n\n\n\nThe important aspect is that the access operator hides the offset indexing and thus keeps the kernels clean.\nNote that this class is similar to what we try to use in our first attempt, but this time we do not bury it deep into the Framework but rather only use it for making the individual kernels performance portable. The access operators need to be decorated with \nKOKKOS_INLINE_FUNCTION\n macros because they will be called from the device.\n\n\nThe average (restriction kernel) becomes:\n\nvoid\n \nC_AVERAGE\n(\nconst\n \nBox\n \nbx\n,\n\n               \nconst\n \nint\n \nnc\n,\n\n               \nFArrayBox\n \nc\n,\n\n               \nconst\n \nFArrayBox\n \nf\n){\n\n\n    \nconst\n \nint\n \n*\nlo\n \n=\n \nbx\n.\nloVect\n();\n\n    \nconst\n \nint\n \n*\nhi\n \n=\n \nbx\n.\nhiVect\n();\n\n    \nconst\n \nint\n*\n \ncb\n \n=\n \nbx\n.\ncbVect\n();\n\n\n    \n//convert fabs to views\n\n    \nViewFab\nReal\n \ncv\n(\nc\n,\nc\n),\n \nfv\n(\nf\n,\nf\n);\n\n\n    \n//define iteration policy\n\n    \ntypedef\n \nKokkos\n::\nExperimental\n::\nMDRangePolicy\nKokkos\n::\nExperimental\n::\nRank\n3\n \n \nt_policy\n;\n\n\n    \n//execute kernel\n\n    \nKokkos\n::\nExperimental\n::\nmd_parallel_for\n(\nt_policy\n({\n0\n,\nlo\n[\n2\n],\nlo\n[\n1\n],\nlo\n[\n0\n]},\n\n                                                   \n{\nnc\n,\nhi\n[\n2\n]\n+\n1\n,\nhi\n[\n1\n]\n+\n1\n,\nhi\n[\n0\n]\n+\n1\n},\n\n                                                   \n{\nnc\n,\ncb\n[\n2\n],\ncb\n[\n1\n],\ncb\n[\n0\n]}),\n\n      \nKOKKOS_LAMBDA\n(\nconst\n \nint\n \nn\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nj\n,\n \nconst\n \nint\n \ni\n){\n\n        \ncv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n=\n  \n0.125\n \n*\n \n(\n   \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n,\n2\n*\nk\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n,\n2\n*\nk\n,\nn\n)\n\n                               \n);\n\n        \ncv\n(\ni\n,\nj\n,\nk\n,\nn\n)\n \n+=\n \n0.125\n \n*\n \n(\n   \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n+\n1\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n+\n1\n,\n2\n*\nj\n,\n2\n*\nk\n+\n1\n,\nn\n)\n \n                                 \n+\n \nfv\n(\n2\n*\ni\n,\n2\n*\nj\n,\n2\n*\nk\n+\n1\n,\nn\n)\n\n                               \n);\n\n      \n});\n\n\n    \n//write results back to fab\n\n    \ncv\n.\nfill\n(\nc\n);\n\n\n}\n\n\n\n\nIn order to employ loop-collapsing and additional cache blocking, we use the experimental multi-dimensional iteration policy feature. We added a cache-block-sizes vector \ncb\n which can be specified in the input file passed to the application. In principle, all kernels can be ported like the above example, with the exception of the GSRB kernel.\nThere we use a 2D iteration policy and express the loop over \ni\n explicitly in the lambda. This is necessary because the Kokkos iteration policies cannot do strided data access at the moment. The relevant part of the kernel is displayed below:\n\n\nKokkos\n::\nExperimental\n::\nmd_parallel_for\n(\nt_policy\n({\n0\n,\nlo\n[\n2\n],\nlo\n[\n1\n]},\n\n                                               \n{\nnc\n,\nhi\n[\n2\n]\n+\n1\n,\nhi\n[\n1\n]\n+\n1\n},\n\n                                               \n{\nnc\n,\ncb\n[\n2\n],\ncb\n[\n1\n]}),\n\n  \nKOKKOS_LAMBDA\n(\nconst\n \nint\n \nn\n,\n \nconst\n \nint\n \nk\n,\n \nconst\n \nint\n \nj\n){\n\n    \nint\n \nioff\n \n=\n \n(\nlo\n[\n0\n]\n \n+\n \nj\n \n+\n \nk\n \n+\n \nrb\n)\n \n%\n \n2\n;\n\n    \nfor\n(\nint\n \ni\n=\nioff\n;\n \ni\n=\nhi\n[\n0\n];\n \ni\n+=\n2\n){\n\n      \n...\n\n    \n}\n\n  \n});\n\n\n\n\n\nWe experimented with including the full loop over \ni\n into our iteration policy and then skipping the iteration if \n(i + j + k) % 2 != 0\n, but that decreased performance on the CPU by more than 50%. For facilitating this, it would be desirable if Kokkos would provide an iteration policy which can perform strided loops.\n\n\nThis approach clearly comes with data transfer overhead which we would have avoided if we would have followed through our first attempt. However, in the performance timings we will report later on, we explicitly exclude that overhead and just assess the runtime of the kernels themselves. That way, we can determine if Kokkos is a viable framework for ensuring performance portability of BoxLib across acthitectures.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#kokkos-implementation", 
            "text": "This section is written as some kind of lab report, since we think that it illustrates best what challenges users might face when making complicated framworks such as BoxLib performance portable.", 
            "title": "Kokkos Implementation"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#first-attempt", 
            "text": "This section will describe our first, unfinished attempt, to port BoxLib over to Kokkos. The approach was designed to be the cleanest but also the most difficult one. We learned some lessons in the process which we want to share with the reader of this case study.", 
            "title": "First Attempt"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#memory-management", 
            "text": "In theory, one would like to make the data resident on the  device  all the time to avoid unneccessary data transfer. We thus implemented a  KArenaND  class which allows us to use Kokkos views instead of plain arrays for storing the data. This is the abstract, N-dimensional class which provides a part of the interface:  //generic ND template  template   typename   T ,   int   D  class   KArenaND   {  public : \n     // \n     // Allocates a dynamic memory arena of size sz. \n     // Returns a pointer to this memory. \n     // \n     virtual   void *   alloc   ( const   std :: vector std :: size_t   _sz )   =   0 ;  protected : \n     virtual   void   free ()   =   0 ;  };   We then (partially) specialize this class in order generate a  KArena3D -variant, as this is what we mostly use in our code. This class is defined as  template   typename   T  class   KArenaND T , 3   {  public : \n     // \n     // Allocates a dynamic memory arena of size sz. \n     // Returns a pointer to this memory. \n     // \n     void *   alloc   ( const   std :: vector size_t   sz_vec ); \n     //pass access operator through to simplify things \n     T   operator ()( int   a0 ,   int   a1 ,   int   a2 ); \n     const   T   operator ()( int   a0 ,   int   a1 ,   int   a2 )   const ; \n     T   operator ()( const   IntVect   a ); \n     const   T   operator ()( const   IntVect   a )   const ; \n     //and return the view \n     Kokkos :: View T ***   viewData (){   return   view ;   }  private : \n     void   free (); \n     Kokkos :: View T ***   view ;  };   The corresponding allocator looks like then:  template typename   T  void *   KArenaND T , 3 :: alloc   ( const   std :: vector size_t   _sz_vec )  { \n     if ( _sz_vec . size () != 3 ){ \n         BoxLib :: Abort ( Error, the vector size passed to KArenaND has to be equal to its dimension! ); \n     } \n     //important: reverse dimensions for optimal access \n     view   =   Kokkos :: View T *** ( KArena_view3D , _sz_vec [ 2 ], _sz_vec [ 1 ], _sz_vec [ 0 ]); \n     //provide interface compatibility with the rest of boxlib, but never use that pointer. \n     return   reinterpret_cast void * ( view . ptr_on_device ());  }    Warning  When packing Views into classes, avoid using pointers and the  new  operator to instantiate a new View.  This pointer will be a host-only pointer and will have NULL value when accessed from a device which uses a different address space.  The reference counting is only guaranteed to work properly if the View is stored and passed by value. In that sense, also avoid passing references to views. Note that Kokkos::Views are lighweight objects so there is no performance reason to use pointers/references instead of values in this case.   We further implemented operators to access the memory, which basically pass the View access operator to the outside. For example:  template typename   T  T   KArenaND T , 3 :: operator ()( int   a0 ,   int   a1 ,   int   a2 )  { \n     //indices reversed compared to BoxLib \n     //in roder to ensure interface compatibility \n     return   view ( a2 , a1 , a0 );  }   Note that we reverse the order of indices here. This is because BoxLib uses the indexing  (x,y,z)  whereas for Kokkos views it is more convenient if the order  (z,y,x)  is used so that one can use Kokkos default layouts, i.e.  Layout::Left  on GPU and  Layout::Right  on CPU. If one would use the BoxLib indexing on the view level, this logic would need to be inverted whenever a Kokkos parallel dispatch is used. Instead, we invert it on the access operator level so that we neither need to explicitly specify an iteration policy nor break the BoxLib indexing order in the rest of the code.  There are advantages and disadvantages to burying the Kokkos data containers deep into the framework. The obvious advantage is that once it works, basically the majority of the framwork will already be Kokkos compatible. The disadvantage is that incremental porting is not possible, it is an all-or-nothing approach.", 
            "title": "Memory Management"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#rewriting-fortran-kernels", 
            "text": "A big difficulty with porting the BoxLib GMG to Kokkos is that most of the kernels are written in Fortran. For Kokkos, we need those kernels in C++ so we have started rewriting those kernels accordingly. As it turns out, the Kokkos GMG tutorial touches many of these Fortran kernels, and so we stopped after altering almost 10K lines of code in about 50 files. Below you find the comparison between the BoxLib master branch from which we started to the current state of the Kokkos port branch.  ~/BoxLib  git diff --stat cpp_kernels_kokkos-views \n\n Src/C_BaseLib/FArrayBox.H                            |    2 -\n Src/C_BaseLib/FabArray.H                             |    8 +-\n Src/C_BaseLib/IArrayBox.H                            |    1 -\n Src/C_BaseLib/KArena.H                               |  265 ---------\n Src/C_BaseLib/KBaseFab.H                             | 3615 -----------------------------------------------------------------------------------------------------------------\n Src/C_BaseLib/Looping.H                              |  770 +-----------------------\n Src/C_BaseLib/Make.package                           |    6 +-\n Src/C_BaseLib/MultiFabUtil.cpp                       |  284 +++++----\n Src/C_BaseLib/MultiFabUtil_3d.cpp                    |  247 --------\n Src/C_BaseLib/MultiFabUtil_F.H                       |    8 -\n Src/C_BoundaryLib/Mask.H                             |    1 -\n Src/C_BoundaryLib/Mask.cpp                           |    2 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.H           |    9 +-\n Src/LinearSolvers/C_CellMG/ABecLaplacian.cpp         | 1119 ++++++++++++++++-------------------\n Src/LinearSolvers/C_CellMG/ABec_3D.F                 |    4 +-\n Src/LinearSolvers/C_CellMG/CGSolver.H                |    6 +-\n Src/LinearSolvers/C_CellMG/CGSolver.cpp              |   13 +-\n Src/LinearSolvers/C_CellMG/LO_3D_cpp.cpp             |  235 --------\n Src/LinearSolvers/C_CellMG/LO_F.H                    |    5 -\n Src/LinearSolvers/C_CellMG/Laplacian.H               |    3 +-\n Src/LinearSolvers/C_CellMG/Laplacian.cpp             |  343 ++++++-----\n Src/LinearSolvers/C_CellMG/LinOp.H                   |   12 +-\n Src/LinearSolvers/C_CellMG/LinOp.cpp                 |   85 +--\n Src/LinearSolvers/C_CellMG/MG_3D_cpp.cpp             |  464 ---------------\n Src/LinearSolvers/C_CellMG/MG_3D_fortran.F           |   96 ---\n Src/LinearSolvers/C_CellMG/MG_3D_old.cpp             |  222 -------\n Src/LinearSolvers/C_CellMG/MG_F.H                    |   81 ---\n Src/LinearSolvers/C_CellMG/Make.package              |    6 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.H               |    4 +-\n Src/LinearSolvers/C_CellMG/MultiGrid.cpp             | 1463 +++++++++++++++++++++++-----------------------\n Src/LinearSolvers/C_CellMG/old/MG_3D_cpp.cpp-average |   39 --\n Src/LinearSolvers/C_CellMG4/ABec2.H                  |    5 +-\n Src/LinearSolvers/C_CellMG4/ABec4.H                  |    3 +-\n Src/LinearSolvers/C_CellMG4/ABec4.cpp                |    7 +-\n Tools/C_mk/Make.rules                                |    4 +-\n Tools/Postprocessing/F_Src/GNUmakefile               |    2 +-\n Tutorials/MultiGrid_C/COEF_3D.F90                    |   14 +-\n Tutorials/MultiGrid_C/COEF_F.H                       |   10 +-\n Tutorials/MultiGrid_C/GNUmakefile                    |   28 +-\n Tutorials/MultiGrid_C/KokkosCore_config.h            |   11 -\n Tutorials/MultiGrid_C/KokkosCore_config.tmp          |   11 -\n Tutorials/MultiGrid_C/MG_helpers_cpp.cpp             |  162 ------\n Tutorials/MultiGrid_C/Make.package                   |    2 +-\n Tutorials/MultiGrid_C/RHS_3D.F90                     |  143 ++---\n Tutorials/MultiGrid_C/RHS_F.H                        |    3 +-\n Tutorials/MultiGrid_C/fcompare                       |  Bin 3475616 -  0 bytes\n Tutorials/MultiGrid_C/inputs                         |    6 +-\n Tutorials/MultiGrid_C/main.cpp                       | 1530 ++++++++++++++++++++++++------------------------\n Tutorials/MultiGrid_C/out-F                          |  522 -----------------\n Tutorials/MultiGrid_C/out-cpp                        |  522 -----------------\n 55 files changed, 2455 insertions(+), 10764 deletions(-)  Clearly, this is a major endeavour and we stopped our explorations for the moment at this point. \nFurthermore, it is not clear what performance Kokkos can deliver for the tasks at hand. To assess that, we abandoned the full port and continued with a partial port described below.", 
            "title": "Rewriting Fortran Kernels"
        }, 
        {
            "location": "/case_studies/amr/kokkos_implementation/#second-attempt", 
            "text": "Since porting the full application is a major effort but we still want to assess Kokkos' potential for BoxLib, we followed a different strategy in this attempt: we will port all performance relevant GMG kernels to Kokkos, copying data into a suitable view before calling the kernel, then using Kokkos' parallel dispatcher to launch the kernels, and then fill the results back into BoxLib's own  BaseFab  datatype. \nIt is important to note that BoxLib uses many offsets for indexing and those can be different for different fields.   We thus decided to encapsulate this complexity into a new class. Below we show the declaration of the one specialized for  FArraBox  datatypes, i.e.  BaseFab  instances with types  Real .  template  class   ViewFab Real   {  public : \n\n     //swap indices here to get kokkos -canonical layout \n     KOKKOS_INLINE_FUNCTION \n     Real   operator ()( const   int   i ,   const   int   j ,   const   int   k ,   const   int   n   =   0 ){ \n       return   data ( n ,   k - smallend [ 2 ],   j - smallend [ 1 ],   i - smallend [ 0 ]); \n     } \n\n     KOKKOS_INLINE_FUNCTION \n     Real   operator ()( const   int   i ,   const   int   j ,   const   int   k ,   const   int   n   =   0 )   const   { \n         return   data ( n ,   k - smallend [ 2 ],   j - smallend [ 1 ],   i - smallend [ 0 ]); \n     } \n\n     void   init ( const   FArrayBox   rhs_ ,   const   std :: string   name_ ); \n\n     ViewFab (){} \n\n     ViewFab ( const   FArrayBox   rhs_ ,   const   std :: string   name_ ){ \n         init ( rhs_ , name_ ); \n     } \n\n     ViewFab Real   operator = ( const   ViewFab Real   rhs_ ); \n\n     //write the view data into a FArrayBox \n     void   fill ( FArrayBox   lhs_ )   const ;  private : \n     std :: string   name ; \n     int   numvars ; \n     IntVect   smallend ,   bigend ,   length ; \n     Kokkos :: View Real ****   data ;  };   The important aspect is that the access operator hides the offset indexing and thus keeps the kernels clean.\nNote that this class is similar to what we try to use in our first attempt, but this time we do not bury it deep into the Framework but rather only use it for making the individual kernels performance portable. The access operators need to be decorated with  KOKKOS_INLINE_FUNCTION  macros because they will be called from the device.  The average (restriction kernel) becomes: void   C_AVERAGE ( const   Box   bx , \n                const   int   nc , \n                FArrayBox   c , \n                const   FArrayBox   f ){ \n\n     const   int   * lo   =   bx . loVect (); \n     const   int   * hi   =   bx . hiVect (); \n     const   int *   cb   =   bx . cbVect (); \n\n     //convert fabs to views \n     ViewFab Real   cv ( c , c ),   fv ( f , f ); \n\n     //define iteration policy \n     typedef   Kokkos :: Experimental :: MDRangePolicy Kokkos :: Experimental :: Rank 3     t_policy ; \n\n     //execute kernel \n     Kokkos :: Experimental :: md_parallel_for ( t_policy ({ 0 , lo [ 2 ], lo [ 1 ], lo [ 0 ]}, \n                                                    { nc , hi [ 2 ] + 1 , hi [ 1 ] + 1 , hi [ 0 ] + 1 }, \n                                                    { nc , cb [ 2 ], cb [ 1 ], cb [ 0 ]}), \n       KOKKOS_LAMBDA ( const   int   n ,   const   int   k ,   const   int   j ,   const   int   i ){ \n         cv ( i , j , k , n )   =    0.125   *   (     fv ( 2 * i + 1 , 2 * j + 1 , 2 * k , n )  \n                                  +   fv ( 2 * i , 2 * j + 1 , 2 * k , n )  \n                                  +   fv ( 2 * i + 1 , 2 * j , 2 * k , n )  \n                                  +   fv ( 2 * i , 2 * j , 2 * k , n ) \n                                ); \n         cv ( i , j , k , n )   +=   0.125   *   (     fv ( 2 * i + 1 , 2 * j + 1 , 2 * k + 1 , n )  \n                                  +   fv ( 2 * i , 2 * j + 1 , 2 * k + 1 , n )  \n                                  +   fv ( 2 * i + 1 , 2 * j , 2 * k + 1 , n )  \n                                  +   fv ( 2 * i , 2 * j , 2 * k + 1 , n ) \n                                ); \n       }); \n\n     //write results back to fab \n     cv . fill ( c );  }   In order to employ loop-collapsing and additional cache blocking, we use the experimental multi-dimensional iteration policy feature. We added a cache-block-sizes vector  cb  which can be specified in the input file passed to the application. In principle, all kernels can be ported like the above example, with the exception of the GSRB kernel.\nThere we use a 2D iteration policy and express the loop over  i  explicitly in the lambda. This is necessary because the Kokkos iteration policies cannot do strided data access at the moment. The relevant part of the kernel is displayed below:  Kokkos :: Experimental :: md_parallel_for ( t_policy ({ 0 , lo [ 2 ], lo [ 1 ]}, \n                                                { nc , hi [ 2 ] + 1 , hi [ 1 ] + 1 }, \n                                                { nc , cb [ 2 ], cb [ 1 ]}), \n   KOKKOS_LAMBDA ( const   int   n ,   const   int   k ,   const   int   j ){ \n     int   ioff   =   ( lo [ 0 ]   +   j   +   k   +   rb )   %   2 ; \n     for ( int   i = ioff ;   i = hi [ 0 ];   i += 2 ){ \n       ... \n     } \n   });   We experimented with including the full loop over  i  into our iteration policy and then skipping the iteration if  (i + j + k) % 2 != 0 , but that decreased performance on the CPU by more than 50%. For facilitating this, it would be desirable if Kokkos would provide an iteration policy which can perform strided loops.  This approach clearly comes with data transfer overhead which we would have avoided if we would have followed through our first attempt. However, in the performance timings we will report later on, we explicitly exclude that overhead and just assess the runtime of the kernels themselves. That way, we can determine if Kokkos is a viable framework for ensuring performance portability of BoxLib across acthitectures.", 
            "title": "Second Attempt"
        }, 
        {
            "location": "/case_studies/gw/", 
            "text": "", 
            "title": "GW Kernels"
        }, 
        {
            "location": "/case_studies/qcd/overview/", 
            "text": "Introduction to Lattice QCD\n\n\nLattice QCD\n is a numerical method to evaluate \nQuantum Chromodynamics (QCD)\n,\nthe theory of the strong interaction which binds quarks into nucleons and nucleons into nuclei,\nin a straightforward way with quantifiable uncertainties. It is non-perturbative and thus has access\nto energy regimes where common analytical methods fail.\nIn orer to transform continuum QCD to Lattice QCD, one first rotates the time axis to imaginary times which \ntransforms the 4-dimensional \nMinkowski space\n into Eculidian \n\\(\\mathbb{R}^4\\)\n. Then,\neuclidian space-time is discretized by introducing a lattice spacing \n\\(a\\)\n as well as finite volume with side extents \n\\(L\\)\n.\n\n\nWilson Fermions\n\n\nThe most expensive part of Lattice QCD is the calculation of so-called quark propagators, i.e.\ncomputing the solution of the Dirac equation\n\n\\((m - /\\!\\!\\!\\!D)\\psi = \\eta\\)\n, where \n\\(m\\)\n is the mass of the particle, \n\\(\\eta\\)\n is a given vector (we will refer to this object as \nsource\n or \nright-hand-side spinor\n)\nand \n\\(/\\!\\!\\!\\!D\\)\n is a so-called gauge-covariant, Fermion derivative operator. There are many possibilities for discretizing the\ncontinuum version of the Fermion derivative operator and the most common one are the so-called \nWilson fermions\n. In this discretizaton,\nthe operator, also called Wilson operator, is given by\n\n\n\\[\n/\\!\\!\\!\\!D(x,y) = \\sum\\limits_{\\mu=0}^3 U_{\\mu}(x)(1-\\gamma_{\\mu})\\delta_{y,x+\\hat{\\mu}}+U^{\\dagger}_{\\mu}(x-\\hat{\\mu})(1+\\gamma_{\\mu})\\delta_{y,x-\\hat{\\mu}}.\n\\]\nHere, \n\\(\\hat{\\mu}\\)\n denotes a displacement in \n\\(\\mu\\)\n-direction by one lattice site. \n\\(U_{\\mu}(x)\\)\n are the so-called links connecting the neighboring sites \n\\(x\\)\n and \n\\(x+\\hat{\\mu}\\)\n in a gauge-covariant way. They are elements of \n\\(SU(3)\\)\n, i.e. they can be described by 3x3 complex-valued, \nunitary matrices\n with unit \ndeterminant\n. The \n\\(\\gamma_{\\mu}\\)\n are sparse 4x4 matrices and are the generators of the so-called \nDirac algebra\n, a 4-dimensional spin \nClifford algebra\n. The indices of \n\\(U\\)\n and \n\\(\\gamma\\)\n are called color and spin indices respectively. \nNote that the Wilson operator couples only neighboring lattice sites and is thus ultra-local.\n\n\nIn modern lattice calculations, the majority of CPU time is spent on solving the Dirac equation. Therefore,\nmost optimization efforts focus on optimizing the Wilson operator as well as solvers which use this operator as their kernel.\nIt is thus importanto to find out whether the Wilson operator can be implemented in a performance portable way.\n\n\nImplementation\n\n\nIn this section we will briefly discuss architecture-independent implementation details of the Wilson operator. \n\n\nMultiple Right Hand Sides\n\n\nAn efficient way to increase the arithmetic intensity in sparse linear systems is to solve for multiple right hand side vectors simulatenously.\nThis case is also relevant to many lattice QCD applications so that we have implemented this optimization in our small test case. \n\n\nArithmetic Intensity\n\n\nThe arithmetic intensity for the Wilson operator can be computed as follows:\n\n\n\\[\n\\frac{\\#\\mathrm{Flops}}{\\#\\mathrm{Bytes}} = \\frac{1320}{8G + (9-R+r)S},\n\\]\nwhere \n\\(G\\)\n is the size of a gauge link, \n\\(S\\)\n the size of a spinor, \n\\(R\\)\n the nearest neighbor spinor reuse factor and \n\\(r=0\\)\n if streaming stores are used and \n\\(r=1\\)\n otherwise (read-for-write). The constant factors account for the fact that in 4 dimensions, each lattice site has 8 neighbors and thus 8 links and spinors needs to be read from memory and one spinor needs to be written. If no streaming stores are used, the output spinor needs to be read into cache first and thus the total number of spinors transferred per computed site will be 10 in this case. Whereas the spinor always consists of 12 complex numbers (3 color and 4 spin components), the gauge links G can be in theory compressed to 8 real numbers by using properties of \nLie algebras\n along with the generators of \n\\(SU(3)\\)\n. However, this is very expensive so that usually a less aggressive form of compression is used by simply dropping one row or column of the gauge link and reconstruct it on the fly when needed. This format is called \n12-compression\n and widely used in modern Wilson operator implementations. In our simple test case however, we do not use this kind of compression and thus the expected arithmetic intensity is between \n\\(0.86\\)\n \n\\((R=0,\\,r=1,\\,G=18)\\)\n and \n\\(1.72\\)\n \n\\((R=7,\\,r=0,\\,G=18)\\)\n for single precision.\n\n\nWe have applied two optimizations to our Wilson dslash test code\n\n\n\n\nwe replace the Wilson operator with it's \nSchur complement\n, i.e. we use \n\\(M_{oo} = m - /\\!\\!\\!\\!D_{oe} m^{-1} /\\!\\!\\!\\!D_{eo}\\)\n instead of applying \n\\(/\\!\\!\\!\\!D\\)\n directly. Here, the indices \n\\(oo\\)\n, \n\\(oe\\)\n and \n\\(eo\\)\n indicate that the respective operators only couple odd-odd, odd-even or even-odd sites respectively. With this optimization, the modified problem can essentially be solved on a volume half as big as the original problem and the solution easily be reconstructed for the other half.\n\n\nwe use properties of the Dirac matrices to project the 4-spinors to two pairs of linear dependent 2-spinors before applying the dslash, saving 50% of the required flops.\n\n\nwe solve for multiple right hand side vectors simultaneously to increase the arithmetic intensity. This optimizations amounts to multiplying the number of flops as well as the number of reads and stores by the number of right hand sides \n\\(N\\)\n. Since all these vectors should ideally be kept in cache, the effective reuse factor \n\\(R\\)\n will drop with increasing \n\\(N\\)\n. In our testcase we vectorize using SIMD or SIMT over these right hand side vectors, so \n\\(N\\)\n should ideally be an integer multiple of the vector/warp size. These two aspects have to be taken into account when optimizing the performance.", 
            "title": "Overview"
        }, 
        {
            "location": "/case_studies/qcd/overview/#introduction-to-lattice-qcd", 
            "text": "Lattice QCD  is a numerical method to evaluate  Quantum Chromodynamics (QCD) ,\nthe theory of the strong interaction which binds quarks into nucleons and nucleons into nuclei,\nin a straightforward way with quantifiable uncertainties. It is non-perturbative and thus has access\nto energy regimes where common analytical methods fail.\nIn orer to transform continuum QCD to Lattice QCD, one first rotates the time axis to imaginary times which \ntransforms the 4-dimensional  Minkowski space  into Eculidian  \\(\\mathbb{R}^4\\) . Then,\neuclidian space-time is discretized by introducing a lattice spacing  \\(a\\)  as well as finite volume with side extents  \\(L\\) .", 
            "title": "Introduction to Lattice QCD"
        }, 
        {
            "location": "/case_studies/qcd/overview/#wilson-fermions", 
            "text": "The most expensive part of Lattice QCD is the calculation of so-called quark propagators, i.e.\ncomputing the solution of the Dirac equation \\((m - /\\!\\!\\!\\!D)\\psi = \\eta\\) , where  \\(m\\)  is the mass of the particle,  \\(\\eta\\)  is a given vector (we will refer to this object as  source  or  right-hand-side spinor )\nand  \\(/\\!\\!\\!\\!D\\)  is a so-called gauge-covariant, Fermion derivative operator. There are many possibilities for discretizing the\ncontinuum version of the Fermion derivative operator and the most common one are the so-called  Wilson fermions . In this discretizaton,\nthe operator, also called Wilson operator, is given by  \\[\n/\\!\\!\\!\\!D(x,y) = \\sum\\limits_{\\mu=0}^3 U_{\\mu}(x)(1-\\gamma_{\\mu})\\delta_{y,x+\\hat{\\mu}}+U^{\\dagger}_{\\mu}(x-\\hat{\\mu})(1+\\gamma_{\\mu})\\delta_{y,x-\\hat{\\mu}}.\n\\] Here,  \\(\\hat{\\mu}\\)  denotes a displacement in  \\(\\mu\\) -direction by one lattice site.  \\(U_{\\mu}(x)\\)  are the so-called links connecting the neighboring sites  \\(x\\)  and  \\(x+\\hat{\\mu}\\)  in a gauge-covariant way. They are elements of  \\(SU(3)\\) , i.e. they can be described by 3x3 complex-valued,  unitary matrices  with unit  determinant . The  \\(\\gamma_{\\mu}\\)  are sparse 4x4 matrices and are the generators of the so-called  Dirac algebra , a 4-dimensional spin  Clifford algebra . The indices of  \\(U\\)  and  \\(\\gamma\\)  are called color and spin indices respectively. \nNote that the Wilson operator couples only neighboring lattice sites and is thus ultra-local.  In modern lattice calculations, the majority of CPU time is spent on solving the Dirac equation. Therefore,\nmost optimization efforts focus on optimizing the Wilson operator as well as solvers which use this operator as their kernel.\nIt is thus importanto to find out whether the Wilson operator can be implemented in a performance portable way.", 
            "title": "Wilson Fermions"
        }, 
        {
            "location": "/case_studies/qcd/overview/#implementation", 
            "text": "In this section we will briefly discuss architecture-independent implementation details of the Wilson operator.", 
            "title": "Implementation"
        }, 
        {
            "location": "/case_studies/qcd/overview/#multiple-right-hand-sides", 
            "text": "An efficient way to increase the arithmetic intensity in sparse linear systems is to solve for multiple right hand side vectors simulatenously.\nThis case is also relevant to many lattice QCD applications so that we have implemented this optimization in our small test case.", 
            "title": "Multiple Right Hand Sides"
        }, 
        {
            "location": "/case_studies/qcd/overview/#arithmetic-intensity", 
            "text": "The arithmetic intensity for the Wilson operator can be computed as follows:  \\[\n\\frac{\\#\\mathrm{Flops}}{\\#\\mathrm{Bytes}} = \\frac{1320}{8G + (9-R+r)S},\n\\] where  \\(G\\)  is the size of a gauge link,  \\(S\\)  the size of a spinor,  \\(R\\)  the nearest neighbor spinor reuse factor and  \\(r=0\\)  if streaming stores are used and  \\(r=1\\)  otherwise (read-for-write). The constant factors account for the fact that in 4 dimensions, each lattice site has 8 neighbors and thus 8 links and spinors needs to be read from memory and one spinor needs to be written. If no streaming stores are used, the output spinor needs to be read into cache first and thus the total number of spinors transferred per computed site will be 10 in this case. Whereas the spinor always consists of 12 complex numbers (3 color and 4 spin components), the gauge links G can be in theory compressed to 8 real numbers by using properties of  Lie algebras  along with the generators of  \\(SU(3)\\) . However, this is very expensive so that usually a less aggressive form of compression is used by simply dropping one row or column of the gauge link and reconstruct it on the fly when needed. This format is called  12-compression  and widely used in modern Wilson operator implementations. In our simple test case however, we do not use this kind of compression and thus the expected arithmetic intensity is between  \\(0.86\\)   \\((R=0,\\,r=1,\\,G=18)\\)  and  \\(1.72\\)   \\((R=7,\\,r=0,\\,G=18)\\)  for single precision.  We have applied two optimizations to our Wilson dslash test code   we replace the Wilson operator with it's  Schur complement , i.e. we use  \\(M_{oo} = m - /\\!\\!\\!\\!D_{oe} m^{-1} /\\!\\!\\!\\!D_{eo}\\)  instead of applying  \\(/\\!\\!\\!\\!D\\)  directly. Here, the indices  \\(oo\\) ,  \\(oe\\)  and  \\(eo\\)  indicate that the respective operators only couple odd-odd, odd-even or even-odd sites respectively. With this optimization, the modified problem can essentially be solved on a volume half as big as the original problem and the solution easily be reconstructed for the other half.  we use properties of the Dirac matrices to project the 4-spinors to two pairs of linear dependent 2-spinors before applying the dslash, saving 50% of the required flops.  we solve for multiple right hand side vectors simultaneously to increase the arithmetic intensity. This optimizations amounts to multiplying the number of flops as well as the number of reads and stores by the number of right hand sides  \\(N\\) . Since all these vectors should ideally be kept in cache, the effective reuse factor  \\(R\\)  will drop with increasing  \\(N\\) . In our testcase we vectorize using SIMD or SIMT over these right hand side vectors, so  \\(N\\)  should ideally be an integer multiple of the vector/warp size. These two aspects have to be taken into account when optimizing the performance.", 
            "title": "Arithmetic Intensity"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/", 
            "text": "Code Structure\n\n\nOur testcode is written in C++ and designed completely from scratch. We use type definitions, templates, template-specialization, overloading and other C++ features to provide flexibility in changing precision, testing datatypes which help vectorization and also making it easier to hide architecture dependent code. The general idea is to decompose the problem into a loop over lattice site and then for each lattice site and each direction, we:\n\n\n\n\nstream-in the relevant spinors (or block of spinors in case of multiple right hand sides) from memory\n\n\nproject 4-spinors to 2-spinors\n\n\nread relevant gauge links and apply the dslash to those vectors\n\n\ninject 2-spinors into 4-spinors\n\n\nstream-out the solution vectors to memory\n\n\n\n\nIn multi-node implementations the application step would be separated into bulk- and boundary application and the former interleaved with boundary communication.\n\n\nData Primitives\n\n\nFor facilitating this workflow, we define spinor and gauge link classes (in C++-like pseudocode):\n\n\ntemplate\ntypename\n \nST\n,\nint\n \nnspin\n \n\nclass\n \nCBSpinor\n \n{\n\n\npublic\n:\n \n    \n...\n\n\nprivate\n:\n\n    \n// dims are: site, color, spin\n\n    \nspin_container\nST\n[\nsite\n,\ncolor\n,\nspin\n]\n \ndata\n;\n\n\n};\n\n\n\ntemplate\ntypename\n \nGT\n \n\nclass\n \nCBGaugeField\n \n{\n\n\npublic\n:\n\n    \n...\n\n\nprivate\n:\n\n    \n// dims are: site, direction, color, color\n\n    \ngauge_container\nGT\n[\nsite\n,\n4\n,\n3\n,\n3\n]\n \ndata\n;\n\n\n};\n\n\n\n\n\nhere, ST and GT refer to spinor-type and gauge-type respectively. Those types could be SIMD or scalar types and they do not neccesarily need to be the same. The data containers can be plain arrays, e.g. for (unportable) plain implementations, or arrays decorated with pragmas (e.g. for OpenMP 4.5 offloading) or more general data container classes such as Kokkos::Views, etc.. The member functions are adopted to the container classes used in the individual implementations. Note that this design allows us to test different performance portable frameworks/methods without having to restructure large parts of the code. The additional template paramter \nnspin\n allows us to easily define 2- and 4-spinor objects. \n\n\nWilson Operator\n\n\nAt this point in time, the dslash testcode is not multi-node ready, so we will focus solely on on-node parallelism for the moment. Our goal is to achieve this by threading over lattice sites and applying SIMD/SIMT parallelism over multiple right hand sides. In theory, one could achieve vectorization for single right hand side vectors also by using an array or structure of array data layout but we will not consider this technique here. We will nevertheless compare our single right hand side performance we achieved with our performance portable implementations with those of optimized libraries which feature such improvements.\n\n\nOur dslash class is implemented as follow:\n\n\ntemplate\ntypename\n \nGT\n,\n \ntypename\n \nST\n,\n \ntypename\n \nTST\n\n\nclass\n \nDslash\n \n{\n\n    \npublic\n:\n\n    \nvoid\n \noperator\n(\nconst\n \nCBSpinor\nST\n,\n4\n \ns_in\n,\n\n                  \nconst\n \nCBGaugeField\nGT\n \ng_in\n,\n\n                  \nCBSpinor\nST\n,\n4\n \ns_out\n,\n\n                  \nint\n \nplus_minus\n)\n \n    \n{\n\n        \n// Threaded loop over sites\n\n        \nparallel_for\n(\nint\n \ni\n=\n0\n;\n \ni\nnum_sites\n;\n \ni\n++\n){\n\n            \nCBThreadSpinor\nTST\n,\n4\n \nres_sum_\n;\n\n            \nCBThreadSpinor\nTST\n,\n2\n \nproj_res\n,\n \nmult_proj_res\n;\n\n\n            \n//go for directions +T\n\n            \n//stream-in from +T direction and project to 2-spinor\n\n            \nproject_dir\nST\n,\nTST\n(\ns_in\n[\ni\n],\n \ni\n,\n \nproj_res\n,\n \nT_PLUS\n);\n\n            \n//multiply with gauge link\n\n            \nmult_adj_u_halfspinor\nGT\n,\nTST\n(\ng_in\n[\ni\n][\nT_PLUS\n],\n \nproj_res\n,\n \nmult_proj_res\n);\n\n            \n//reconstruct and add to result\n\n            \nreconstruct_dir\nTST\n,\nST\n(\nmult_proj_res\n,\n \nres_sum\n);\n\n\n            \n//go for direction -T\n\n            \n...\n\n        \n}\n\n    \n}\n\n\n};\n\n\n\n\n\nHere, the type \nTST\n denotes a thread-spinor-type which belongs to the \nCBThreadSpinor\n class. It is important to make the distinction between \nCBSpinor\n and \nCBThreadSpinor\n because, depending on the performance portability framework used, this type has to be different on CPU or GPU. What we would like to achieve ultimately is displayed in the picture below:\n\n\n\n\nIn case of the GPU (left), individual threads are each working on a single/scalar entry of the global spinor, i.e. on a single right hand side component. In case of the CPU (right), each thread is working on a chunk of right hand sites, ideally using its vector units. In both cases, the input and output spinor datatype is the same and the work spinor type is optimized for the targeted architecture. \n\n\nNote that, similar to the data classes discussed above, this skeleton-dslash allows us to specialize the Wilson operator for a variety of performance portable frameworks. Additionally, if we need more architectural specialization than the various frameworks could offer, this can be implemented cleanly by operator overloading and template specializations.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#code-structure", 
            "text": "Our testcode is written in C++ and designed completely from scratch. We use type definitions, templates, template-specialization, overloading and other C++ features to provide flexibility in changing precision, testing datatypes which help vectorization and also making it easier to hide architecture dependent code. The general idea is to decompose the problem into a loop over lattice site and then for each lattice site and each direction, we:   stream-in the relevant spinors (or block of spinors in case of multiple right hand sides) from memory  project 4-spinors to 2-spinors  read relevant gauge links and apply the dslash to those vectors  inject 2-spinors into 4-spinors  stream-out the solution vectors to memory   In multi-node implementations the application step would be separated into bulk- and boundary application and the former interleaved with boundary communication.", 
            "title": "Code Structure"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#data-primitives", 
            "text": "For facilitating this workflow, we define spinor and gauge link classes (in C++-like pseudocode):  template typename   ST , int   nspin   class   CBSpinor   {  public :  \n     ...  private : \n     // dims are: site, color, spin \n     spin_container ST [ site , color , spin ]   data ;  };  template typename   GT   class   CBGaugeField   {  public : \n     ...  private : \n     // dims are: site, direction, color, color \n     gauge_container GT [ site , 4 , 3 , 3 ]   data ;  };   here, ST and GT refer to spinor-type and gauge-type respectively. Those types could be SIMD or scalar types and they do not neccesarily need to be the same. The data containers can be plain arrays, e.g. for (unportable) plain implementations, or arrays decorated with pragmas (e.g. for OpenMP 4.5 offloading) or more general data container classes such as Kokkos::Views, etc.. The member functions are adopted to the container classes used in the individual implementations. Note that this design allows us to test different performance portable frameworks/methods without having to restructure large parts of the code. The additional template paramter  nspin  allows us to easily define 2- and 4-spinor objects.", 
            "title": "Data Primitives"
        }, 
        {
            "location": "/case_studies/qcd/code_structure/#wilson-operator", 
            "text": "At this point in time, the dslash testcode is not multi-node ready, so we will focus solely on on-node parallelism for the moment. Our goal is to achieve this by threading over lattice sites and applying SIMD/SIMT parallelism over multiple right hand sides. In theory, one could achieve vectorization for single right hand side vectors also by using an array or structure of array data layout but we will not consider this technique here. We will nevertheless compare our single right hand side performance we achieved with our performance portable implementations with those of optimized libraries which feature such improvements.  Our dslash class is implemented as follow:  template typename   GT ,   typename   ST ,   typename   TST  class   Dslash   { \n     public : \n     void   operator ( const   CBSpinor ST , 4   s_in , \n                   const   CBGaugeField GT   g_in , \n                   CBSpinor ST , 4   s_out , \n                   int   plus_minus )  \n     { \n         // Threaded loop over sites \n         parallel_for ( int   i = 0 ;   i num_sites ;   i ++ ){ \n             CBThreadSpinor TST , 4   res_sum_ ; \n             CBThreadSpinor TST , 2   proj_res ,   mult_proj_res ; \n\n             //go for directions +T \n             //stream-in from +T direction and project to 2-spinor \n             project_dir ST , TST ( s_in [ i ],   i ,   proj_res ,   T_PLUS ); \n             //multiply with gauge link \n             mult_adj_u_halfspinor GT , TST ( g_in [ i ][ T_PLUS ],   proj_res ,   mult_proj_res ); \n             //reconstruct and add to result \n             reconstruct_dir TST , ST ( mult_proj_res ,   res_sum ); \n\n             //go for direction -T \n             ... \n         } \n     }  };   Here, the type  TST  denotes a thread-spinor-type which belongs to the  CBThreadSpinor  class. It is important to make the distinction between  CBSpinor  and  CBThreadSpinor  because, depending on the performance portability framework used, this type has to be different on CPU or GPU. What we would like to achieve ultimately is displayed in the picture below:   In case of the GPU (left), individual threads are each working on a single/scalar entry of the global spinor, i.e. on a single right hand side component. In case of the CPU (right), each thread is working on a chunk of right hand sites, ideally using its vector units. In both cases, the input and output spinor datatype is the same and the work spinor type is optimized for the targeted architecture.   Note that, similar to the data classes discussed above, this skeleton-dslash allows us to specialize the Wilson operator for a variety of performance portable frameworks. Additionally, if we need more architectural specialization than the various frameworks could offer, this can be implemented cleanly by operator overloading and template specializations.", 
            "title": "Wilson Operator"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/", 
            "text": "Kokkos Implementation\n\n\nIn this implementation, we will use the \nKokkos::View\n type as our data container. Therefore, the \nspinor and gaugefield classes\n become\n\n\ntemplate\ntypename\n \nST\n,\nint\n \nnspin\n \n\nclass\n \nCBSpinor\n \n{\n\n\npublic\n:\n \n    \n...\n\n\nprivate\n:\n\n    \n// dims are: site, color, spin\n\n    \nKokkos\n::\nView\nST\n*\n[\n3\n][\nnspin\n]\n \ndata\n;\n\n\n};\n\n\n\ntemplate\ntypename\n \nGT\n \n\nclass\n \nCBGaugeField\n \n{\n\n\npublic\n:\n\n    \n...\n\n\nprivate\n:\n\n    \n// dims are: site, direction, color, color\n\n    \ngauge_container\nGT\n*\n[\n4\n][\n3\n][\n3\n]\n \ndata\n;\n\n\n};\n\n\n\n\n\nNote that the site index dimension is a runtime dimension (denoted by \n*\n) whereas the other dimensions - color and spin - are fixed (denoted by \n[const]\n). Explicitly stating this is recommended by the kokkos developers because it should help the compiler to optimize the code. \n\n\nIn the \nWilsion operator class\n, all what we need to do is to insert the kokkos parallel dispatcher. Hence it becomes\n\n\ntemplate\ntypename\n \nGT\n,\n \ntypename\n \nST\n,\n \ntypename\n \nTST\n\n\nclass\n \nDslash\n \n{\n\n    \npublic\n:\n\n    \nvoid\n \noperator\n(\nconst\n \nCBSpinor\nST\n,\n4\n \ns_in\n,\n\n                  \nconst\n \nCBGaugeField\nGT\n \ng_in\n,\n\n                  \nCBSpinor\nST\n,\n4\n \ns_out\n,\n\n                  \nint\n \nplus_minus\n)\n \n    \n{\n\n        \n// Threaded loop over sites\n\n        \nKokkos\n::\nparallel_for\n(\nnum_sites\n,\n \nKOKKOS_LAMBDA\n(\nint\n \ni\n)\n \n{\n\n            \n...\n\n            \n});\n\n    \n}\n\n\n};\n\n\n\n\n\nComplex Numbers and C++\n\n\nWe want to emphasize a subtle performance pitfall when it comes to complex numbers in C++. The language standards inhibit the compiler to efficiently optimize operations such as \n\n\nc\n \n+=\n \na\n \n*\n \nb\n\n\n\n\n\nwhen \na\n, \nb\n and \nc\n are complex numbers. Naively, this expression could be expanded into \n\n\nre\n(\nc\n)\n \n+=\n \nre\n(\na\n)\n \n*\n \nre\n(\nb\n)\n \n-\n \nim\n(\na\n)\n \n*\n \nim\n(\nb\n)\n\n\nim\n(\nc\n)\n \n+=\n \nre\n(\na\n)\n \n*\n \nim\n(\nb\n)\n \n+\n \nim\n(\na\n)\n \n*\n \nre\n(\nb\n)\n\n\n\n\n\nwhere \nre(.), im(.)\n denote the real and imaginary part of its argument respectively. This expresson can nicely be packed into a total of four FMA operations per line. However, in the simplified form above which is usually used in context of operator overloading, the compier would have to evaluate the right hand side first and then sum the result into \nc\n. This is much less efficient since in that case, only two FMA as well as two multiplications and additions could be used. One has to keep that in mind when doing complex algebra in C++. In many cases it is better to inline code and avoid otherwise useful operator overloading techniques for complex algebra.\n\n\nEnsuring Vectorization\n\n\nVectorization in Kokkos is achieved by a two-level \nnested parallelism\n, where the outer loop spawns threads (pthreads, OpenMP-threads) on the CPU and threads in CUDA-block y-direction on the GPU. The inner loop then applies vectorization pragmas on the CPU or spwans threads in x-direction on the GPU. This is where we have to show some awareness of architectural differences: the spinor work type \nTST\n needs to be a scalar type on the GPU and a vector type on the CPU. Hence we declare the following types on GPU and CPU respectively\n\n\ntemplate\ntypename\n \nT\n,\nN\n \n\nstruct\n \nCPUSIMDComplex\n \n{\n\n    \nKokkos\n::\ncomplex\nT\n \n_data\n[\nN\n];\n\n    \nT\n \noperator\n()(\nint\n \nlane\n)\n \n{\n\n        \nreturn\n \n_data\n[\nlane\n];\n\n    \n}\n\n    \n...\n\n\n};\n\n\n\ntemplate\ntypename\n \nT\n,\nN\n \n\nstruct\n \nGPUSIMDComplex\n \n{\n\n    \nKokkos\n::\ncomplex\nT\n \n_data\n;\n\n    \nT\n \noperator\n()(\nint\n \nlane\n)\n \n{\n\n        \nreturn\n \n_data\n;\n\n    \n}\n\n    \n...\n\n\n};\n\n\n\n\n\nThe latter construct might look confusing first, because the access operator ignores the \nlane\n parameter. This is because the SIMT threading is implicit in Kokkos and each SIMT thread is holding it's own data \n_data\n. Nevertheless, it is useful to implement the access operator that way to preserve a common, portable style throughout the rest of the code.\n\n\nSpecialization for CPU\n\n\nIn theory, these two types are sufficient for ensuring proper vectorization on both CPU and GPU. In our experiments however, we found that neither Intel nor GNU compiler could vectorize the complex operations inside the spinors properly, leading to a very poor performance. This is not a problem of Kokkos itself, it is merely the inability of compilers to efficiently vectorized complex algebra.\nWe therefore provided a template  specialization for the \nCPUSIMDComplex\n datatype which we implemented by explicitly using AVX512 intrinsics. For example, the datatype then becomes\n\n\ntemplate\n\n\nstruct\n \nCPUSIMDComplex\nfloat\n,\n8\n \n{\n\n    \nexplicit\n \nCPUSIMDComplex\nfloat\n,\n8\n()\n \n{}\n\n\n    \nunion\n \n{\n\n        \nKokkos\n::\ncomplex\nfloat\n \n_data\n[\n8\n];\n\n        \n__m512\n \n_vdata\n;\n\n    \n};\n\n    \n...\n\n\n};\n\n\n\n\n\nand, for example, the vectorized multiplication of two complex numbers\n\n\ntemplate\n \nKOKKOS_FORCEINLINE_FUNCTION\n\n\nvoid\n \nComplexCMadd\nfloat\n,\n8\n,\nCPUSIMDComplex\n,\nCPUSIMDComplex\n(\nCPUSIMDComplex\nfloat\n,\n8\n \nres\n,\n\n                                                         \nconst\n \nKokkos\n::\ncomplex\nfloat\n \na\n,\n\n                                                         \nconst\n \nCPUSIMDComplex\nfloat\n,\n8\n \nb\n)\n\n\n{\n\n  \n__m512\n \navec_re\n \n=\n \n_mm512_set1_ps\n(\n \na\n.\nreal\n()\n \n);\n\n  \n__m512\n \navec_im\n \n=\n \n_mm512_set1_ps\n(\n \na\n.\nimag\n()\n \n);\n\n\n  \n__m512\n \nsgnvec\n \n=\n \n_mm512_set_ps\n(\n \n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n,\n1\n,\n-\n1\n);\n\n  \n__m512\n \nperm_b\n \n=\n \n_mm512_mul_ps\n(\nsgnvec\n,\n_mm512_shuffle_ps\n(\nb\n.\n_vdata\n,\nb\n.\n_vdata\n,\n0xb1\n));\n\n\n  \nres\n.\n_vdata\n \n=\n \n_mm512_fmadd_ps\n(\n \navec_re\n,\n \nb\n.\n_vdata\n,\n \nres\n.\n_vdata\n);\n\n  \nres\n.\n_vdata\n \n=\n \n_mm512_fmadd_ps\n(\n \navec_im\n,\nperm_b\n,\n \nres\n.\n_vdata\n);\n\n\n}\n\n\n\n\n\nNote that we use inter-lane shuffle operations to swap complex and imaginary parts and use vectorized FMA operations. We suspect that compilers are unable to detect the opportunity of performing those inter-lane shuffles and thus fail to properly vectorize the code.", 
            "title": "Kokkos"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#kokkos-implementation", 
            "text": "In this implementation, we will use the  Kokkos::View  type as our data container. Therefore, the  spinor and gaugefield classes  become  template typename   ST , int   nspin   class   CBSpinor   {  public :  \n     ...  private : \n     // dims are: site, color, spin \n     Kokkos :: View ST * [ 3 ][ nspin ]   data ;  };  template typename   GT   class   CBGaugeField   {  public : \n     ...  private : \n     // dims are: site, direction, color, color \n     gauge_container GT * [ 4 ][ 3 ][ 3 ]   data ;  };   Note that the site index dimension is a runtime dimension (denoted by  * ) whereas the other dimensions - color and spin - are fixed (denoted by  [const] ). Explicitly stating this is recommended by the kokkos developers because it should help the compiler to optimize the code.   In the  Wilsion operator class , all what we need to do is to insert the kokkos parallel dispatcher. Hence it becomes  template typename   GT ,   typename   ST ,   typename   TST  class   Dslash   { \n     public : \n     void   operator ( const   CBSpinor ST , 4   s_in , \n                   const   CBGaugeField GT   g_in , \n                   CBSpinor ST , 4   s_out , \n                   int   plus_minus )  \n     { \n         // Threaded loop over sites \n         Kokkos :: parallel_for ( num_sites ,   KOKKOS_LAMBDA ( int   i )   { \n             ... \n             }); \n     }  };", 
            "title": "Kokkos Implementation"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#complex-numbers-and-c", 
            "text": "We want to emphasize a subtle performance pitfall when it comes to complex numbers in C++. The language standards inhibit the compiler to efficiently optimize operations such as   c   +=   a   *   b   when  a ,  b  and  c  are complex numbers. Naively, this expression could be expanded into   re ( c )   +=   re ( a )   *   re ( b )   -   im ( a )   *   im ( b )  im ( c )   +=   re ( a )   *   im ( b )   +   im ( a )   *   re ( b )   where  re(.), im(.)  denote the real and imaginary part of its argument respectively. This expresson can nicely be packed into a total of four FMA operations per line. However, in the simplified form above which is usually used in context of operator overloading, the compier would have to evaluate the right hand side first and then sum the result into  c . This is much less efficient since in that case, only two FMA as well as two multiplications and additions could be used. One has to keep that in mind when doing complex algebra in C++. In many cases it is better to inline code and avoid otherwise useful operator overloading techniques for complex algebra.", 
            "title": "Complex Numbers and C++"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#ensuring-vectorization", 
            "text": "Vectorization in Kokkos is achieved by a two-level  nested parallelism , where the outer loop spawns threads (pthreads, OpenMP-threads) on the CPU and threads in CUDA-block y-direction on the GPU. The inner loop then applies vectorization pragmas on the CPU or spwans threads in x-direction on the GPU. This is where we have to show some awareness of architectural differences: the spinor work type  TST  needs to be a scalar type on the GPU and a vector type on the CPU. Hence we declare the following types on GPU and CPU respectively  template typename   T , N   struct   CPUSIMDComplex   { \n     Kokkos :: complex T   _data [ N ]; \n     T   operator ()( int   lane )   { \n         return   _data [ lane ]; \n     } \n     ...  };  template typename   T , N   struct   GPUSIMDComplex   { \n     Kokkos :: complex T   _data ; \n     T   operator ()( int   lane )   { \n         return   _data ; \n     } \n     ...  };   The latter construct might look confusing first, because the access operator ignores the  lane  parameter. This is because the SIMT threading is implicit in Kokkos and each SIMT thread is holding it's own data  _data . Nevertheless, it is useful to implement the access operator that way to preserve a common, portable style throughout the rest of the code.", 
            "title": "Ensuring Vectorization"
        }, 
        {
            "location": "/case_studies/qcd/kokkos_implementation/#specialization-for-cpu", 
            "text": "In theory, these two types are sufficient for ensuring proper vectorization on both CPU and GPU. In our experiments however, we found that neither Intel nor GNU compiler could vectorize the complex operations inside the spinors properly, leading to a very poor performance. This is not a problem of Kokkos itself, it is merely the inability of compilers to efficiently vectorized complex algebra.\nWe therefore provided a template  specialization for the  CPUSIMDComplex  datatype which we implemented by explicitly using AVX512 intrinsics. For example, the datatype then becomes  template  struct   CPUSIMDComplex float , 8   { \n     explicit   CPUSIMDComplex float , 8 ()   {} \n\n     union   { \n         Kokkos :: complex float   _data [ 8 ]; \n         __m512   _vdata ; \n     }; \n     ...  };   and, for example, the vectorized multiplication of two complex numbers  template   KOKKOS_FORCEINLINE_FUNCTION  void   ComplexCMadd float , 8 , CPUSIMDComplex , CPUSIMDComplex ( CPUSIMDComplex float , 8   res , \n                                                          const   Kokkos :: complex float   a , \n                                                          const   CPUSIMDComplex float , 8   b )  { \n   __m512   avec_re   =   _mm512_set1_ps (   a . real ()   ); \n   __m512   avec_im   =   _mm512_set1_ps (   a . imag ()   ); \n\n   __m512   sgnvec   =   _mm512_set_ps (   1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 , 1 , - 1 ); \n   __m512   perm_b   =   _mm512_mul_ps ( sgnvec , _mm512_shuffle_ps ( b . _vdata , b . _vdata , 0xb1 )); \n\n   res . _vdata   =   _mm512_fmadd_ps (   avec_re ,   b . _vdata ,   res . _vdata ); \n   res . _vdata   =   _mm512_fmadd_ps (   avec_im , perm_b ,   res . _vdata );  }   Note that we use inter-lane shuffle operations to swap complex and imaginary parts and use vectorized FMA operations. We suspect that compilers are unable to detect the opportunity of performing those inter-lane shuffles and thus fail to properly vectorize the code.", 
            "title": "Specialization for CPU"
        }, 
        {
            "location": "/case_studies/nek/", 
            "text": "", 
            "title": "NekBone"
        }, 
        {
            "location": "/case_studies/md/", 
            "text": "Developing a new molecular dynamics code with an eye towards portability\n\n\nClassical molecular dynamics\n \nhas become a ubiquitous\ncomputational modeling tool for a number of disciplines,\nfrom biology and biochemistry, to geochemistry and polymer\nphysics.  Due to intense efforts from a\nnumber of developers over the past 50 years, several MD programs have been highly successful in achieving commendable\nefficiency and overall performance.\n\n\nThe classical molecular dynamics algorithm involves three\nmain components: the integration step, the calculation of\nshort-range forces, and the calculation of long-range\nforces. The integration step is generally the quickest part\nof the calculation, and as it has some memory-intensive\naspects, is often calculated using the CPU, in\nimplementations using heterogenous architectures. The\nlong-range force calculation, in most implementations,\ninvolves an Ewald sum. This requires the use of Fourier transform\nmethods, which are fast for smaller systems, but do not\nscale well for large systems. This is an active area of\ndevelopment and is not addressed here. The\nmajor bottleneck for all system sizes is the short-range\nnon-bonded forces (SNFs) calculation, as it involves a sum\nof pairwise interactions over multiple subsets of the\nparticle space.\n\n\nAs part of our portable performance studies, we have written\na new SNF kernel, wherein we use directives (OpenACC) to\nimplement the parallel steps of the computation. We have also produced  an alternate\nimplementation where matrix-matrix multiplication is used to \ncalculate pairwise distances in the SNF calculation. This\nalternate implementation, though requiring more\nfloating-point operations, is shown to perform well because\nof the performance of platform-specific BLAS libraries.  \n\n\nDetails of the MD experiment can be found in this\n\nreport\n.", 
            "title": "MD"
        }, 
        {
            "location": "/case_studies/md/#developing-a-new-molecular-dynamics-code-with-an-eye-towards-portability", 
            "text": "Classical molecular dynamics  \nhas become a ubiquitous\ncomputational modeling tool for a number of disciplines,\nfrom biology and biochemistry, to geochemistry and polymer\nphysics.  Due to intense efforts from a\nnumber of developers over the past 50 years, several MD programs have been highly successful in achieving commendable\nefficiency and overall performance.  The classical molecular dynamics algorithm involves three\nmain components: the integration step, the calculation of\nshort-range forces, and the calculation of long-range\nforces. The integration step is generally the quickest part\nof the calculation, and as it has some memory-intensive\naspects, is often calculated using the CPU, in\nimplementations using heterogenous architectures. The\nlong-range force calculation, in most implementations,\ninvolves an Ewald sum. This requires the use of Fourier transform\nmethods, which are fast for smaller systems, but do not\nscale well for large systems. This is an active area of\ndevelopment and is not addressed here. The\nmajor bottleneck for all system sizes is the short-range\nnon-bonded forces (SNFs) calculation, as it involves a sum\nof pairwise interactions over multiple subsets of the\nparticle space.  As part of our portable performance studies, we have written\na new SNF kernel, wherein we use directives (OpenACC) to\nimplement the parallel steps of the computation. We have also produced  an alternate\nimplementation where matrix-matrix multiplication is used to \ncalculate pairwise distances in the SNF calculation. This\nalternate implementation, though requiring more\nfloating-point operations, is shown to perform well because\nof the performance of platform-specific BLAS libraries.    Details of the MD experiment can be found in this report .", 
            "title": "Developing a new molecular dynamics code with an eye towards portability"
        }, 
        {
            "location": "/resources/", 
            "text": "Bronson / Jack / Tim to Write\n\n\nMove to performance-portability", 
            "title": "Other Resources"
        }, 
        {
            "location": "/demo/demo/", 
            "text": "Welcome to NERSC\n\n\nWelcome to the National Energy Research Scientific Computing Center, a high performance scientific computing center.\nThis document will guide you through the basics of using NERSC\u2019s supercomputers, storage systems, and services.\n\n\nWhat is NERSC?\n\n\nNERSC provides High Performance Computing and Storage facilities and support for research sponsored by, and of interest to, the U.S. Department of Energy Office of Science. NERSC has the unique programmatic role of supporting all six Office of Science program offices: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics, and Nuclear Physics. Scientists who have been awarded research funding by any of the offices are eligible to apply for an allocation of NERSC time. Additional awards may be given to non-DOE funded project teams whose research is aligned with the Office of Science's mission. Allocations of time and storage are made by DOE.\n\n\nNERSC is a national center, organizationally part of Lawrence Berkeley National Laboratory in Berkeley, CA. NERSC staff and facilities are primarily located at Berkeley Lab's Shyh Wang Hall on the Berkeley Lab campus.\n\n\nExternal links\n\n\n\n\nOLCF\n\n\nALCF\n\n\nNERSC\n\n\n\n\nInternal links\n\n\n\n\nPortability definition\n\n\n\n\nTables:\n\n\n\n\n\n\n\n\nSystem Type\n\n\nCray XC40\n\n\n\n\n\n\n\n\n\n\nTheoretical Peak Performance (System)\n\n\n31.4 PFlops\n\n\n\n\n\n\nTheoretical Peak Performance (Haswell nodes)\n\n\n2.3 PFlops\n\n\n\n\n\n\nTheoretical Peak Performance (Xeon Phi nodes)\n\n\n29.1 PFlops\n\n\n\n\n\n\nCompute Nodes (Haswell)\n\n\n2,388\n\n\n\n\n\n\n\n\nInclude scripts/ source code\n\n\nThis site supports an include extension to Markdown.\n\n\nOne way to run a pure MPI job on Cori is\n\n\n#!/bin/bash -l\n\n\n#SBATCH -p debug\n\n\n#SBATCH -N 64\n\n\n#SBATCH -t 00:20:00\n\n\n#SBATCH -J my_job\n\n\n#SBATCH -L SCRATCH\n\n\n#SBATCH -C haswell\n\n\n\n# an extra -c 2 flag is optional for fully packed pure MPI\n\nsrun -n \n2048\n ./mycode.exe\n\n\n\n\n\n\nWarning\n\n\nThe \n-c\n and \n--cpu_bind=\n options for \nsrun\n are \nrequired\n for hybrid jobs or jobs which do not utilize all physical cores \n\n\n\n\nSome source code\n\n\nInstrumented C code to measure AI\n\n\n// Code must be built with appropriate paths for VTune include file (ittnotify.h) and library (-littnotify)\n\n\n#include\n \nittnotify.h\n\n\n\n__SSC_MARK\n(\n0x111\n);\n \n// start SDE tracing, note it uses 2 underscores\n\n\n__itt_resume\n();\n \n// start VTune, again use 2 underscores\n\n\n\nfor\n \n(\nk\n=\n0\n;\n \nk\nNTIMES\n;\n \nk\n++\n)\n \n{\n\n \n#pragma omp parallel for\n\n \nfor\n \n(\nj\n=\n0\n;\n \nj\nSTREAM_ARRAY_SIZE\n;\n \nj\n++\n)\n\n \na\n[\nj\n]\n \n=\n \nb\n[\nj\n]\n+\nscalar\n*\nc\n[\nj\n];\n\n\n}\n\n\n\n__itt_pause\n();\n \n// stop VTune\n\n\n__SSC_MARK\n(\n0x222\n);\n \n// stop SDE tracing\n\n\n\n\n\nAnd some totally unrelated python code\n\n\ndef\n \ncount_cross_connections\n(\ncounts\n):\n\n    \nCounts is a list of the number of nodes in each (non-zero) group\n\n    \nreturn\n \nsum\n(\n \nx\n[\n0\n]\n*\nx\n[\n1\n]\n \nfor\n \nx\n \nin\n \nitertools\n.\ncombinations\n(\ncounts\n,\n \n2\n)\n \n)\n\n\n\n\n\nLaTex support\n\n\n\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\]\nfrom:\n\n\n$$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$\n\n\n\n\nImages\n\n\n\n\nInline HTML\n\n\nThis is probably best avoided if possible, but it works.\n\n\n\n  \nDefinition list\n\n  \nIs something people use sometimes.\n\n\n  \nMarkdown in HTML\n\n  \nDoes *not* work **very** well. Use HTML \ntags\n.", 
            "title": "Demo"
        }, 
        {
            "location": "/demo/demo/#welcome-to-nersc", 
            "text": "Welcome to the National Energy Research Scientific Computing Center, a high performance scientific computing center.\nThis document will guide you through the basics of using NERSC\u2019s supercomputers, storage systems, and services.", 
            "title": "Welcome to NERSC"
        }, 
        {
            "location": "/demo/demo/#what-is-nersc", 
            "text": "NERSC provides High Performance Computing and Storage facilities and support for research sponsored by, and of interest to, the U.S. Department of Energy Office of Science. NERSC has the unique programmatic role of supporting all six Office of Science program offices: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics, and Nuclear Physics. Scientists who have been awarded research funding by any of the offices are eligible to apply for an allocation of NERSC time. Additional awards may be given to non-DOE funded project teams whose research is aligned with the Office of Science's mission. Allocations of time and storage are made by DOE.  NERSC is a national center, organizationally part of Lawrence Berkeley National Laboratory in Berkeley, CA. NERSC staff and facilities are primarily located at Berkeley Lab's Shyh Wang Hall on the Berkeley Lab campus.", 
            "title": "What is NERSC?"
        }, 
        {
            "location": "/demo/demo/#external-links", 
            "text": "OLCF  ALCF  NERSC", 
            "title": "External links"
        }, 
        {
            "location": "/demo/demo/#internal-links", 
            "text": "Portability definition   Tables:     System Type  Cray XC40      Theoretical Peak Performance (System)  31.4 PFlops    Theoretical Peak Performance (Haswell nodes)  2.3 PFlops    Theoretical Peak Performance (Xeon Phi nodes)  29.1 PFlops    Compute Nodes (Haswell)  2,388", 
            "title": "Internal links"
        }, 
        {
            "location": "/demo/demo/#include-scripts-source-code", 
            "text": "This site supports an include extension to Markdown.  One way to run a pure MPI job on Cori is  #!/bin/bash -l  #SBATCH -p debug  #SBATCH -N 64  #SBATCH -t 00:20:00  #SBATCH -J my_job  #SBATCH -L SCRATCH  #SBATCH -C haswell  # an extra -c 2 flag is optional for fully packed pure MPI \nsrun -n  2048  ./mycode.exe   Warning  The  -c  and  --cpu_bind=  options for  srun  are  required  for hybrid jobs or jobs which do not utilize all physical cores", 
            "title": "Include scripts/ source code"
        }, 
        {
            "location": "/demo/demo/#some-source-code", 
            "text": "Instrumented C code to measure AI  // Code must be built with appropriate paths for VTune include file (ittnotify.h) and library (-littnotify)  #include   ittnotify.h  __SSC_MARK ( 0x111 );   // start SDE tracing, note it uses 2 underscores  __itt_resume ();   // start VTune, again use 2 underscores  for   ( k = 0 ;   k NTIMES ;   k ++ )   { \n  #pragma omp parallel for \n  for   ( j = 0 ;   j STREAM_ARRAY_SIZE ;   j ++ ) \n  a [ j ]   =   b [ j ] + scalar * c [ j ];  }  __itt_pause ();   // stop VTune  __SSC_MARK ( 0x222 );   // stop SDE tracing   And some totally unrelated python code  def   count_cross_connections ( counts ): \n     Counts is a list of the number of nodes in each (non-zero) group \n     return   sum (   x [ 0 ] * x [ 1 ]   for   x   in   itertools . combinations ( counts ,   2 )   )", 
            "title": "Some source code"
        }, 
        {
            "location": "/demo/demo/#latex-support", 
            "text": "\\[\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n\\] from:  $$\n\\frac{n!}{k!(n-k)!} = \\binom{n}{k}\n$$", 
            "title": "LaTex support"
        }, 
        {
            "location": "/demo/demo/#images", 
            "text": "", 
            "title": "Images"
        }, 
        {
            "location": "/demo/demo/#inline-html", 
            "text": "This is probably best avoided if possible, but it works.  \n   Definition list \n   Is something people use sometimes. \n\n   Markdown in HTML \n   Does *not* work **very** well. Use HTML  tags .", 
            "title": "Inline HTML"
        }
    ]
}